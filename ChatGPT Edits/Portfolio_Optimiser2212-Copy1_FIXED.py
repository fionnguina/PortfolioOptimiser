{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grabbing Data From Yahoo Finance For Stock Build last updated 22/12/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### BLOCK 1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda_Depository\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "import yfinance as yf\n",
    "import openpyxl\n",
    "import pathlib\n",
    "import hashlib\n",
    "import shutil\n",
    "import io\n",
    "import sys, os\n",
    "import json\n",
    "import xlwings as xw\n",
    "import statsmodels.api as sm\n",
    "import re, zipfile\n",
    "import multiprocessing as mp\n",
    "import pptx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import cvxpy as cp\n",
    "from datetime import datetime\n",
    "from scipy.optimize import minimize\n",
    "from numpy.linalg import pinv\n",
    "from pathlib import Path\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "from pptx.enum.text import MSO_AUTO_SIZE\n",
    "try:\n",
    "    import win32com.client as win32\n",
    "    HAS_WIN32COM = True\n",
    "except Exception:\n",
    "    HAS_WIN32COM = False\n",
    "\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### BLOCK 2 Global codes and Data Retrieval from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Central base directory\n",
    "# ---------------------------------------------------------------------\n",
    "def _app_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Determine the application directory dynamically:\n",
    "      - When frozen (PyInstaller): use the exe folder\n",
    "      - When run as a script: use the script's folder\n",
    "      - When interactive (Jupyter/IPython): use cwd\n",
    "    \"\"\"\n",
    "    if getattr(sys, \"frozen\", False):\n",
    "        return Path(sys.executable).parent\n",
    "    if \"__file__\" in globals():\n",
    "        return Path(__file__).resolve().parent\n",
    "    return Path(os.getcwd())\n",
    "\n",
    "\n",
    "# Absolute path to your central config root (for dev use)\n",
    "_DEV_BASE = Path.home() / \"Portfolio_Optimiser\"\n",
    "\n",
    "# Use the dev folder if it exists, otherwise fall back to dynamic app dir\n",
    "APP_DIR = _DEV_BASE if _DEV_BASE.exists() else _app_dir()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config file and Excel workbook paths\n",
    "# ---------------------------------------------------------------------\n",
    "def _default_excel_path() -> str:\n",
    "    \"\"\"Return full path to the default Excel workbook.\"\"\"\n",
    "    return str((APP_DIR / \"Stock Analysis.xlsm\").resolve())\n",
    "\n",
    "CONFIG_PATH = APP_DIR / \"config.json\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Export directory (for generated reports)\n",
    "# ---------------------------------------------------------------------\n",
    "EXPORT_DIR = APP_DIR \n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Default configuration values\n",
    "# ---------------------------------------------------------------------\n",
    "_DEFAULTS = {\n",
    "    \"excel_path\": _default_excel_path(),\n",
    "    \"marginal_tax_rate\": 0.37,\n",
    "    \"carry_forward_losses\": 0.0,\n",
    "    \"lot_match_method\": \"HIFO\",\n",
    "    \"open_after_save\": True,\n",
    "    \"use_xlwings\": True,\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config loader\n",
    "# ---------------------------------------------------------------------\n",
    "def load_config() -> dict:\n",
    "    cfg = dict(_DEFAULTS)\n",
    "    try:\n",
    "        if CONFIG_PATH.exists():\n",
    "            with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                user_cfg = json.load(f)\n",
    "            for k, v in user_cfg.items():\n",
    "                if k in cfg:\n",
    "                    cfg[k] = v\n",
    "    except Exception as e:\n",
    "        print(f\"[config] using defaults (error reading config.json): {e}\")\n",
    "\n",
    "    # Ensure workbook directory exists\n",
    "    try:\n",
    "        os.makedirs(Path(cfg[\"excel_path\"]).parent, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Definition for PPTX\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "CFG = load_config()\n",
    "\n",
    "def _os_open(path: str) -> None:\n",
    "    try:\n",
    "        os.startfile(path)  # Windows\n",
    "    except AttributeError:\n",
    "        import subprocess, sys\n",
    "        if sys.platform == \"darwin\":\n",
    "            subprocess.run([\"open\", path])\n",
    "        else:\n",
    "            subprocess.run([\"xdg-open\", path])\n",
    "\n",
    "user_opts = {}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Actual Code\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "TILT_FACTORS = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"MOM\"]\n",
    "\n",
    "# Bind config into existing globals you already use\n",
    "filename               = CFG[\"excel_path\"]\n",
    "MARGINAL_TAX_RATE      = float(CFG[\"marginal_tax_rate\"])\n",
    "CAPITAL_LOSS_CARRY_FWD = float(CFG[\"carry_forward_losses\"])\n",
    "LOT_MATCH_METHOD       = str(CFG[\"lot_match_method\"]).upper()\n",
    "OPEN_AFTER_SAVE        = bool(CFG.get(\"open_after_save\", True))\n",
    "USE_XLWINGS            = bool(CFG.get(\"use_xlwings\", True))\n",
    "\n",
    "# -------- Risk-free (AU): current RBA cash rate target --------\n",
    "def get_rba_cash_rate_target_current(default=0.04):\n",
    "    \"\"\"\n",
    "    Returns the latest RBA cash rate target as a decimal.\n",
    "    Handles BOTH:\n",
    "      - RBA HTML table (Cash Rate Target %)\n",
    "      - RBA F1.1 CSV (multiple column naming variants)\n",
    "    Falls back safely to `default` if all fail.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Try HTML source from the main RBA page\n",
    "    try:\n",
    "        dfs = pd.read_html(\"https://www.rba.gov.au/statistics/cash-rate/\")\n",
    "        for df in dfs:\n",
    "            # Normalise column names first\n",
    "            df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "            # Look for ANY column containing both 'cash' and 'target'\n",
    "            candidates = [\n",
    "                c for c in df.columns\n",
    "                if (\"cash\" in c and \"target\" in c)\n",
    "            ]\n",
    "            if candidates:\n",
    "                col = candidates[0]\n",
    "                val = pd.to_numeric(df[col], errors=\"coerce\").dropna().iloc[0]\n",
    "                return float(val) / 100.0\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Try CSV fallback (this ALWAYS exists)\n",
    "    try:\n",
    "        csv = \"https://www.rba.gov.au/statistics/tables/csv/f1.1-data.csv\"\n",
    "        df = pd.read_csv(csv)\n",
    "\n",
    "        # Normalise header\n",
    "        df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "\n",
    "        # Try to locate any cash-rate-target column\n",
    "        candidates = [\n",
    "            c for c in df.columns\n",
    "            if (\"cash\" in c and \"target\" in c)\n",
    "        ]\n",
    "        if candidates:\n",
    "            col = candidates[0]\n",
    "            vals = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n",
    "            if not vals.empty:\n",
    "                return float(vals.iloc[-1]) / 100.0\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Hard fallback\n",
    "    return float(default)\n",
    "\n",
    "\n",
    "# -------- Simple on-disk cache (7-day TTL) for FF5 + MOM --------\n",
    "_CACHE_DIR = pathlib.Path(os.path.expanduser(\"~\")) / \".portfolio_optimiser_cache\"\n",
    "_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _cache_path(url: str) -> pathlib.Path:\n",
    "    key = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
    "    return _CACHE_DIR / f\"{key}.csv\"\n",
    "\n",
    "def _cached_read(url: str, build_df_fn, ttl_days: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If we have a cached CSV newer than ttl_days, load it.\n",
    "    Otherwise call build_df_fn() to construct the DataFrame, then store it.\n",
    "    Assumes the DF has a DatetimeIndex.\n",
    "    \"\"\"\n",
    "    p = _cache_path(url)\n",
    "    try:\n",
    "        if p.exists():\n",
    "            age_sec = time.time() - p.stat().st_mtime\n",
    "            if age_sec <= ttl_days * 86400:\n",
    "                df = pd.read_csv(p, index_col=0, parse_dates=[0])\n",
    "                # ensure sorted Date index\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                return df.sort_index()\n",
    "    except Exception as e:\n",
    "        print(f\"[cache] read miss due to: {e}\")\n",
    "\n",
    "    df = build_df_fn()\n",
    "    try:\n",
    "        df.to_csv(p)\n",
    "    except Exception as e:\n",
    "        print(f\"[cache] write skipped due to: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------- FF5F + Momentum loaders (Dartmouth) --------\n",
    "FF5_DAILY_ZIP = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_daily_CSV.zip\"\n",
    "MOM_DAILY_ZIP = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_daily_CSV.zip\"\n",
    "\n",
    "def get_mom_daily():\n",
    "    def _builder():\n",
    "        r = requests.get(MOM_DAILY_ZIP, timeout=60); r.raise_for_status()\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        csv = next(n for n in z.namelist() if n.lower().endswith(\".csv\"))\n",
    "        raw = z.read(csv).decode(\"latin1\", errors=\"ignore\").splitlines()\n",
    "        num_rx = re.compile(r\"^\\s*\\d{6,8}\\s*[,\\s]\")\n",
    "        first = next(i for i, ln in enumerate(raw) if num_rx.match(ln))\n",
    "        header = \"Date,MOM\"\n",
    "        data = [header] + [ln.strip() for ln in raw[first:] if num_rx.match(ln)]\n",
    "        df = pd.read_csv(io.StringIO(\"\\n\".join(data)), engine=\"python\", sep=r\"\\s*,\\s*\")\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"Date\"]).set_index(\"Date\").sort_index()\n",
    "        df[\"MOM\"] = pd.to_numeric(df[\"MOM\"], errors=\"coerce\") / 100.0  # decimal\n",
    "        return df[[\"MOM\"]]\n",
    "    df = _cached_read(MOM_DAILY_ZIP, _builder, ttl_days=7)\n",
    "    # ensure schema exactly as expected\n",
    "    df = df.copy()\n",
    "    if \"MOM\" not in df.columns:\n",
    "        df[\"MOM\"] = pd.to_numeric(df.iloc[:, 0], errors=\"coerce\")\n",
    "        df = df[[\"MOM\"]]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "def get_ff5_daily(cache_csv_path=None):\n",
    "    \"\"\"\n",
    "    Fama\u2013French 5 Factors (2x3) [Daily].\n",
    "    Returns columns (decimals): ['Mkt-RF','SMB','HML','RMW','CMA','RF'] indexed by Date.\n",
    "    Uses 7-day cached CSV in %USERPROFILE%\\\\.portfolio_optimiser_cache\n",
    "    \"\"\"\n",
    "    def _builder():\n",
    "        resp = requests.get(FF5_DAILY_ZIP, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        csv_name = next(n for n in zf.namelist() if n.lower().endswith(\".csv\"))\n",
    "\n",
    "        raw = zf.read(csv_name).decode(\"latin1\", errors=\"ignore\")\n",
    "        lines = raw.splitlines()\n",
    "\n",
    "        num_rx = re.compile(r\"^\\s*\\d{6,8}\\s*[,\\s]\")\n",
    "        first_data_idx = next(i for i, ln in enumerate(lines) if num_rx.match(ln))\n",
    "\n",
    "        header_idx = None\n",
    "        for i in range(max(0, first_data_idx-5), first_data_idx+1):\n",
    "            if re.search(r\"\\bdate\\b\", lines[i], flags=re.I) and (\"mkt\" in lines[i].lower()):\n",
    "                header_idx = i\n",
    "                break\n",
    "\n",
    "        header = lines[header_idx].strip() if header_idx is not None else \"Date,Mkt-RF,SMB,HML,RMW,CMA,RF\"\n",
    "        data_lines = [header]\n",
    "        for ln in lines[first_data_idx:]:\n",
    "            if not num_rx.match(ln):\n",
    "                break\n",
    "            data_lines.append(ln.strip())\n",
    "\n",
    "        df = pd.read_csv(io.StringIO(\"\\n\".join(data_lines)), engine=\"python\", sep=r\"\\s*,\\s*\")\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        col_map = {c.lower().replace(\" \", \"\"): c for c in df.columns}\n",
    "        ren = {}\n",
    "        for want in [\"Date\",\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]:\n",
    "            key = want.lower().replace(\" \", \"\")\n",
    "            if key in col_map:\n",
    "                ren[col_map[key]] = want\n",
    "        df = df.rename(columns=ren)\n",
    "\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"Date\"]).set_index(\"Date\").sort_index()\n",
    "        factor_cols = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]\n",
    "        df[factor_cols] = df[factor_cols].apply(pd.to_numeric, errors=\"coerce\") / 100.0\n",
    "        df = df.dropna(subset=factor_cols)\n",
    "        return df\n",
    "\n",
    "    df = _cached_read(FF5_DAILY_ZIP, _builder, ttl_days=7)\n",
    "\n",
    "    # Optional external cache file output for your own debugging\n",
    "    if cache_csv_path:\n",
    "        try:\n",
    "            df.to_csv(cache_csv_path, index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[ff5] could not write cache_csv_path: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ff5_mom_daily():\n",
    "    \"\"\"\n",
    "    Return daily FF5+MOM factors (USD) as decimals with columns:\n",
    "        ['Mkt-RF','SMB','HML','RMW','CMA','MOM','RF']\n",
    "    on a common date index.\n",
    "\n",
    "    No FX or yfinance downloads here \u2013 just join FF5 and MOM.\n",
    "    \"\"\"\n",
    "    ff5_only = get_ff5_daily()   # already decimals\n",
    "    mom = get_mom_daily()        # already decimals\n",
    "\n",
    "    out = ff5_only.join(mom, how=\"inner\").sort_index()\n",
    "\n",
    "    # Ensure consistent column order\n",
    "    cols = [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"MOM\", \"RF\"]\n",
    "    return out[cols]\n",
    "\n",
    "\n",
    "# -------- Foreign Exchange from Yahoo Finance --------\n",
    "def _last_numeric(x):\n",
    "    v = x.iloc[-1]\n",
    "    if isinstance(v, pd.Series):\n",
    "        v = v.iloc[0]\n",
    "    return float(v)\n",
    "\n",
    "def get_usd_aud_fx(default=1.50):\n",
    "    \"\"\"\n",
    "    Return the latest USD/AUD FX rate using the global `fx` series\n",
    "    (downloaded in Block 3 from 'USDAUD=X').\n",
    "\n",
    "    No yfinance downloads here \u2013 all downloads stay in Block 3.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # `fx` is built in Block 3:\n",
    "        # fx_raw = yf.download(\"USDAUD=X\", ... )[\"Close\"]\n",
    "        # fx = fx_raw.sort_index().ffill()\n",
    "        if \"fx\" in globals():\n",
    "            series = globals()[\"fx\"]\n",
    "\n",
    "            # Handle both Series and single-column DataFrame\n",
    "            if isinstance(series, pd.DataFrame):\n",
    "                s = series.iloc[:, 0]\n",
    "            else:\n",
    "                s = series\n",
    "\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "            if not s.empty:\n",
    "                last = _last_numeric(s)\n",
    "                if last > 0:\n",
    "                    return float(last)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback if fx is missing or broken\n",
    "    return float(default)\n",
    "\n",
    "def fx_to_aud_for_tickers(tickers, usd_aud_rate):\n",
    "    \"\"\"1.0 for AUS tickers (*.AX) & indices (^...), usd_aud_rate for others (assume USD).\"\"\"\n",
    "    out = {}\n",
    "    for t in map(str, tickers):\n",
    "        out[t] = 1.0 if (t.startswith(\"^\") or t.endswith(\".AX\")) else float(usd_aud_rate)\n",
    "    return pd.Series(out, name=\"FX to AUD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 3 Downloading Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XL_PATH = C:\\Users\\Fionn Guina\\Portfolio_Optimiser\\Stock Analysis.xlsm\n",
      "Tickers loaded from sheet: ['A200.AX', 'IEU.AX', 'IJP.AX', 'IOO.AX', 'IVV.AX', 'MTUM.AX', 'MVW.AX', 'QHAL.AX', 'QLTY.AX', 'QUAL.AX', 'SMH', 'SMLL.AX', 'SPY', 'VAS.AX', 'VDHG.AX', 'VGE.AX', 'VGS.AX', 'VLU', 'VLUE.AX', 'VMIN.AX', 'VSO.AX', 'VVLU.AX', '^AORD']\n",
      "[data] Benchmarks downloaded successfully: ['^AORD', '^GSPC', '^IXIC']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) DOWNLOAD PRICES  \u2014 universe comes from Holdings sheet + static starters\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "# If Block 7 defines `filename`, we\u2019ll reuse it. Otherwise set your path here.\n",
    "_XL_PATH = globals().get(\"filename\", _default_excel_path())\n",
    "\n",
    "# Your static \u201cstarter\u201d universe (kept as a safety net / defaults)\n",
    "STATIC_STARTERS = ['^AORD']\n",
    "\n",
    "EXCLUDE_FROM_OPT = {'^AORD'}\n",
    "rf_annual = get_rba_cash_rate_target_current()\n",
    "rf_label = f\"{rf_annual*100:.2f}%\"\n",
    "\n",
    "def _tickers_from_holdings(xl_path, sheet='Holdings'):\n",
    "    \"\"\"Extract 'Security' values using pandas (no COM).\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(xl_path, sheet_name=sheet)\n",
    "    except Exception:\n",
    "        return []\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty or \"Security\" not in df.columns:\n",
    "        return []\n",
    "    sec = (df[\"Security\"].dropna().astype(str).str.strip())\n",
    "    return list(dict.fromkeys([t for t in sec if t]))\n",
    "\n",
    "# 0) Build the universe\n",
    "tickers_from_sheet = _tickers_from_holdings(_XL_PATH, sheet='Holdings')  # dynamic\n",
    "print(\"XL_PATH =\", _XL_PATH)\n",
    "print(\"Tickers loaded from sheet:\", tickers_from_sheet)\n",
    "tickers = list(dict.fromkeys((tickers_from_sheet or []) + STATIC_STARTERS))\n",
    "if '^AORD' not in tickers:\n",
    "    tickers.insert(0, '^AORD')  # always include benchmark\n",
    "\n",
    "# 1) Download prices (robust to single/ multiple tickers)\n",
    "PRICE_PERIOD = '2y'  # set '1y'/'3y' as you like\n",
    "dl = yf.download(tickers, period=PRICE_PERIOD, auto_adjust=True, threads=False, progress=False)\n",
    "\n",
    "if isinstance(dl, pd.DataFrame) and 'Close' in dl.columns:\n",
    "    prices = dl['Close']\n",
    "else:\n",
    "    # yfinance can return a Series for a single ticker\n",
    "    prices = dl if isinstance(dl, pd.Series) else pd.DataFrame()\n",
    "    if isinstance(prices, pd.Series):\n",
    "        prices = prices.to_frame(name=tickers[0])\n",
    "prices.index = pd.to_datetime(prices.index)\n",
    "# Normalise all tickers to a common daily index\n",
    "idx = pd.date_range(start=prices.index.min(), end=prices.index.max(), freq=\"B\")\n",
    "prices = prices.reindex(idx).ffill().bfill()\n",
    "prices.index.name = \"Date\"\n",
    "prices = prices.loc[:, ~prices.columns.duplicated()]  # de-dup any duplicate tickers defensively\n",
    "\n",
    "# === FF5+MOM DAILY FACTORS (downloaded in the download block) ===\n",
    "# This is the only place we actually call get_ff5_mom_daily(), so all downloads live here.\n",
    "ff5_raw = get_ff5_mom_daily()\n",
    "\n",
    "ff5_raw = ff5_raw.loc[:, ~ff5_raw.columns.duplicated()].copy()\n",
    "\n",
    "# Also ensure correct column order and existence\n",
    "expected_cols = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"MOM\",\"RF\"]\n",
    "ff5_raw = ff5_raw.reindex(columns=expected_cols)\n",
    "\n",
    "# 2-year factor window for betas\n",
    "FF5_BETA_WINDOW_DAYS = 504 #(2 years)\n",
    "ff5_win_for_betas = ff5_raw.tail(FF5_BETA_WINDOW_DAYS)\n",
    "\n",
    "# === AUDUSD FX (for factor AUD-adjustment) ===\n",
    "try:\n",
    "    fx_audusd = yf.download(\n",
    "        tickers=\"AUDUSD=X\",\n",
    "        period=\"6y\",\n",
    "        interval=\"1d\",\n",
    "        auto_adjust=True,\n",
    "        progress=False,\n",
    "        threads=False,\n",
    "    )[\"Close\"]\n",
    "\n",
    "    # align FX to the factor index\n",
    "    fx_audusd = fx_audusd.reindex(ff5_raw.index).ffill()\n",
    "except Exception as e:\n",
    "    print(f\"Warning: failed to download AUDUSD=X; using flat FX=1.0 ({e})\")\n",
    "    fx_audusd = pd.Series(1.0, index=ff5_raw.index)\n",
    "\n",
    "# Currency excess return series used everywhere else\n",
    "fx_ret = fx_audusd.pct_change().fillna(0.0)\n",
    "\n",
    "# ------------- FX conversion for US stocks (to AUD for returns) ------------------\n",
    "\n",
    "# 1) AUD per 1 USD (ensure we end up with a Series)\n",
    "fx_raw = yf.download(\"USDAUD=X\", period=\"5y\", interval=\"1d\",\n",
    "                     auto_adjust=True, threads=False, progress=False)\n",
    "fx = fx_raw[\"Close\"] if isinstance(fx_raw, pd.DataFrame) else fx_raw\n",
    "if isinstance(fx, pd.DataFrame):\n",
    "    fx = fx.iloc[:, 0]\n",
    "fx = pd.to_numeric(fx, errors=\"coerce\").reindex(prices.index).ffill()\n",
    "\n",
    "# identify USD-priced tickers\n",
    "usd_cols = [str(c) for c in prices.columns if not str(c).endswith(\".AX\") and not str(c).startswith(\"^\")]\n",
    "\n",
    "# build an AUD-converted copy safely\n",
    "prices_aud_for_returns = prices.copy()\n",
    "usd_part = prices.loc[:, usd_cols].mul(fx, axis=0)  # align by date\n",
    "prices_aud_for_returns.update(usd_part)\n",
    "\n",
    "# 4) Compute returns *from AUD-converted prices*\n",
    "df_melt = (\n",
    "    prices_aud_for_returns.reset_index()\n",
    "      .melt(id_vars='Date', var_name='Security', value_name='Close')\n",
    "      .sort_values(['Security','Date'])\n",
    ")\n",
    "df_melt['Return'] = df_melt.groupby('Security', sort=False)['Close'].pct_change(fill_method=None)\n",
    "df_melt = df_melt.dropna()\n",
    "\n",
    "# 5) FX map for holdings (last-price conversion in the sheet)\n",
    "usd_aud = get_usd_aud_fx()\n",
    "fx_map_all = fx_to_aud_for_tickers(prices.columns, usd_aud)\n",
    "\n",
    "# ------------- Benchmark Indices for Performance Comparison ------------------\n",
    "\n",
    "try:\n",
    "    benchmarks = [\"^AORD\", \"^GSPC\", \"^IXIC\"]  # ASX All Ords, S&P500, NASDAQ\n",
    "    start_date = prices.index[0]\n",
    "    end_date = prices.index[-1]\n",
    "\n",
    "    benchmark_data = yf.download(\n",
    "        benchmarks,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        progress=False,\n",
    "        auto_adjust=True,\n",
    "        threads=False\n",
    "    )\n",
    "\n",
    "    # Handle both multi- and single-index formats\n",
    "    if isinstance(benchmark_data.columns, pd.MultiIndex):\n",
    "        benchmark_data = benchmark_data[\"Close\"]\n",
    "\n",
    "    benchmark_data = benchmark_data.ffill().bfill()\n",
    "    print(f\"[data] Benchmarks downloaded successfully: {list(benchmark_data.columns)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[data] Skipped benchmark download: {e}\")\n",
    "    benchmark_data = pd.DataFrame()\n",
    "\n",
    "# --- Store benchmark data for downstream reporting (PowerPoint, etc.) ---\n",
    "if \"data_dict\" not in globals():\n",
    "    data_dict = {}\n",
    "\n",
    "data_dict[\"benchmark_data\"] = benchmark_data\n",
    "\n",
    "def _fetch_prices_for_new_tickers(tickers_new, base_prices, period=\"5y\"):\n",
    "    \"\"\"\n",
    "    Download prices for any genuinely new tickers and merge into base_prices.\n",
    "    Always returns a DataFrame (may be empty, but never None).\n",
    "    \"\"\"\n",
    "    # Normalise base_prices\n",
    "    if base_prices is None or not isinstance(base_prices, pd.DataFrame):\n",
    "        base_prices = pd.DataFrame()\n",
    "\n",
    "    # Work out which tickers are actually new\n",
    "    add = [t for t in map(str, tickers_new) if t not in base_prices.columns]\n",
    "    if not add:\n",
    "        return base_prices\n",
    "\n",
    "    # Download new tickers\n",
    "    dl = yf.download(add, period=period, auto_adjust=True, threads=False, progress=False)\n",
    "\n",
    "    if isinstance(dl, pd.DataFrame) and \"Close\" in dl.columns:\n",
    "        new_px = dl[\"Close\"]\n",
    "    else:\n",
    "        # yfinance may return a Series for a single ticker\n",
    "        new_px = dl if isinstance(dl, pd.Series) else pd.DataFrame()\n",
    "        if isinstance(new_px, pd.Series) and len(add) == 1:\n",
    "            new_px = new_px.to_frame(name=add[0])\n",
    "\n",
    "    if new_px is None or new_px.empty:\n",
    "        # Nothing usable came back \u2013 just return existing prices\n",
    "        return base_prices\n",
    "\n",
    "    new_px.index = pd.to_datetime(new_px.index)\n",
    "    new_px = new_px.sort_index()\n",
    "    new_px = new_px.loc[:, ~new_px.columns.duplicated()]\n",
    "\n",
    "    # If we already have a price index, align new data to it\n",
    "    if not base_prices.empty:\n",
    "        new_px = new_px.reindex(index=base_prices.index).ffill()\n",
    "\n",
    "    # Merge with base_prices\n",
    "    if base_prices.empty:\n",
    "        combined = new_px\n",
    "    else:\n",
    "        combined = base_prices.copy()\n",
    "        for c in new_px.columns:\n",
    "            if c not in combined.columns:\n",
    "                combined[c] = new_px[c]\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 4 Creating the Stock Holdings Dialog Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2) GUI portfolio editor (Tkinter) + helpers\n",
    "# -------------------------------\n",
    "import tkinter as _tk\n",
    "from tkinter import ttk as _ttk, messagebox as _mb\n",
    "\n",
    "# -------- File-based seed readers (no COM, reliable) --------\n",
    "def _read_holdings_seed_from_path(xl_path, sheet_name=\"Holdings\"):\n",
    "    try:\n",
    "        df = pd.read_excel(xl_path, sheet_name=sheet_name)\n",
    "    except Exception as e:\n",
    "        print(f\"[seed-path] holdings: {e} -> EMPTY\")\n",
    "        return pd.Series(dtype=float), {}\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty or \"Security\" not in df.columns:\n",
    "        print(\"[seed-path] holdings: empty/malformed -> EMPTY\")\n",
    "        return pd.Series(dtype=float), {}\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    df[\"Security\"] = df[\"Security\"].astype(str).str.strip()\n",
    "\n",
    "    if \"Units\" not in df.columns:\n",
    "        for alt in [\"Curr Units\", \"Current Units\", \"Holdings\", \"Qty\"]:\n",
    "            if alt in df.columns:\n",
    "                df[\"Units\"] = df[alt]\n",
    "                break\n",
    "\n",
    "    units = pd.to_numeric(df.get(\"Units\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "    if \"Include?\" in df.columns:\n",
    "        inc = df[\"Include?\"].astype(str).str.strip().str.upper().isin({\"TRUE\",\"1\",\"Y\",\"YES\",\"T\"})\n",
    "    else:\n",
    "        inc = pd.Series(True, index=df.index)\n",
    "\n",
    "    units = pd.Series(units.values, index=df[\"Security\"])\n",
    "    include = dict(zip(df[\"Security\"], inc.astype(bool)))\n",
    "    return units, include\n",
    "\n",
    "\n",
    "def _read_tilts_seed_from_path(xl_path, sheet_name=\"Tilts\"):\n",
    "    factors = list(TILT_FACTORS) if 'TILT_FACTORS' in globals() else [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\"]\n",
    "    default = pd.DataFrame(\n",
    "        {\"Target\": [1.0] + [0.0]*(len(factors)-1),\n",
    "         \"Band\":   [0.05]*len(factors),\n",
    "         \"Use?\":   [True] + [False]*(len(factors)-1)},\n",
    "        index=factors\n",
    "    )\n",
    "    try:\n",
    "        df = pd.read_excel(xl_path, sheet_name=sheet_name)\n",
    "    except Exception as e:\n",
    "        print(f\"[seed-path] tilts: {e} -> DEFAULTS\")\n",
    "        return default\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        print(\"[seed-path] tilts: empty -> DEFAULTS\")\n",
    "        return default\n",
    "\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    need = {\"Factor\",\"Target\",\"Band\",\"Use?\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        print(\"[seed-path] tilts: malformed -> DEFAULTS\")\n",
    "        return default\n",
    "\n",
    "    df[\"Factor\"] = df[\"Factor\"].astype(str).str.strip()\n",
    "    df = df.set_index(\"Factor\").reindex(factors)\n",
    "    out = default.copy()\n",
    "    out.loc[df.index, \"Target\"] = pd.to_numeric(df[\"Target\"], errors=\"coerce\")\n",
    "    out.loc[df.index, \"Band\"]   = pd.to_numeric(df[\"Band\"],   errors=\"coerce\")\n",
    "    out.loc[df.index, \"Use?\"]   = df[\"Use?\"].astype(str).str.upper().isin([\"TRUE\",\"1\",\"Y\",\"YES\",\"T\"])\n",
    "    out[\"Target\"] = out[\"Target\"].fillna(default[\"Target\"]).astype(float)\n",
    "    out[\"Band\"]   = out[\"Band\"].fillna(default[\"Band\"]).astype(float)\n",
    "    return out.reindex(factors)\n",
    "\n",
    "# -------------------------------\n",
    "# Combined dialog (one window)\n",
    "# -------------------------------\n",
    "def edit_holdings_and_tilts_dialog(prices, exclude, seed_units, seed_include, seed_tilts,\n",
    "                                   title=\"Edit Holdings & Factor Tilts\"):\n",
    "        \"\"\"\n",
    "        Returns: (units_series, last_price_series, prices_df, include_flags_dict, tilts_df)\n",
    "        \"\"\"\n",
    "        tickers_all = [t for t in prices.columns if t != \"PortfolioValue\"]\n",
    "        exclude = set(exclude or [])\n",
    "        last_px = prices.ffill().iloc[-1]\n",
    "    \n",
    "        # factor list (use global TILT_FACTORS if set, so MOM shows up)\n",
    "        factors = list(seed_tilts.index) if isinstance(seed_tilts, pd.DataFrame) and not seed_tilts.empty \\\n",
    "                  else (list(TILT_FACTORS) if 'TILT_FACTORS' in globals() else [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"MOM\"])\n",
    "        if not isinstance(seed_tilts, pd.DataFrame) or seed_tilts.empty:\n",
    "            seed_tilts = pd.DataFrame(\n",
    "                {\"Target\": [1.0] + [0.0]*(len(factors)-1),\n",
    "                 \"Band\":   [0.05]*len(factors),\n",
    "                 \"Use?\":   [True] + [False]*(len(factors)-1)},\n",
    "                index=factors\n",
    "            )\n",
    "    \n",
    "        root = _tk.Tk()\n",
    "        root.title(title)\n",
    "        root.geometry(\"980x640\")\n",
    "        root.minsize(920, 560)\n",
    "    \n",
    "        # === Main layout ===\n",
    "        frm_main = _ttk.Frame(root, padding=10); frm_main.pack(fill=\"both\", expand=True)\n",
    "    \n",
    "        # Left: holdings\n",
    "        frm_left = _ttk.LabelFrame(frm_main, text=\"Holdings\", padding=10)\n",
    "        frm_left.pack(side=\"left\", fill=\"both\", expand=True, padx=(0, 6))\n",
    "        for i in range(3):\n",
    "            frm_left.rowconfigure(i, weight=(1 if i == 1 else 0))\n",
    "        frm_left.columnconfigure(0, weight=1)\n",
    "    \n",
    "        # Header\n",
    "        header = _ttk.Frame(frm_left); header.grid(row=0, column=0, sticky=\"ew\")\n",
    "        _ttk.Label(header, text=\"Inc?\", width=5).grid(row=0, column=0, sticky=\"w\")\n",
    "        _ttk.Label(header, text=\"Del?\", width=5).grid(row=0, column=1, sticky=\"w\")\n",
    "        _ttk.Label(header, text=\"Security\", width=20).grid(row=0, column=2, sticky=\"w\")\n",
    "        _ttk.Label(header, text=\"Units\", width=14).grid(row=0, column=3, sticky=\"w\")\n",
    "        _ttk.Label(header, text=\"Last Price\", width=12).grid(row=0, column=4, sticky=\"w\")\n",
    "    \n",
    "        # Scrollable list\n",
    "        list_container = _ttk.Frame(frm_left); list_container.grid(row=1, column=0, sticky=\"nsew\", pady=(4, 6))\n",
    "        list_container.rowconfigure(0, weight=1); list_container.columnconfigure(0, weight=1)\n",
    "        canvas = _tk.Canvas(list_container, highlightthickness=0)\n",
    "        scroll_y = _ttk.Scrollbar(list_container, orient=\"vertical\", command=canvas.yview)\n",
    "        body = _ttk.Frame(canvas)\n",
    "        body.bind(\"<Configure>\", lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\")))\n",
    "        canvas.create_window((0, 0), window=body, anchor=\"nw\")\n",
    "        canvas.configure(yscrollcommand=scroll_y.set)\n",
    "        canvas.grid(row=0, column=0, sticky=\"nsew\"); scroll_y.grid(row=0, column=1, sticky=\"ns\")\n",
    "    \n",
    "        def _on_mousewheel(event):\n",
    "            if event.delta:\n",
    "                canvas.yview_scroll(int(-1*(event.delta/120)), \"units\")\n",
    "            elif getattr(event, \"num\", None) in (4, 5):\n",
    "                canvas.yview_scroll(-1 if event.num == 4 else 1, \"units\")\n",
    "        body.bind(\"<Enter>\", lambda e: canvas.bind_all(\"<MouseWheel>\", _on_mousewheel))\n",
    "        body.bind(\"<Leave>\", lambda e: canvas.unbind_all(\"<MouseWheel>\"))\n",
    "        canvas.bind_all(\"<Button-4>\", _on_mousewheel); canvas.bind_all(\"<Button-5>\", _on_mousewheel)\n",
    "    \n",
    "        row_vars = {}\n",
    "        def _add_row(ticker, units_default=0.0, include_default=True, disabled=False):\n",
    "            r = len(row_vars) + 1\n",
    "            v_inc = _tk.BooleanVar(value=(False if disabled else bool(include_default)))\n",
    "            v_del = _tk.BooleanVar(value=False)\n",
    "            v_units = _tk.StringVar(value=(\"0\" if disabled else str(float(units_default))))\n",
    "            chk_inc = _ttk.Checkbutton(body, variable=v_inc)\n",
    "            chk_del = _ttk.Checkbutton(body, variable=v_del)\n",
    "            ent = _ttk.Entry(body, textvariable=v_units, width=16)\n",
    "            lbl_t = _ttk.Label(body, text=str(ticker), width=20)\n",
    "            last_px_str = f\"{float(last_px.get(ticker, float('nan'))):.4f}\"\n",
    "            lbl_px = _ttk.Label(body, text=last_px_str, width=12)\n",
    "            if disabled:\n",
    "                chk_inc.state([\"disabled\"]); ent.state([\"disabled\"]); lbl_t.configure(foreground=\"#888\")\n",
    "            chk_inc.grid(row=r, column=0, sticky=\"w\", padx=(0, 6), pady=2)\n",
    "            chk_del.grid(row=r, column=1, sticky=\"w\", padx=(0, 6), pady=2)\n",
    "            lbl_t.grid(row=r, column=2, sticky=\"w\", padx=(0, 6), pady=2)\n",
    "            ent.grid(row=r, column=3, sticky=\"w\", padx=(0, 6), pady=2)\n",
    "            lbl_px.grid(row=r, column=4, sticky=\"w\", padx=(0, 6), pady=2)\n",
    "            row_vars[ticker] = {\"inc\": v_inc, \"del\": v_del, \"units\": v_units, \"disabled\": disabled, \"lbl_px\": lbl_px}\n",
    "    \n",
    "        # Prefill rows\n",
    "        for t in tickers_all:\n",
    "            disabled = (t in exclude)\n",
    "            inc_default = bool(pd.Series(seed_include).get(t, True)) and not disabled\n",
    "            units_default = float(pd.Series(seed_units).get(t, 0.0))\n",
    "            _add_row(t, units_default=units_default, include_default=inc_default, disabled=disabled)\n",
    "    \n",
    "        # Add-holding box\n",
    "        add_box = _ttk.LabelFrame(frm_left, text=\"Add holding\", padding=10)\n",
    "        add_box.grid(row=2, column=0, sticky=\"ew\")\n",
    "        _ttk.Label(add_box, text=\"Ticker\").grid(row=0, column=0, sticky=\"w\")\n",
    "        ent_new_ticker = _ttk.Entry(add_box, width=18); ent_new_ticker.grid(row=0, column=1, sticky=\"w\", padx=(4, 12))\n",
    "        _ttk.Label(add_box, text=\"Units\").grid(row=0, column=2, sticky=\"w\")\n",
    "        ent_new_units = _ttk.Entry(add_box, width=14); ent_new_units.grid(row=0, column=3, sticky=\"w\", padx=(4, 12))\n",
    "        _btn_add = _ttk.Button(add_box, text=\"Add\"); _btn_add.grid(row=0, column=4, sticky=\"w\")\n",
    "        added_tickers = []\n",
    "        def _do_add():\n",
    "            t = ent_new_ticker.get().strip()\n",
    "            if not t:\n",
    "                _mb.showwarning(\"Add holding\", \"Please enter a ticker.\"); return\n",
    "            t = t.upper()\n",
    "            if t in row_vars:\n",
    "                _mb.showinfo(\"Add holding\", f\"{t} already listed.\"); return\n",
    "            try:\n",
    "                u = float(ent_new_units.get().strip()) if ent_new_units.get().strip() else 0.0\n",
    "            except ValueError:\n",
    "                _mb.showwarning(\"Add holding\", \"Units must be numeric.\"); return\n",
    "            _add_row(t, units_default=u, include_default=True, disabled=(t in exclude))\n",
    "            added_tickers.append(t)\n",
    "            ent_new_ticker.delete(0, _tk.END); ent_new_units.delete(0, _tk.END)\n",
    "        _btn_add.configure(command=_do_add)\n",
    "    \n",
    "        # Right panel \u2014 Factor Tilts\n",
    "        frm_right = _ttk.LabelFrame(frm_main, text=\"Factor Tilts\", padding=10)\n",
    "        frm_right.pack(side=\"right\", fill=\"y\", padx=(6, 0))\n",
    "        _ttk.Label(frm_right, text=\"Use?\",    width=5 ).grid(row=0, column=0, sticky=\"w\")\n",
    "        _ttk.Label(frm_right, text=\"Factor\",  width=12).grid(row=0, column=1, sticky=\"w\")\n",
    "        _ttk.Label(frm_right, text=\"Target \u03b2\",width=10).grid(row=0, column=2, sticky=\"w\")\n",
    "        _ttk.Label(frm_right, text=\"Band\",    width=10).grid(row=0, column=3, sticky=\"w\")\n",
    "    \n",
    "        tilt_vars = {}\n",
    "        for i, f in enumerate(factors, start=1):\n",
    "            use_default  = bool(seed_tilts.loc[f, \"Use?\"])   if f in seed_tilts.index else False\n",
    "            tgt_default  = float(seed_tilts.loc[f, \"Target\"]) if f in seed_tilts.index else 0.0\n",
    "            band_default = float(seed_tilts.loc[f, \"Band\"])   if f in seed_tilts.index else 0.05\n",
    "            v_use = _tk.BooleanVar(value=use_default)\n",
    "            v_tgt = _tk.StringVar(value=f\"{tgt_default:.3f}\")\n",
    "            v_bnd = _tk.StringVar(value=f\"{band_default:.3f}\")\n",
    "            _ttk.Checkbutton(frm_right, variable=v_use).grid(row=i, column=0, sticky=\"w\", pady=2)\n",
    "            _ttk.Label(frm_right, text=f, width=12).grid(row=i, column=1, sticky=\"w\", pady=2)\n",
    "            _ttk.Entry(frm_right, textvariable=v_tgt, width=10).grid(row=i, column=2, sticky=\"w\", pady=2)\n",
    "            _ttk.Entry(frm_right, textvariable=v_bnd, width=10).grid(row=i, column=3, sticky=\"w\", pady=2)\n",
    "            tilt_vars[f] = (v_use, v_tgt, v_bnd)\n",
    "    \n",
    "        # after fac_cols/f_mean_ann are available (you have them earlier), pass them in or recompute locally.\n",
    "        def _compute_recommended_tilts():\n",
    "            \"\"\"Compute achievable factor-tilt targets using the tickers currently included in the optimiser.\"\"\"\n",
    "            try:\n",
    "                lookback = int(globals().get(\"FF5_LOOKBACK_DAYS\", 252))\n",
    "                ff = get_ff5_mom_daily().tail(lookback)\n",
    "                fac_cols = [c for c in ff.columns if c != \"RF\"]\n",
    "                Fcov_daily = ff[fac_cols].cov()\n",
    "                f_mean_ann = ff[fac_cols].mean() * 252.0\n",
    "\n",
    "                # limit to tickers currently included (and not marked for deletion)\n",
    "                incl = [t for t, vs in row_vars.items() if vs[\"inc\"].get() and (not vs[\"del\"].get())]\n",
    "                if len(incl) == 0:\n",
    "                    incl = list(B.index)\n",
    "\n",
    "                B_sub = B.reindex(incl).dropna(how=\"any\")\n",
    "                if B_sub.empty:\n",
    "                    B_sub = B.dropna(how=\"any\")\n",
    "\n",
    "                reco, _wtilt = recommend_factor_tilts_achievable(B_sub, f_mean_ann, Fcov_daily)\n",
    "\n",
    "                # ensure we return values for all factors in the dialog order\n",
    "                return reco.reindex(list(seed_tilts.index)).fillna(0.0)\n",
    "            except Exception:\n",
    "                return pd.Series(0.0, index=list(seed_tilts.index))\n",
    "        \n",
    "        def _apply_recommended_tilts():\n",
    "            rec = _compute_recommended_tilts()\n",
    "            for f in factors:\n",
    "                v_use, v_tgt, v_bnd = tilt_vars[f]\n",
    "                v_use.set(True)\n",
    "                v_tgt.set(f\"{float(rec.get(f,0.0)):.3f}\")\n",
    "                # keep user band or set to a gentle default:\n",
    "                if not v_bnd.get():\n",
    "                    v_bnd.set(\"0.200\")\n",
    "            _mb.showinfo(\"Tilts\", \"Recommended tilts applied.\\n(You can still edit before Save.)\")\n",
    "        \n",
    "        btn_reco = _ttk.Button(frm_right, text=\"Auto-recommend tilts\", command=_apply_recommended_tilts)\n",
    "        btn_reco.grid(row=len(factors)+2, column=0, columnspan=4, sticky=\"ew\", pady=(12,0))\n",
    "    \n",
    "        # Buttons\n",
    "        def _reset_to_seed_units():\n",
    "            su = pd.Series(seed_units).astype(float)  # seed_units is already a parameter to this function\n",
    "            for t, vs in row_vars.items():\n",
    "                if vs.get(\"disabled\"):\n",
    "                    continue\n",
    "                vs[\"units\"].set(str(int(round(su.get(t, 0.0)))))\n",
    "            _mb.showinfo(\"Holdings\", \"Units reset to the values loaded from Excel at the start of this run.\")\n",
    "    \n",
    "        \n",
    "        frm_btns = _ttk.Frame(root, padding=(10, 0, 10, 10)); frm_btns.pack(fill=\"x\")\n",
    "        _ttk.Button(frm_btns, text=\"Reset to Seed\", command=_reset_to_seed_units).pack(side=\"left\", padx=6)\n",
    "        _ttk.Button(frm_btns, text=\"Cancel\", command=root.destroy).pack(side=\"right\", padx=6)\n",
    "    \n",
    "        def _on_save():\n",
    "            nonlocal prices\n",
    "            if added_tickers:\n",
    "                prices = _fetch_prices_for_new_tickers(added_tickers, prices)\n",
    "    \n",
    "            to_delete = []\n",
    "            units_out, include_flags = {}, {}\n",
    "            for t, vs in row_vars.items():\n",
    "                mark_delete = bool(vs[\"del\"].get())\n",
    "                disabled = vs[\"disabled\"]\n",
    "                inc = bool(vs[\"inc\"].get()) and not disabled and not mark_delete\n",
    "                include_flags[t] = inc\n",
    "                if mark_delete:\n",
    "                    to_delete.append(t)\n",
    "                    continue\n",
    "                if not disabled:\n",
    "                    txt = vs[\"units\"].get().strip()\n",
    "                    try:\n",
    "                        val = float(txt) if txt else 0.0\n",
    "                    except ValueError:\n",
    "                        val = 0.0\n",
    "                    units_out[t] = val\n",
    "                # refresh last px label\n",
    "                if isinstance(prices, pd.DataFrame) and t in prices.columns:\n",
    "                    try:\n",
    "                        lp = float(prices.ffill().iloc[-1].get(t, float('nan')))\n",
    "                        vs[\"lbl_px\"].configure(text=f\"{lp:.4f}\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    \n",
    "            # remove deleted names from the price panel so they don\u2019t enter \u03bc/\u03a3\n",
    "            if to_delete:\n",
    "                keep = [c for c in prices.columns if c not in set(to_delete)]\n",
    "                prices = prices.reindex(columns=keep)\n",
    "    \n",
    "            units_ser = pd.Series(units_out, dtype=float)\n",
    "            last_price_ser = prices.ffill().iloc[-1].reindex(units_ser.index)\n",
    "    \n",
    "            out_rows = []\n",
    "            for f, (v_use, v_tgt, v_bnd) in tilt_vars.items():\n",
    "                try:  tgt = float(v_tgt.get())\n",
    "                except ValueError: tgt = 0.0\n",
    "                try:  bnd = float(v_bnd.get())\n",
    "                except ValueError: bnd = 0.05\n",
    "                out_rows.append({\"Factor\": f, \"Target\": tgt, \"Band\": bnd, \"Use?\": bool(v_use.get())})\n",
    "            tilts_df = pd.DataFrame(out_rows).set_index(\"Factor\").reindex(factors)\n",
    "    \n",
    "            edit_holdings_and_tilts_dialog.result = (units_ser, last_price_ser, prices, include_flags, tilts_df)\n",
    "            root.destroy()\n",
    "    \n",
    "        _ttk.Button(frm_btns, text=\"Save\", command=_on_save).pack(side=\"right\", padx=6)\n",
    "        root.protocol(\"WM_DELETE_WINDOW\", root.destroy)\n",
    "        root.mainloop()\n",
    "        return getattr(edit_holdings_and_tilts_dialog, \"result\", None)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 5 Creating the Covariance Matrix and the Rest of the OPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FX-adjusted returns for Sigma?: True (max |diff|=1.19e-18)\n",
      "[frontier] target_returns from 3.6000% to 38.7938%\n",
      "[frontier] tangency ret \u2248 0.2398, vol \u2248 0.0384\n",
      "\n",
      "--- DEBUG CHECK: Sigma_opt / mu_vec_opt ---\n",
      "Any NaN in Sigma_opt: False\n",
      "Any NaN in mu_vec_opt: False\n",
      "Min variance: 3.995168666652039e-05\n",
      "Number of assets: 22\n",
      "Security       A200.AX        IEU.AX    IJP.AX        IOO.AX        IVV.AX  \\\n",
      "Security                                                                     \n",
      "A200.AX   5.683011e-05  5.038013e-07  0.000005  1.611108e-06  1.772164e-06   \n",
      "IEU.AX    5.038013e-07  4.698376e-05  0.000003  6.523047e-07  5.674787e-07   \n",
      "IJP.AX    4.730669e-06  2.852719e-06  0.000147  5.379795e-06  5.374538e-06   \n",
      "IOO.AX    1.611108e-06  6.523047e-07  0.000005  8.572049e-05  2.450214e-06   \n",
      "IVV.AX    1.772164e-06  5.674787e-07  0.000005  2.450214e-06  8.111680e-05   \n",
      "\n",
      "Security       MTUM.AX        MVW.AX       QHAL.AX       QLTY.AX   QUAL.AX  \\\n",
      "Security                                                                     \n",
      "A200.AX   2.083685e-06  1.110977e-06  1.213388e-06  1.866869e-06  0.000003   \n",
      "IEU.AX    9.235311e-07  2.597739e-07  3.615485e-07  9.909293e-07  0.000001   \n",
      "IJP.AX    7.397586e-06  2.101779e-06  3.109079e-06  6.847022e-06  0.000010   \n",
      "IOO.AX    2.579872e-06  1.372276e-06  1.924900e-06  2.291836e-06  0.000004   \n",
      "IVV.AX    2.433951e-06  2.183474e-06  2.099531e-06  2.542478e-06  0.000004   \n",
      "\n",
      "Security  ...       SPY        VAS.AX       VDHG.AX        VGE.AX  \\\n",
      "Security  ...                                                       \n",
      "A200.AX   ...  0.000008  1.573900e-06  1.280536e-06  2.262339e-07   \n",
      "IEU.AX    ...  0.000005  5.835882e-07  4.790406e-07  4.678880e-07   \n",
      "IJP.AX    ...  0.000036  5.013109e-06  3.954762e-06  2.095227e-06   \n",
      "IOO.AX    ...  0.000006  1.647351e-06  1.600626e-06 -3.088979e-07   \n",
      "IVV.AX    ...  0.000003  1.816050e-06  1.810077e-06 -7.459340e-07   \n",
      "\n",
      "Security        VGS.AX       VLU   VLUE.AX       VMIN.AX        VSO.AX  \\\n",
      "Security                                                                 \n",
      "A200.AX   1.711862e-06  0.000010  0.000003  3.823908e-07  1.121224e-06   \n",
      "IEU.AX    7.627246e-07  0.000006  0.000002 -1.685601e-07  4.705509e-07   \n",
      "IJP.AX    5.815403e-06  0.000042  0.000011 -1.695528e-07  3.281302e-06   \n",
      "IOO.AX    2.267024e-06  0.000008  0.000004  5.073887e-07  1.029984e-06   \n",
      "IVV.AX    2.583774e-06  0.000006  0.000004  7.847282e-07  1.317016e-06   \n",
      "\n",
      "Security   VVLU.AX  \n",
      "Security            \n",
      "A200.AX   0.000003  \n",
      "IEU.AX    0.000002  \n",
      "IJP.AX    0.000011  \n",
      "IOO.AX    0.000004  \n",
      "IVV.AX    0.000004  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Security\n",
      "A200.AX    0.167887\n",
      "IEU.AX     0.209687\n",
      "IJP.AX     0.258128\n",
      "IOO.AX     0.335189\n",
      "IVV.AX     0.289838\n",
      "dtype: float64\n",
      "OPT TICKERS: ['A200.AX', 'IEU.AX', 'IJP.AX', 'IOO.AX', 'IVV.AX', 'MTUM.AX', 'MVW.AX', 'QHAL.AX', 'QLTY.AX', 'QUAL.AX', 'SMH', 'SMLL.AX', 'SPY', 'VAS.AX', 'VDHG.AX', 'VGE.AX', 'VGS.AX', 'VLU', 'VLUE.AX', 'VMIN.AX', 'VSO.AX', 'VVLU.AX']\n",
      "mu: count    22.000000\n",
      "mean      0.254167\n",
      "std       0.111650\n",
      "min       0.138002\n",
      "25%       0.203873\n",
      "50%       0.234729\n",
      "75%       0.269497\n",
      "max       0.667540\n",
      "dtype: float64\n",
      "Sigma diag min/max: 3.995168666652039e-05 0.0006897502882252507\n",
      "Mkt-RF    0.258678\n",
      "SMB      -0.039122\n",
      "HML       0.020428\n",
      "RMW      -0.058772\n",
      "CMA      -0.042272\n",
      "MOM       0.136678\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Analytics helpers (moved from Block 4) ===================================\n",
    "gamma_cgt = 1.0\n",
    "rf_label = f\"{rf_annual*100:.2f}%\"\n",
    "chart_title = f\"Efficient Frontier & CAL (rf={rf_label})\"\n",
    "\n",
    "def holdings_portfolio_returns(prices: pd.DataFrame, units: pd.Series) -> pd.Series:\n",
    "\n",
    "    units = pd.Series(units).reindex(prices.columns).fillna(0.0)\n",
    "    if units.abs().sum() == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    px = prices.reindex(columns=units.index).ffill()\n",
    "    port_val = (px * units.values).sum(axis=1)\n",
    "    ret = port_val.pct_change(fill_method=None)\n",
    "    return ret.dropna()\n",
    "\n",
    "def current_holdings_weights(units: pd.Series,\n",
    "                             last_prices: pd.Series,\n",
    "                             investable: list[str],\n",
    "                             fx_to_aud: pd.Series | float | None = None) -> pd.Series:\n",
    "    # FX handling\n",
    "    if isinstance(fx_to_aud, pd.Series):\n",
    "        fx = fx_to_aud.reindex(units.index).fillna(1.0)\n",
    "    else:\n",
    "        fx = 1.0\n",
    "\n",
    "    mv = (pd.Series(units, dtype=float) * pd.Series(last_prices, dtype=float) * fx)\n",
    "    mv = mv.reindex(investable).fillna(0.0)\n",
    "    den = mv.sum()\n",
    "    return (mv / den) if den > 0 else mv\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) COVARIANCE MATRIX (daily)\n",
    "# ------------------------------------------------------------\n",
    "df_cov_wide = (\n",
    "    df_melt[['Date','Security','Return']]\n",
    "    .pivot(index='Date', columns='Security', values='Return')\n",
    ")\n",
    "\n",
    "Sigma_daily = df_cov_wide.cov()  # DAILY cov (AUD returns)\n",
    "\n",
    "# (Optional) sanity that Sigma came from AUD-converted prices\n",
    "Sigma_from_aud = (\n",
    "    pd.melt(prices_aud_for_returns.reset_index(), id_vars=\"Date\",\n",
    "            var_name=\"Security\", value_name=\"Close\")\n",
    "      .sort_values([\"Security\",\"Date\"])\n",
    "      .assign(Return=lambda d: d.groupby(\"Security\")[\"Close\"].pct_change(fill_method=None))\n",
    "      .pivot(index=\"Date\", columns=\"Security\", values=\"Return\")\n",
    "      .cov()\n",
    ").reindex(index=Sigma_daily.index, columns=Sigma_daily.columns)\n",
    "max_abs_diff = (Sigma_daily - Sigma_from_aud).abs().to_numpy().max()\n",
    "using_fx = np.allclose(Sigma_daily.to_numpy(), Sigma_from_aud.to_numpy(), rtol=0, atol=1e-12)\n",
    "print(f\"Using FX-adjusted returns for Sigma?: {using_fx} (max |diff|={max_abs_diff:.2e})\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) GEOMETRIC (LOG-BASED) EXPECTED RETURNS (annual) \u2014 sample \u03bc\n",
    "# ------------------------------------------------------------\n",
    "df_melt['LogRet'] = np.log1p(df_melt['Return'])\n",
    "mu_log_ann = df_melt.groupby('Security')['LogRet'].mean() * 252.0\n",
    "mu_ann_geo = np.expm1(mu_log_ann)  # ANNUAL (geom)\n",
    "\n",
    "# Align\n",
    "securities_all = [s for s in Sigma_daily.columns if s != \"PortfolioValue\"]\n",
    "Sigma_daily = Sigma_daily.loc[securities_all, securities_all]\n",
    "mu_vec_all = mu_ann_geo.reindex(securities_all)\n",
    "\n",
    "valid_all = [s for s in securities_all\n",
    "             if pd.notna(mu_vec_all.get(s, np.nan)) and pd.notna(Sigma_daily.loc[s, s])]\n",
    "Sigma_daily = Sigma_daily.loc[valid_all, valid_all]\n",
    "mu_vec_all = mu_vec_all.reindex(valid_all)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) FF5 Helper functions and definitions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def compute_ff5_betas(df_cov_wide, ff5_returns, min_obs=120):\n",
    "    \"\"\"\n",
    "    Estimate FF5+MOM betas for each security using rolling regression.\n",
    "    df_cov_wide: wide daily returns (Date index \u00d7 securities)\n",
    "    ff5_returns: FF5+MOM factor returns (Date index \u00d7 factors)\n",
    "    \"\"\"\n",
    "    # Align data\n",
    "    joined = df_cov_wide.join(ff5_returns, how=\"inner\").dropna()\n",
    "    if joined.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    securities = df_cov_wide.columns\n",
    "    factors = [c for c in ff5_returns.columns if c != \"RF\"]\n",
    "\n",
    "    B = pd.DataFrame(index=securities, columns=factors, dtype=float)\n",
    "    alpha_daily = pd.Series(index=securities, dtype=float)\n",
    "    resid_var = pd.Series(index=securities, dtype=float)\n",
    "\n",
    "    for sec in securities:\n",
    "        y = joined[sec]\n",
    "        X = joined[factors]\n",
    "\n",
    "        if y.count() < min_obs:\n",
    "            continue\n",
    "\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X, missing=\"drop\").fit()\n",
    "\n",
    "        alpha_daily.loc[sec] = model.params.get(\"const\", float(\"nan\"))\n",
    "        resid_var.loc[sec] = model.resid.var()\n",
    "\n",
    "        for f in factors:\n",
    "            B.loc[sec, f] = model.params.get(f, float(\"nan\"))\n",
    "\n",
    "    return B, alpha_daily, resid_var\n",
    "\n",
    "def compute_factor_feasible_ranges(B, include_flags, factor_order=None):\n",
    "    \"\"\"\n",
    "    Compute feasible min/max beta ranges for each factor in B under:\n",
    "        - long-only weights\n",
    "        - sum(w)=1\n",
    "        - include_flags filtering\n",
    "\n",
    "    B: DataFrame of betas (index = tickers, columns = factors)\n",
    "    include_flags: dict {ticker: True/False} selecting usable securities\n",
    "    factor_order: list of factor names for row order (optional)\n",
    "\n",
    "    Returns: DataFrame with columns: [\"Min \u03b2\", \"Max \u03b2\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to included tickers\n",
    "    tickers = [t for t in B.index if include_flags.get(t, False)]\n",
    "    if not tickers:\n",
    "        return pd.DataFrame(columns=[\"Min \u03b2\",\"Max \u03b2\"])\n",
    "\n",
    "    B_sub = B.loc[tickers]\n",
    "\n",
    "    factors = factor_order if factor_order else list(B_sub.columns)\n",
    "    out = {}\n",
    "\n",
    "    n = len(tickers)\n",
    "    bounds = [(0,1)] * n\n",
    "    cons = ({\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0},)\n",
    "    w0 = np.full(n, 1/n)\n",
    "\n",
    "    for f in factors:\n",
    "        b_vec = B_sub[f].to_numpy(dtype=float)\n",
    "\n",
    "        def obj_min(w):\n",
    "            return float(np.dot(w, b_vec))\n",
    "\n",
    "        def obj_max(w):\n",
    "            return -float(np.dot(w, b_vec))\n",
    "\n",
    "        res_min = minimize(obj_min, w0, method=\"SLSQP\",\n",
    "                           bounds=bounds, constraints=cons,\n",
    "                           options={\"maxiter\": 500, \"ftol\": 1e-9})\n",
    "        beta_min = float(res_min.fun) if res_min.success else np.nan\n",
    "\n",
    "        res_max = minimize(obj_max, w0, method=\"SLSQP\",\n",
    "                           bounds=bounds, constraints=cons,\n",
    "                           options={\"maxiter\": 500, \"ftol\": 1e-9})\n",
    "        beta_max = -float(res_max.fun) if res_max.success else np.nan\n",
    "\n",
    "        out[f] = {\"Min \u03b2\": beta_min, \"Max \u03b2\": beta_max}\n",
    "\n",
    "    df = pd.DataFrame(out).T[[\"Min \u03b2\",\"Max \u03b2\"]]\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Choose \u03bc and \u03a3 source for the optimiser\n",
    "#       - Sample: Sigma_daily (DAILY), mu_ann_geo (ANNUAL)\n",
    "#       - FF5:    Sigma_ff (DAILY),   mu_ff_ann (ANNUAL)\n",
    "# ------------------------------------------------------------\n",
    "# Use the pre-downloaded FF5+MOM factors to estimate betas\n",
    "B, alpha_daily, resid_var = compute_ff5_betas(\n",
    "    df_cov_wide,\n",
    "    ff5_win_for_betas,\n",
    "    min_obs=120,\n",
    ")\n",
    "\n",
    "# Build an AUD-adjusted factor set from the pre-downloaded factors + FX excess return\n",
    "def get_ff5_mom_aud():\n",
    "    ff = ff5_raw.copy()\n",
    "    ff = ff.loc[:, ~ff.columns.duplicated()]\n",
    "    # fx_ret is the currency excess return series computed in Block 2/3\n",
    "    fx_series = fx_ret.reindex(ff.index).fillna(0.0).squeeze()\n",
    "    for col in ff.columns:\n",
    "        if col != \"RF\":  # RF stays untouched\n",
    "            ff[col] = ff[col].add(fx_series, axis=0)\n",
    "    return ff\n",
    "\n",
    "WINDOW = 504   # 2 years of daily data\n",
    "USE_FF5 = True  # flip to False to use sample moments\n",
    "\n",
    "if USE_FF5 and (B is not None) and not B.empty:\n",
    "    \n",
    "    # 1) Get AUD-adjusted FF5+MOM factors\n",
    "    ff_aud = get_ff5_mom_aud()\n",
    "    \n",
    "    # 2) Match window for factors + equity returns\n",
    "    ff5_win = ff_aud.tail(WINDOW)\n",
    "    fac_cols = [c for c in ff5_win.columns if c != \"RF\"]\n",
    "    \n",
    "    # 3) Annualised factor premia\n",
    "    f_mean_ann = ff5_win[fac_cols].mean() * 252.0\n",
    "    \n",
    "    # 4) Regressions were already run above \u2192 B (betas) and alpha_daily exist\n",
    "    alpha_ann = alpha_daily * 252.0\n",
    "    \n",
    "    # 5) Proper expected-return model in AUD space:\n",
    "    mu_ff_ann = (\n",
    "        alpha_ann.reindex(B.index).fillna(0.0)\n",
    "        + B @ f_mean_ann\n",
    "        + float(rf_annual)\n",
    "    )\n",
    "    \n",
    "    # 6) Define optimisation set\n",
    "    securities_opt = [t for t in B.index if t not in EXCLUDE_FROM_OPT]\n",
    "    Sigma_ff_daily = (B @ ff5_win[fac_cols].cov() @ B.T) + np.diag(\n",
    "        resid_var.reindex(B.index).clip(lower=0).fillna(0)\n",
    "    )\n",
    "    Sigma_opt = Sigma_ff_daily.loc[securities_opt, securities_opt]\n",
    "    mu_vec_opt = mu_ff_ann.reindex(securities_opt)\n",
    "    \n",
    "    exp_ret_label = \"Expected Return (annual, FF5 AUD-adjusted)\"\n",
    "\n",
    "else:\n",
    "\n",
    "    securities_all = [s for s in Sigma_daily.columns if s != \"PortfolioValue\"]\n",
    "    mu_vec_all = mu_ann_geo.reindex(securities_all)\n",
    "\n",
    "    valid_all = [\n",
    "        s for s in securities_all\n",
    "        if pd.notna(mu_vec_all.get(s)) and pd.notna(Sigma_daily.loc[s, s])\n",
    "    ]\n",
    "\n",
    "    securities_opt = [s for s in valid_all if s not in EXCLUDE_FROM_OPT]\n",
    "    Sigma_opt = Sigma_daily.loc[securities_opt, securities_opt]\n",
    "    mu_vec_opt = mu_vec_all.reindex(securities_opt)\n",
    "\n",
    "    exp_ret_label = \"Expected Return (ann., geom)\"\n",
    "\n",
    "    # Force frontier and statistics table to use the same Sigma & mu source\n",
    "    Sigma_frontier = Sigma_opt.copy()\n",
    "    mu_frontier    = mu_vec_opt.copy()\n",
    "    mu_plus = mu_vec_opt.copy()\n",
    "    cov_plus = Sigma_opt.copy()\n",
    "\n",
    "\n",
    "    # === RECOMMENDED FACTOR TILTS ================================================\n",
    "    # Compute an achievable recommendation based on your current investable set\n",
    "    tilt_reco_achievable, w_tilt = recommend_factor_tilts_achievable(B, f_mean_ann, Fcov_daily)\n",
    "\n",
    "    def recommend_factor_tilts(f_mean_ann, Fcov_daily, normalise=True):\n",
    "        \"\"\"\n",
    "        Recommend optimal factor tilts given estimated factor premia and covariance.\n",
    "        Returns a Series of recommended beta targets for each factor.\n",
    "        \"\"\"\n",
    "        fac = f_mean_ann.index\n",
    "        mu = f_mean_ann.values\n",
    "        Sigma = Fcov_daily.loc[fac, fac].values\n",
    "    \n",
    "        # Mean\u2013variance optimal factor exposure vector: \u03a3\u207b\u00b9 \u03bc\n",
    "        Sigma_inv = np.linalg.pinv(Sigma)\n",
    "        t_opt = Sigma_inv @ mu\n",
    "    \n",
    "        # Normalise so that Market \u03b2 = 1 (interpretable scaling)\n",
    "        if normalise and \"Mkt-RF\" in fac:\n",
    "            t_opt = t_opt / t_opt[list(fac).index(\"Mkt-RF\")]\n",
    "    \n",
    "        return pd.Series(t_opt, index=fac, name=\"Recommended \u03b2\")\n",
    "    \n",
    "    # Compute and display recommended tilts\n",
    "    tilt_reco = recommend_factor_tilts(f_mean_ann, Fcov_daily)\n",
    "    print(\"\\nRecommended factor tilts (based on current factor premia):\")\n",
    "    print(tilt_reco.round(3))\n",
    "    # ==============================================================================\n",
    "\n",
    "# Display tables (once \u03bc/\u03a3 are final)\n",
    "n_opt = len(securities_opt)\n",
    "cov_plus = pd.DataFrame(0.0, index=securities_opt + ['w'], columns=securities_opt + ['w'])\n",
    "cov_plus.iloc[:n_opt, :n_opt] = Sigma_opt.values\n",
    "exp_ret_df = mu_vec_opt.rename(exp_ret_label).to_frame()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) OPTIMISATION UTILITIES (unconstrained + tilt-constrained)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def solve_frontier_point_cvxpy(\n",
    "    mu: pd.Series,\n",
    "    Sigma: pd.DataFrame,\n",
    "    target_return: float,\n",
    "    *,\n",
    "    use_inequality: bool = True\n",
    ") -> tuple[np.ndarray, bool, str]:\n",
    "    \"\"\"\n",
    "    Pure Markowitz long-only solver using CVXPY.\n",
    "\n",
    "    minimise:     w' \u03a3 w\n",
    "    subject to:   sum(w) = 1\n",
    "                  w >= 0\n",
    "                  \u03bc\u00b7w >= target_return   (or == if use_inequality=False)\n",
    "\n",
    "    Returns: (weights, ok, note)\n",
    "    \"\"\"\n",
    "\n",
    "    # Align inputs\n",
    "    mu = mu.reindex(Sigma.index).astype(float)\n",
    "    Sigma = Sigma.astype(float)\n",
    "\n",
    "    if mu.isna().any() or Sigma.isna().any().any():\n",
    "        return np.full(len(mu), np.nan), False, \"NaNs in mu or Sigma\"\n",
    "\n",
    "    n = len(mu)\n",
    "\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    Sigma_np = Sigma.to_numpy()\n",
    "    mu_np = mu.to_numpy()\n",
    "\n",
    "    objective = cp.Minimize(cp.quad_form(w, Sigma_np))\n",
    "\n",
    "    constraints = [\n",
    "        cp.sum(w) == 1,\n",
    "        w >= 0\n",
    "    ]\n",
    "\n",
    "    if use_inequality:\n",
    "        constraints.append(mu_np @ w >= target_return)\n",
    "    else:\n",
    "        constraints.append(mu_np @ w == target_return)\n",
    "\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "\n",
    "    try:\n",
    "        prob.solve(\n",
    "            solver=cp.OSQP,\n",
    "            verbose=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return np.full(n, np.nan), False, f\"Solver error: {e}\"\n",
    "\n",
    "    if w.value is None:\n",
    "        return np.full(n, np.nan), False, \"Infeasible\"\n",
    "\n",
    "    return np.asarray(w.value).flatten(), True, \"CVXPY success\"\n",
    "\n",
    "def optimise_unconstrained_analytic(mu, Sigma, target_return):\n",
    "    mu = np.asarray(mu, dtype=float)\n",
    "    Sigma = np.asarray(Sigma, dtype=float)\n",
    "    n = len(mu); ones = np.ones(n)\n",
    "    Sigma_inv = np.linalg.pinv(Sigma)\n",
    "    A = ones @ Sigma_inv @ ones\n",
    "    Bv = ones @ Sigma_inv @ mu\n",
    "    C = mu @ Sigma_inv @ mu\n",
    "    M = np.array([[A, Bv], [Bv, C]]); rhs = np.array([1.0, float(target_return)])\n",
    "    try:\n",
    "        alpha, beta = np.linalg.solve(M, rhs)\n",
    "        w = Sigma_inv @ (alpha * ones + beta * mu)\n",
    "        return w, \"Analytic solution.\"\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.full(n, np.nan), \"Analytic solver failed (singular).\"\n",
    "\n",
    "def optimise_long_only_with_tilts(mu, Sigma, target_return, B, tilt_targets, tilt_bands, use_mask):\n",
    "    from scipy.optimize import minimize\n",
    "    mu = np.asarray(mu, dtype=float)\n",
    "    Sigma = np.asarray(Sigma, dtype=float)\n",
    "    n = len(mu)\n",
    "    def obj(w): return float(w @ Sigma @ w)\n",
    "    cons = [\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0},\n",
    "        {'type': 'eq', 'fun': lambda w: float(mu @ w) - float(target_return)},\n",
    "    ]\n",
    "    factors = list(tilt_targets.keys()) if hasattr(tilt_targets, \"keys\") else [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\"]\n",
    "    for f in factors:\n",
    "        if not use_mask.get(f, True): \n",
    "            continue\n",
    "        t = float(tilt_targets.get(f, 0.0))\n",
    "        b = float(tilt_bands.get(f, 0.05))\n",
    "        if hasattr(B, 'columns') and f not in list(B.columns):\n",
    "            continue\n",
    "        v = np.asarray(B[f], dtype=float)\n",
    "        cons.append({'type':'ineq', 'fun': (lambda v=v, t=t, b=b: lambda w: (t + b) - float(v @ w))()})\n",
    "        cons.append({'type':'ineq', 'fun': (lambda v=v, t=t, b=b: lambda w:  float(v @ w) - (t - b))()})\n",
    "    bounds = [(0.0, 1.0)] * n\n",
    "    w0 = np.full(n, 1.0/n)\n",
    "    res = minimize(obj, w0, method='SLSQP', bounds=bounds, constraints=cons,\n",
    "                   options={'maxiter': 1000, 'ftol': 1e-12, 'disp': False})\n",
    "    if res.success and np.isfinite(res.fun):\n",
    "        return res.x, \"tilt SLSQP success\"\n",
    "    return np.full(n, np.nan), f\"tilt SLSQP failed ({getattr(res,'message','unknown')})\"\n",
    "###ADD SOMETHING HERE\n",
    "\n",
    "def _build_frontier(\n",
    "    mu_vec_opt: pd.Series,\n",
    "    Sigma_opt: pd.DataFrame,\n",
    "    target_returns: list[float] | None = None,\n",
    "    *,\n",
    "    n_points: int = 18,\n",
    "    max_excess_return: float = 0.40,   # cap at rf + 40% p.a.\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, float, float]:\n",
    "    \"\"\"\n",
    "    Build a long-only efficient frontier on a realistic range of target returns.\n",
    "\n",
    "    Strategy:\n",
    "      1. Compute the global minimum-variance (MVP) portfolio (no return constraint).\n",
    "      2. Use its expected return R_mvp as the centre of the grid where possible.\n",
    "      3. Choose a symmetric range [R_mvp - span, R_mvp + span], but clipped so that:\n",
    "           - low end >= rf_annual\n",
    "           - high end <= min(mu_max, rf_annual + max_excess_return)\n",
    "      4. For each target return, solve a clean Markowitz problem with _solve_frontier_point.\n",
    "    \"\"\"\n",
    "    # Clean \u03bc and \u03a3 aligned\n",
    "    mu_clean = pd.to_numeric(mu_vec_opt, errors=\"coerce\").dropna()\n",
    "    if mu_clean.empty:\n",
    "        raise ValueError(\"mu_vec_opt has no finite entries\")\n",
    "\n",
    "    mu = mu_vec_opt.reindex(Sigma_opt.index).to_numpy(dtype=float)\n",
    "    S  = Sigma_opt.to_numpy(dtype=float)\n",
    "    n  = len(mu)\n",
    "\n",
    "    mu_max = float(np.nanmax(mu))\n",
    "\n",
    "    # ---- 1) Global minimum-variance portfolio (no return constraint) ----\n",
    "    def obj_var(w: np.ndarray) -> float:\n",
    "        return float(w @ S @ w)\n",
    "\n",
    "    cons_mvp = ({\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0},)\n",
    "    bounds = [(0.0, 1.0)] * n\n",
    "    w0 = np.full(n, 1.0 / n)\n",
    "\n",
    "    # MVP via CVXPY (same cleaning as above)\n",
    "    mu_clean = pd.to_numeric(mu_vec_opt, errors=\"coerce\").reindex(Sigma_opt.index)\n",
    "    keep = mu_clean.index[mu_clean.notna()]\n",
    "    Sigma_sub = Sigma_opt.loc[keep, keep].copy()\n",
    "    Sigma_sub = Sigma_sub.dropna(axis=0, how=\"any\").dropna(axis=1, how=\"any\")\n",
    "    mu_clean = mu_clean.reindex(Sigma_sub.index)\n",
    "    \n",
    "    mu = mu_clean.to_numpy(dtype=float)\n",
    "    S  = Sigma_sub.to_numpy(dtype=float)\n",
    "    n  = len(mu)\n",
    "    \n",
    "    # ridge\n",
    "    S = S + 1e-10 * np.eye(n)\n",
    "    \n",
    "    w_var = cp.Variable(n)\n",
    "    prob_mvp = cp.Problem(cp.Minimize(cp.quad_form(w_var, S)),\n",
    "                          [cp.sum(w_var) == 1, w_var >= 0])\n",
    "    try:\n",
    "        prob_mvp.solve(solver=cp.OSQP, verbose=False)\n",
    "    except Exception:\n",
    "        prob_mvp.solve(solver=cp.ECOS, verbose=False)\n",
    "    \n",
    "    if w_var.value is None:\n",
    "        w_mvp = np.full(n, 1.0/n)\n",
    "        mvp_note = \"MVP fallback: CVXPY no solution\"\n",
    "    else:\n",
    "        w_mvp = np.asarray(w_var.value).reshape(-1)\n",
    "        mvp_note = \"MVP CVXPY success\"\n",
    "    \n",
    "    # For downstream calculations in _build_frontier, keep using Sigma_opt\u2019s full index:\n",
    "    w_mvp_full = pd.Series(0.0, index=Sigma_opt.index)\n",
    "    w_mvp_full.loc[Sigma_sub.index] = w_mvp\n",
    "    w_mvp = w_mvp_full.to_numpy(dtype=float)\n",
    "    # Compute MVP expected return (annualised)\n",
    "    R_mvp_ann = float(w_mvp @ mu_vec_opt.reindex(Sigma_opt.index).to_numpy(dtype=float))\n",
    "    \n",
    "    # ---- 2) Choose a symmetric return range around MVP, with clipping ----\n",
    "    # Floor at rf (so we don't target silly negative returns even if some assets have them)\n",
    "    low_floor = float(rf_annual)\n",
    "\n",
    "    # How far can we go up/down while:\n",
    "    #  - staying above rf on the low side\n",
    "    #  - not exceeding mu_max or rf + max_excess_return on the high side\n",
    "    max_high_allowed = min(mu_max, low_floor + max_excess_return)\n",
    "\n",
    "    span_down = max(R_mvp_ann - low_floor, 0.0)\n",
    "    span_up   = max_high_allowed - R_mvp_ann\n",
    "\n",
    "    # Use the smaller of up/down spans for symmetry if possible\n",
    "    span = min(span_down, span_up)\n",
    "\n",
    "    if span <= 0:\n",
    "        # MVP is below rf or too close to bounds; just use [rf, rf + max_excess_return]\n",
    "        low = low_floor\n",
    "        high = max_high_allowed\n",
    "    else:\n",
    "        low = R_mvp_ann - span\n",
    "        high = R_mvp_ann + span\n",
    "\n",
    "    # Safety: ensure we have some spread\n",
    "    if high <= low + 0.01:\n",
    "        high = low + 0.06  # force at least a 6%-wide frontier\n",
    "\n",
    "    if target_returns is None:\n",
    "        target_returns = np.linspace(low, high, n_points).tolist()\n",
    "\n",
    "    print(f\"[frontier] target_returns from {target_returns[0]:.4%} to {target_returns[-1]:.4%}\")\n",
    "\n",
    "    # ---- 3) Solve each frontier point with the clean solver ----\n",
    "    weights_dict: dict[float, np.ndarray] = {}\n",
    "    stats_rows: list[dict] = []\n",
    "\n",
    "    for R in target_returns:\n",
    "        w, ok, note = solve_frontier_point_cvxpy(mu_vec_opt, Sigma_opt, R)\n",
    "        weights_dict[R] = w\n",
    "\n",
    "        if np.all(np.isfinite(w)):\n",
    "            vol_ann = float(np.sqrt(w @ S @ w) * np.sqrt(252.0))\n",
    "            achieved = float(mu @ w)\n",
    "        else:\n",
    "            vol_ann = np.nan\n",
    "            achieved = np.nan\n",
    "\n",
    "        sharpe = ((achieved - rf_annual) / vol_ann\n",
    "                  if (pd.notna(vol_ann) and vol_ann > 0)\n",
    "                  else np.nan)\n",
    "\n",
    "        stats_rows.append({\n",
    "            \"Target Return\":     R,\n",
    "            \"Achieved Return\":   achieved,\n",
    "            \"Volatility (ann.)\": vol_ann,\n",
    "            \"Sharpe\":            sharpe,\n",
    "            \"Method\":            \"Frontier SLSQP\" if ok else \"Frontier failed\",\n",
    "            \"Note\":              note,\n",
    "        })\n",
    "\n",
    "    # ---- 4) Turn into W and stats_df, pick tangency point ----\n",
    "    target_returns = [r for r in target_returns if np.isfinite(r)]\n",
    "    \n",
    "    cols = [\n",
    "        f\"{r*100:.1f}%\" if (r*100) % 1 != 0 else f\"{int(r*100):d}%\"\n",
    "        for r in target_returns\n",
    "    ]\n",
    "\n",
    "    W = pd.DataFrame(\n",
    "        {c: weights_dict[R] for c, R in zip(cols, target_returns)},\n",
    "        index=Sigma_opt.index,\n",
    "    )\n",
    "\n",
    "    stats_df = pd.DataFrame(stats_rows)\n",
    "    stats_df.insert(0, \"Target (%)\", cols)\n",
    "    stats_df = stats_df.drop(columns=[\"Target Return\"])\n",
    "\n",
    "    sh = pd.to_numeric(stats_df[\"Sharpe\"], errors=\"coerce\")\n",
    "    if sh.notna().any():\n",
    "        best_idx = int(sh.idxmax())\n",
    "    else:\n",
    "        vol_series = pd.to_numeric(stats_df[\"Volatility (ann.)\"], errors=\"coerce\")\n",
    "        best_idx = int(vol_series.idxmin()) if vol_series.notna().any() else 0\n",
    "\n",
    "    tan_ret = float(pd.to_numeric(stats_df.loc[best_idx, \"Achieved Return\"], errors=\"coerce\"))\n",
    "    tan_vol = float(pd.to_numeric(stats_df.loc[best_idx, \"Volatility (ann.)\"], errors=\"coerce\"))\n",
    "    if not np.isfinite(tan_ret) or not np.isfinite(tan_vol):\n",
    "        tan_ret, tan_vol = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    print(f\"[frontier] tangency ret \u2248 {tan_ret:.4f}, vol \u2248 {tan_vol:.4f}\")\n",
    "\n",
    "    return W, stats_df, tan_ret, tan_vol\n",
    "    \n",
    "# ------------------------------------------------------------\n",
    "# 9) FRONTIERS: MVP-centred long-only frontier\n",
    "# ------------------------------------------------------------\n",
    "mu_frontier = mu_vec_opt.copy()\n",
    "Sigma_frontier = Sigma_opt.copy()\n",
    "\n",
    "W, stats_df, tan_ret, tan_vol = _build_frontier(\n",
    "    mu_frontier,\n",
    "    Sigma_frontier,\n",
    "    target_returns=None,\n",
    "    n_points=20,\n",
    "    max_excess_return=0.95,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10) PREPARE A Trade Plan\n",
    "# ------------------------------------------------------------\n",
    "cov_plus = cov_plus.fillna(0.0)\n",
    "\n",
    "# investable assets only (matches weights grid)\n",
    "exp_ret_df = mu_vec_opt.rename(exp_ret_label).to_frame()\n",
    "# or, if you prefer to show all (incl. ^AORD), use mu_vec_all instead.\n",
    "\n",
    "def make_trade_plan(units_cur, last_px, fx_map, w_target, include_flags, include_zero_lines=False):\n",
    "    \"\"\"\n",
    "    Return (trade_df, residual_cash) to move from current units to target weights (AUD).\n",
    "    Ensures indices align; includes Cash Flow (AUD) column.\n",
    "    \"\"\"\n",
    "    # 1) Align inputs\n",
    "    tickers = pd.Index(w_target.index, name=\"Security\")\n",
    "    lp = pd.to_numeric(last_px, errors=\"coerce\").reindex(tickers).fillna(0.0)\n",
    "    fx = pd.Series(1.0, index=tickers)\n",
    "    if isinstance(fx_map, (dict, pd.Series)):\n",
    "        fx = pd.to_numeric(pd.Series(fx_map), errors=\"coerce\").reindex(tickers).fillna(1.0)\n",
    "    px_aud = (lp * fx).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    cur_units = pd.to_numeric(units_cur, errors=\"coerce\").reindex(tickers).fillna(0).astype(int)\n",
    "\n",
    "    # 2) Current portfolio value (AUD)\n",
    "    cur_val = float((cur_units * px_aud).sum())\n",
    "    if not np.isfinite(cur_val) or cur_val <= 0:\n",
    "        # Degenerate: nothing held; target units come from weights * arbitrary scale of 1 AUD\n",
    "        tgt_units = (w_target * 0.0).reindex(tickers).fillna(0.0).round().astype(int)\n",
    "    else:\n",
    "        # 3) Convert target weights -> target units (rounded)\n",
    "        tgt_val = pd.to_numeric(w_target, errors=\"coerce\").reindex(tickers).fillna(0.0) * cur_val\n",
    "        tgt_units = (tgt_val / px_aud.replace(0.0, np.nan)).fillna(0.0).round().astype(int)\n",
    "\n",
    "    # 4) Apply include flags: if an asset is excluded, freeze units (no trade)\n",
    "    if isinstance(include_flags, dict):\n",
    "        inc = pd.Series(include_flags).reindex(tickers).fillna(True).astype(bool)\n",
    "        tgt_units.loc[~inc] = cur_units.loc[~inc]\n",
    "\n",
    "    # 5) Small-trade suppressor (value-based; avoid paying flat brokerage for tiny trades)\n",
    "    delta = (tgt_units - cur_units).astype(int)\n",
    "\n",
    "    # 6) Cash flow (AUD), positive = proceeds from sales\n",
    "    cash_flow = (delta * px_aud).astype(float)\n",
    "\n",
    "    # 7) Assemble table\n",
    "    df = pd.DataFrame({\n",
    "        \"Security\": tickers,\n",
    "        \"Curr Units\": cur_units.values,\n",
    "        \"Target Units\": tgt_units.values,\n",
    "        \"\u0394 Units\": delta.values,\n",
    "        \"Last Px (AUD)\": px_aud.values,\n",
    "        \"Cash Flow (AUD)\": cash_flow.values,\n",
    "    }).set_index(\"Security\")\n",
    "\n",
    "    if not include_zero_lines:\n",
    "        df = df.loc[df[\"\u0394 Units\"] != 0]\n",
    "\n",
    "    residual_cash = float(df[\"Cash Flow (AUD)\"].sum())\n",
    "    return df, residual_cash\n",
    "\n",
    "def compute_target_units_for_holdings(units_cur, last_px, fx_map, w_target, include_flags):\n",
    "    tickers = list(pd.Index(w_target.index))\n",
    "    inc = pd.Series(include_flags).reindex(tickers).fillna(True).astype(bool)\n",
    "    tickers = [t for t in tickers if inc.get(t, True)]\n",
    "\n",
    "    lp_aud = (pd.Series(last_px).reindex(tickers).astype(float) *\n",
    "              pd.Series(fx_map).reindex(tickers).fillna(1.0).astype(float))\n",
    "    cur_units = pd.Series(units_cur).reindex(tickers).fillna(0.0).astype(float)\n",
    "    cur_val = float((cur_units * lp_aud).sum())\n",
    "    if cur_val <= 0:\n",
    "        return pd.Series(0, index=w_target.index, dtype=int)\n",
    "    # --- Soft penalty for small trades ---\n",
    "    # Access current holdings from outer scope\n",
    "    global current_holdings_units\n",
    "    \n",
    "    if current_holdings_units is not None:\n",
    "        # build target_units first (the variable was missing)\n",
    "        target_units = (w_target * cur_val / lp_aud).fillna(0)\n",
    "    \n",
    "        delta = target_units - current_holdings_units.reindex(target_units.index).fillna(0)\n",
    "    \n",
    "        small_trade_mask = delta.abs() <= 2\n",
    "        target_units.loc[small_trade_mask] = current_holdings_units.reindex(target_units.index)[small_trade_mask]\n",
    "\n",
    "    tgt_val = pd.Series(w_target).reindex(tickers).fillna(0.0) * cur_val\n",
    "    tgt_units_float = (tgt_val / lp_aud).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    tgt_units_int = tgt_units_float.round().astype(int)\n",
    "    return tgt_units_int.reindex(w_target.index).fillna(0).astype(int)\n",
    "\n",
    "def compute_achieved_tilts(B: pd.DataFrame, w: pd.Series, factors=None, renormalise_missing=True) -> pd.Series:\n",
    "    if B is None or B.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    w_all = pd.Series(w).reindex(B.index).fillna(0.0)\n",
    "    if renormalise_missing and w_all.sum() > 0:\n",
    "        w_use = w_all / w_all.sum()\n",
    "    else:\n",
    "        w_use = w_all\n",
    "    t = (B.T @ w_use).rename(\"Achieved \u03b2\")\n",
    "    if factors is not None:\n",
    "        t = t.reindex(factors)\n",
    "    return t\n",
    "\n",
    "# ============================\n",
    "# Utility: Compute MVP-centred return targets\n",
    "# ============================\n",
    "\n",
    "def generate_targets_mvp_centric(mu_vec, Sigma, \n",
    "                                 span_vol=0.20, n_points=20):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "        MVP \u2192 portfolio with minimum variance\n",
    "        Targets \u2192 returns from MVP \u00b1 (span_vol * MVP_vol)\n",
    "    \"\"\"\n",
    "    # Solve for MVP weights: minimise w' \u03a3 w subject to sum(w)=1, w>=0\n",
    "    n = len(mu_vec)\n",
    "\n",
    "    x0 = np.ones(n)/n\n",
    "\n",
    "    cons = ({'type':'eq','fun':lambda w: np.sum(w)-1})\n",
    "    bnds = [(0,1)]*n\n",
    "\n",
    "    def vol_objective(w):\n",
    "        return np.sqrt(w @ Sigma @ w)\n",
    "\n",
    "    sol = minimize(vol_objective, x0, bounds=bnds, constraints=cons)\n",
    "\n",
    "    if not sol.success:\n",
    "        raise RuntimeError(\"Could not compute MVP\")\n",
    "\n",
    "    w_mvp = sol.x\n",
    "    mu_mvp = float(w_mvp @ mu_frontier)\n",
    "    vol_mvp = float(np.sqrt(w_mvp @ Sigma_frontier @ w_mvp))\n",
    "\n",
    "    # Build target-return vector around MVP\n",
    "    lo = mu_mvp - span_vol*vol_mvp\n",
    "    hi = mu_mvp + span_vol*vol_mvp\n",
    "\n",
    "    targets = np.linspace(lo, hi, n_points)\n",
    "    return targets, mu_mvp, vol_mvp\n",
    "    \n",
    "# ============================\n",
    "# Utility: recommended factor tilts\n",
    "# ============================\n",
    "\n",
    "def recommend_factor_tilts_achievable(B, f_mean_ann, Fcov_daily):\n",
    "    \"\"\"\n",
    "    Achievable factor tilt recommendation:\n",
    "    - Use \u03a3\u207b\u00b9 \u03bc as the theoretical optimum\n",
    "    - Then project into the space achievable by your investable betas B\n",
    "    \"\"\"\n",
    "\n",
    "    # theoretical optimal tilt: t_opt = Sigma^{-1} \u03bc\n",
    "    fac = f_mean_ann.index\n",
    "    mu = f_mean_ann.values\n",
    "    Sigma = Fcov_daily.loc[fac, fac].values\n",
    "    Sigma_inv = np.linalg.pinv(Sigma)\n",
    "    t_opt = Sigma_inv @ mu\n",
    "\n",
    "    # convert to Series for alignment\n",
    "    t_opt = pd.Series(t_opt, index=fac)\n",
    "\n",
    "    # project into achievable space:\n",
    "    # minimise ||B w - t_opt|| with weights summing to 1, w>=0\n",
    "    try:\n",
    "        tickers = list(B.index)\n",
    "        Bmat = B[fac].to_numpy(dtype=float)\n",
    "        n = len(tickers)\n",
    "\n",
    "        def obj(w):\n",
    "            return float(np.sum((Bmat.T @ w - t_opt.values)**2))\n",
    "\n",
    "        cons = ({'type':'eq', 'fun': lambda w: np.sum(w) - 1.0},)\n",
    "        bnds = [(0,1)] * n\n",
    "        w0 = np.full(n, 1.0/n)\n",
    "\n",
    "        sol = minimize(obj, w0, method='SLSQP', bounds=bnds, constraints=cons)\n",
    "        if sol.success:\n",
    "            w = sol.x\n",
    "            achievable = pd.Series(Bmat.T @ w, index=fac)\n",
    "        else:\n",
    "            achievable = t_opt.copy()\n",
    "    except Exception:\n",
    "        achievable = t_opt.copy()\n",
    "\n",
    "    # return both (achievable tilts, raw theoretical tilts)\n",
    "    return achievable, t_opt\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- DEBUG CHECK: Sigma_opt / mu_vec_opt ---\")\n",
    "print(\"Any NaN in Sigma_opt:\", Sigma_opt.isna().any().any())\n",
    "print(\"Any NaN in mu_vec_opt:\", mu_vec_opt.isna().any())\n",
    "print(\"Min variance:\", np.min(np.diag(Sigma_opt)))\n",
    "print(\"Number of assets:\", len(Sigma_opt))\n",
    "print(Sigma_opt.head())\n",
    "print(mu_vec_opt.head())\n",
    "print(\"OPT TICKERS:\", list(securities_opt))\n",
    "print(\"mu:\", mu_vec_opt.describe())\n",
    "print(\"Sigma diag min/max:\", Sigma_opt.values.diagonal().min(), Sigma_opt.values.diagonal().max())\n",
    "print(f_mean_ann)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 6 Transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Brokerage & CGT config (edit these to suit) ---\n",
    "BROKERAGE = {\n",
    "    \"ASX\": {\"first_buy_free_threshold\": 1000.0, \"min_fee\": 11.0, \"rate\": 0.001},  # 0.10%\n",
    "    \"US\":  {\"min_fee\": 0.0, \"rate\": 0.0},  # CMC U.S. brokerage $0\n",
    "}\n",
    "\n",
    "MIN_TRADE_VALUE = 11.0\n",
    "\n",
    "def _market_of(ticker: str) -> str:\n",
    "    t = str(ticker)\n",
    "    if t.startswith(\"^\"): return \"INDEX\"\n",
    "    if t.endswith(\".AX\"): return \"ASX\"\n",
    "    return \"US\"\n",
    "\n",
    "def suppress_small_trades_by_value(trade_df: pd.DataFrame, min_trade_value_aud: float = MIN_TRADE_VALUE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hard rule:\n",
    "      If abs(\u0394 Units) * Last Px (AUD) <= min_trade_value_aud, set \u0394 Units to 0.\n",
    "\n",
    "    Also recomputes Cash Flow (AUD) based on the suppressed \u0394 Units.\n",
    "    Adds helper columns:\n",
    "      - Trade Value (AUD)\n",
    "      - Suppressed (bool)\n",
    "    \"\"\"\n",
    "    if trade_df is None or trade_df.empty:\n",
    "        return trade_df\n",
    "\n",
    "    out = trade_df.copy()\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if \"\u0394 Units\" not in out.columns or \"Last Px (AUD)\" not in out.columns:\n",
    "        return out\n",
    "\n",
    "    du = pd.to_numeric(out[\"\u0394 Units\"], errors=\"coerce\").fillna(0.0)\n",
    "    px = pd.to_numeric(out[\"Last Px (AUD)\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    trade_val = (du.abs() * px).astype(float)\n",
    "    suppressed = trade_val <= float(min_trade_value_aud)\n",
    "\n",
    "    # Apply suppression\n",
    "    du_adj = du.where(~suppressed, 0.0)\n",
    "\n",
    "    out[\"Trade Value (AUD)\"] = trade_val\n",
    "    out[\"Suppressed\"] = suppressed.astype(bool)\n",
    "    out[\"\u0394 Units\"] = du_adj.round().astype(int)\n",
    "\n",
    "    # Recompute cash flow if present (or create it)\n",
    "    out[\"Cash Flow (AUD)\"] = (pd.to_numeric(out[\"\u0394 Units\"], errors=\"coerce\").fillna(0.0) * px).astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "def compute_brokerage(trade_df: pd.DataFrame) -> tuple[float, pd.Series]:\n",
    "    \"\"\"Return (total_brokerage_AUD, per_row_series).\"\"\"\n",
    "    if trade_df.empty:\n",
    "        return 0.0, pd.Series(dtype=float)\n",
    "\n",
    "    fees = []\n",
    "    asx_buy_candidates = []  # (row_idx, trade_value) eligible for $0\n",
    "\n",
    "    for i, r in trade_df.iterrows():\n",
    "        # Skip rows with no actual trade\n",
    "        units = float(r.get(\"\u0394 Units\", 0.0))\n",
    "        if abs(units) < 1e-12:\n",
    "            fees.append(0.0)\n",
    "            continue\n",
    "\n",
    "        mkt = _market_of(r[\"Security\"])\n",
    "        px  = float(r.get(\"Last Px (AUD)\", 0.0))\n",
    "        trade_val = abs(units) * px\n",
    "\n",
    "        if mkt == \"US\":\n",
    "            fee = 0.0\n",
    "        elif mkt == \"ASX\":\n",
    "            fee = max(BROKERAGE[\"ASX\"][\"min_fee\"], BROKERAGE[\"ASX\"][\"rate\"] * trade_val)\n",
    "            # Track eligible \u201cfirst buy \u2264 $1k\u201d\n",
    "            if units > 0 and trade_val <= BROKERAGE[\"ASX\"][\"first_buy_free_threshold\"] + 1e-9:\n",
    "                asx_buy_candidates.append((i, trade_val))\n",
    "        else:\n",
    "            fee = 0.0\n",
    "\n",
    "        fees.append(fee)\n",
    "\n",
    "    fees = pd.Series(fees, index=trade_df.index, name=\"Brokerage (AUD)\")\n",
    "\n",
    "    # Apply \u201cfirst ASX buy \u2264 $1k is $0\u201d to ONE eligible row\n",
    "    if asx_buy_candidates:\n",
    "        # (Leave as smallest-eligible; change to reverse=True to pick the largest-eligible instead.)\n",
    "        idx0 = sorted(asx_buy_candidates, key=lambda x: x[1])[0][0]\n",
    "        fees.loc[idx0] = 0.0\n",
    "\n",
    "    return float(fees.sum()), fees\n",
    "\n",
    "\n",
    "def _read_lots_from_path(xl_path, sheet=\"Lots\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lots sheet schema:\n",
    "      Security | AcqDate | Units | CostBaseAUD\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(xl_path, sheet_name=sheet)\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=[\"Security\",\"AcqDate\",\"Units\",\"CostBaseAUD\"])\n",
    "\n",
    "    if df.empty: \n",
    "        return pd.DataFrame(columns=[\"Security\",\"AcqDate\",\"Units\",\"CostBaseAUD\"])\n",
    "\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    if \"AcqDate\" in df.columns:\n",
    "        df[\"AcqDate\"] = pd.to_datetime(df[\"AcqDate\"], errors=\"coerce\")\n",
    "    for col in [\"Units\",\"CostBaseAUD\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Security\",\"AcqDate\",\"Units\",\"CostBaseAUD\"])\n",
    "    df[\"Security\"] = df[\"Security\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def _is_long_term_au(acq_date: pd.Timestamp, sale_date: pd.Timestamp) -> bool:\n",
    "    \"\"\"\n",
    "    Australian CGT 50% discount rule:\n",
    "    asset must be held for at least 12 months between acquisition and sale.\n",
    "    Uses relativedelta to handle leap years correctly.\n",
    "    \"\"\"\n",
    "    if pd.isna(acq_date) or pd.isna(sale_date):\n",
    "        return False\n",
    "    return sale_date >= (pd.Timestamp(acq_date) + relativedelta(years=1))\n",
    "\n",
    "def _allocate_sale_to_lots(lots: pd.DataFrame, sell_units: float, sale_price_aud: float,\n",
    "                           sale_date: pd.Timestamp, method: str = \"HIFO\"):\n",
    "    \"\"\"\n",
    "    Consume lot units to satisfy a sale. Returns list of dicts with:\n",
    "      qty, acq_date, proceed, cost_base, gain, long_term\n",
    "    \"\"\"\n",
    "    if lots.empty or sell_units <= 0:\n",
    "        return []\n",
    "\n",
    "    lots = lots.copy()\n",
    "    if \"AcqDate\" in lots.columns:\n",
    "        lots[\"AcqDate\"] = pd.to_datetime(lots[\"AcqDate\"], errors=\"coerce\")\n",
    "\n",
    "    # Sort by matching method\n",
    "    if method.upper() == \"HIFO\":\n",
    "        lots = lots.sort_values(by=[\"CostBaseAUD\", \"AcqDate\"], ascending=[False, True])\n",
    "    else:  # FIFO\n",
    "        lots = lots.sort_values(by=[\"AcqDate\"], ascending=True)\n",
    "\n",
    "    out = []\n",
    "    remaining = float(sell_units)\n",
    "    for _, L in lots.iterrows():\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        have = float(L[\"Units\"])\n",
    "        if have <= 0:\n",
    "            continue\n",
    "\n",
    "        qty = min(remaining, have)\n",
    "        cb_unit = float(L[\"CostBaseAUD\"])\n",
    "        acq = pd.Timestamp(L[\"AcqDate\"])\n",
    "\n",
    "        proceed = float(sale_price_aud) * qty\n",
    "        cost_base = cb_unit * qty\n",
    "        gain = proceed - cost_base\n",
    "        long_term = _is_long_term_au(acq, sale_date)\n",
    "\n",
    "        out.append({\n",
    "            \"qty\": qty,\n",
    "            \"acq_date\": acq,\n",
    "            \"proceed\": proceed,\n",
    "            \"cost_base\": cost_base,\n",
    "            \"gain\": gain,\n",
    "            \"long_term\": bool(long_term),\n",
    "        })\n",
    "        remaining -= qty\n",
    "\n",
    "    return out\n",
    "\n",
    "def compute_cgt_tax(trade_df: pd.DataFrame, lots_df: pd.DataFrame, sale_date: pd.Timestamp,\n",
    "                    marginal_rate: float, carry_forward_loss: float = 0.0,\n",
    "                    method: str = \"HIFO\") -> tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    Returns (tax_AUD, breakdown_dict) with an 'audit' DataFrame so users can see exactly\n",
    "    how CGT was computed (per-lot).\n",
    "    \"\"\"\n",
    "    if trade_df.empty:\n",
    "        return 0.0, {\"st_gain\":0.0, \"lt_gain\":0.0, \"losses\":0.0,\n",
    "                     \"discounted_lt_after_losses\":0.0, \"taxable\":0.0, \"audit\": pd.DataFrame()}\n",
    "\n",
    "    lots_df = lots_df.copy()\n",
    "    if \"AcqDate\" in lots_df.columns:\n",
    "        lots_df[\"AcqDate\"] = pd.to_datetime(lots_df[\"AcqDate\"], errors=\"coerce\")\n",
    "\n",
    "    # group lots by security for fast lookup\n",
    "    lots_by_sec = {s: g.copy() for s, g in lots_df.groupby(\"Security\")} if not lots_df.empty else {}\n",
    "\n",
    "    audit_rows = []\n",
    "    st_gain = 0.0; lt_gain = 0.0; losses = 0.0\n",
    "\n",
    "    for _, r in trade_df.iterrows():\n",
    "        dU = int(r.get(\"\u0394 Units\", 0))\n",
    "        if dU >= 0:  # only sells trigger CGT\n",
    "            continue\n",
    "        sec  = str(r[\"Security\"])\n",
    "        px_aud = float(r[\"Last Px (AUD)\"])\n",
    "        sell_qty = abs(dU)\n",
    "\n",
    "        ledger = _allocate_sale_to_lots(\n",
    "            lots_by_sec.get(sec, pd.DataFrame(columns=[\"Security\",\"AcqDate\",\"Units\",\"CostBaseAUD\"])),\n",
    "            sell_qty, px_aud, sale_date, method=method\n",
    "        )\n",
    "        sold = 0.0\n",
    "        for row in ledger:\n",
    "            sold += row[\"qty\"]\n",
    "            g = row[\"gain\"]\n",
    "            audit_rows.append({\n",
    "                \"Security\": sec,\n",
    "                \"Qty\": row[\"qty\"],\n",
    "                \"AcqDate\": row[\"acq_date\"],\n",
    "                \"SaleDate\": pd.Timestamp(sale_date),\n",
    "                \"Proceeds\": row[\"proceed\"],\n",
    "                \"CostBase\": row[\"cost_base\"],\n",
    "                \"Gain\": row[\"gain\"],\n",
    "                \"LongTermEligible\": bool(row[\"long_term\"])\n",
    "            })\n",
    "            if g >= 0:\n",
    "                if row[\"long_term\"]:\n",
    "                    lt_gain += g\n",
    "                else:\n",
    "                    st_gain += g\n",
    "            else:\n",
    "                losses += -g  # store as positive\n",
    "\n",
    "        # excess sells beyond recorded lots \u2192 treat as zero-gain (conservative)\n",
    "        _unused = max(0.0, sell_qty - sold)\n",
    "\n",
    "    # Apply losses (including carry-forward) first, optimally vs ST then LT\n",
    "    rem_losses = float(carry_forward_loss) + float(losses)\n",
    "    st_off = min(rem_losses, st_gain);  st_gain -= st_off;  rem_losses -= st_off\n",
    "    lt_off = min(rem_losses, lt_gain);  lt_gain -= lt_off;  rem_losses -= lt_off\n",
    "\n",
    "    # 50% discount on remaining long-term gains (AU individual rule)\n",
    "    discounted_lt = 0.5 * max(0.0, lt_gain)\n",
    "    taxable = max(0.0, st_gain + discounted_lt)\n",
    "\n",
    "    tax = float(marginal_rate) * float(taxable)\n",
    "    audit_df = pd.DataFrame(audit_rows)\n",
    "\n",
    "    bkd = {\n",
    "        \"st_gain\": float(st_gain),\n",
    "        \"lt_gain\": float(lt_gain),\n",
    "        \"losses\": float(losses + carry_forward_loss),\n",
    "        \"discounted_lt_after_losses\": float(discounted_lt),\n",
    "        \"taxable\": float(taxable),\n",
    "        \"audit\": audit_df\n",
    "    }\n",
    "    return float(tax), bkd\n",
    "\n",
    "def evaluate_transaction_costs(\n",
    "    trade_df: pd.DataFrame,\n",
    "    lots_df: pd.DataFrame,\n",
    "    sale_date: pd.Timestamp,\n",
    "    marginal_rate: float,\n",
    "    carry_forward_loss: float = 0.0,\n",
    "    method: str = \"HIFO\"\n",
    ") -> dict:\n",
    "    brok_total, brok_series = compute_brokerage(trade_df)\n",
    "    tax_total, tax_bkd = compute_cgt_tax(\n",
    "        trade_df, lots_df, sale_date,\n",
    "        marginal_rate=float(marginal_rate),\n",
    "        carry_forward_loss=float(carry_forward_loss),\n",
    "        method=str(method),\n",
    "    )\n",
    "    return {\n",
    "        \"brokerage\": brok_total,\n",
    "        \"cgt_tax\": tax_total,\n",
    "        \"total_cost\": brok_total + tax_total,\n",
    "        \"breakdown\": tax_bkd,\n",
    "        \"per_row_brokerage\": brok_series\n",
    "    }\n",
    "\n",
    "\n",
    "def _update_lots_after_trades(lots_df: pd.DataFrame, trade_df: pd.DataFrame,\n",
    "                              sale_date: pd.Timestamp, fx_map: pd.Series | dict):\n",
    "    \"\"\"\n",
    "    Apply executed trades to the Lots table:\n",
    "      - Sells: decrement matched lots using the current LOT_MATCH_METHOD (HIFO/FIFO).\n",
    "      - Buys:  append a new lot with AcqDate = sale_date and CostBaseAUD = Last Px (AUD).\n",
    "    Returns a NEW lots DataFrame (original not mutated).\n",
    "    \"\"\"\n",
    "    out = lots_df.copy()\n",
    "    if \"AcqDate\" in out.columns:\n",
    "        out[\"AcqDate\"] = pd.to_datetime(out[\"AcqDate\"], errors=\"coerce\")\n",
    "\n",
    "    for _, tr in trade_df.iterrows():\n",
    "        sec = str(tr[\"Security\"])\n",
    "        dU = int(tr.get(\"\u0394 Units\", 0))\n",
    "        px_aud = float(tr.get(\"Last Px (AUD)\", 0.0))\n",
    "\n",
    "        if dU < 0:\n",
    "            # Sells: consume existing lots in the same order used for CGT allocation\n",
    "            lot_block = out[out[\"Security\"] == sec].copy()\n",
    "            if LOT_MATCH_METHOD.upper() == \"HIFO\":\n",
    "                lot_block = lot_block.sort_values(by=[\"CostBaseAUD\",\"AcqDate\"], ascending=[False, True])\n",
    "            else:\n",
    "                lot_block = lot_block.sort_values(by=[\"AcqDate\"], ascending=True)\n",
    "\n",
    "            remaining = abs(dU)\n",
    "            for i in lot_block.index:\n",
    "                if remaining <= 0:\n",
    "                    break\n",
    "                have = float(out.at[i, \"Units\"])\n",
    "                take = min(remaining, have)\n",
    "                out.at[i, \"Units\"] = have - take\n",
    "                remaining -= take\n",
    "\n",
    "            # remove fully consumed lots\n",
    "            out = out[out[\"Units\"] > 0.0].copy()\n",
    "\n",
    "        elif dU > 0:\n",
    "            # Buys: create a new lot at today's AUD price\n",
    "            out = pd.concat([out, pd.DataFrame([{\n",
    "                \"Security\": sec,\n",
    "                \"AcqDate\": pd.Timestamp(sale_date),\n",
    "                \"Units\": int(dU),\n",
    "                \"CostBaseAUD\": px_aud\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "        # dU == 0 \u2192 no action for this row\n",
    "\n",
    "    return out\n",
    "    \n",
    "#------------------------------\n",
    "### BLOCK 5 CODE GRAVEYARD\n",
    "#------------------------------\n",
    "def expand_with_lots(trade_df, lots_df, sale_date, method=\"FIFO\"):\n",
    "    \"\"\"\n",
    "    Expand trade_df by matching sells (\u0394 Units < 0) to lots_df parcels.\n",
    "\n",
    "    Returns new DataFrame with:\n",
    "        Security, \u0394 Units, Last Px (AUD), Cash Flow (AUD), Brokerage (AUD),\n",
    "        AcqDate, UnitsSold, AcqPrice, CostBase, RealisedGain\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean and prepare\n",
    "    lots = lots_df.copy()\n",
    "    lots = lots.dropna(subset=[\"Security\", \"Units\", \"CostBaseAUD\"])\n",
    "    lots[\"Units\"] = lots[\"Units\"].astype(int)\n",
    "    lots[\"CostBaseAUD\"] = lots[\"CostBaseAUD\"].astype(float)\n",
    "    lots[\"AcqDate\"] = pd.to_datetime(lots[\"AcqDate\"], errors=\"coerce\")\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for idx, row in trade_df.iterrows():\n",
    "        sec = row[\"Security\"]\n",
    "        delta = int(row[\"\u0394 Units\"])\n",
    "\n",
    "        # Only process sells\n",
    "        if delta >= 0:\n",
    "            continue\n",
    "\n",
    "        units_to_sell = -delta\n",
    "\n",
    "        # Get lots for this security\n",
    "        sec_lots = lots[lots[\"Security\"] == sec].copy()\n",
    "        if sec_lots.empty:\n",
    "            # No lots? Produce a placeholder row but warn\n",
    "            out_rows.append({\n",
    "                \"Security\": sec,\n",
    "                \"AcqDate\": pd.NaT,\n",
    "                \"UnitsSold\": units_to_sell,\n",
    "                \"AcqPrice\": float(\"nan\"),\n",
    "                \"CostBase\": float(\"nan\"),\n",
    "                \"Last Px (AUD)\": row[\"Last Px (AUD)\"],\n",
    "                \"Cash Flow (AUD)\": row[\"Cash Flow (AUD)\"],\n",
    "                \"Brokerage (AUD)\": row[\"Brokerage (AUD)\"],\n",
    "                \"RealisedGain\": float(\"nan\"),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Choose method\n",
    "        if method == \"FIFO\":\n",
    "            sec_lots = sec_lots.sort_values(\"AcqDate\")\n",
    "        elif method == \"HIFO\":\n",
    "            sec_lots = sec_lots.sort_values(\"CostBaseAUD\", ascending=False)\n",
    "\n",
    "        # Consume lots\n",
    "        for _, lot in sec_lots.iterrows():\n",
    "            if units_to_sell <= 0:\n",
    "                break\n",
    "\n",
    "            take = min(units_to_sell, lot[\"Units\"])\n",
    "            units_to_sell -= take\n",
    "\n",
    "            acq_price = lot[\"CostBaseAUD\"]\n",
    "            cost_base = take * acq_price\n",
    "            proceeds = take * row[\"Last Px (AUD)\"]\n",
    "            realised = proceeds - cost_base\n",
    "\n",
    "            out_rows.append({\n",
    "                \"Security\": sec,\n",
    "                \"AcqDate\": lot[\"AcqDate\"],\n",
    "                \"UnitsSold\": int(take),\n",
    "                \"AcqPrice\": float(acq_price),\n",
    "                \"CostBase\": float(cost_base),\n",
    "                \"Last Px (AUD)\": row[\"Last Px (AUD)\"],\n",
    "                \"Cash Flow (AUD)\": row[\"Cash Flow (AUD)\"],\n",
    "                \"Brokerage (AUD)\": row[\"Brokerage (AUD)\"],\n",
    "                \"RealisedGain\": float(realised),\n",
    "            })\n",
    "\n",
    "        # If not enough lots \u2014 warn and attach unmatched\n",
    "        if units_to_sell > 0:\n",
    "            out_rows.append({\n",
    "                \"Security\": sec,\n",
    "                \"AcqDate\": pd.NaT,\n",
    "                \"UnitsSold\": int(units_to_sell),\n",
    "                \"AcqPrice\": float(\"nan\"),\n",
    "                \"CostBase\": float(\"nan\"),\n",
    "                \"Last Px (AUD)\": row[\"Last Px (AUD)\"],\n",
    "                \"Cash Flow (AUD)\": row[\"Cash Flow (AUD)\"],\n",
    "                \"Brokerage (AUD)\": row[\"Brokerage (AUD)\"],\n",
    "                \"RealisedGain\": float(\"nan\"),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "# Load parcels once (if the sheet is missing, you just get an empty table)\n",
    "lots_df = _read_lots_from_path(filename, \"Lots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 7 Writing into the excel (i.e. formatting and building the actual sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cfg] excel_path: C:\\Users\\Fionn Guina\\Portfolio_Optimiser\\Stock Analysis.xlsm\n",
      "[chart] Skipping chart update (safe wrapper): cannot access local variable 'x_rng' where it is not associated with a value\n",
      "1 name: Chart 1 | title: Efficient Frontier & CAL (rf=3.60%)\n",
      "\n",
      "=== LOT-EXPANDED TABLE ===\n",
      "  Security AcqDate  UnitsSold  AcqPrice  CostBase  Last Px (AUD)  \\\n",
      "0  A200.AX     NaT          5       NaN       NaN     145.160004   \n",
      "1   IJP.AX     NaT         25       NaN       NaN     126.570000   \n",
      "2  MTUM.AX     NaT          7       NaN       NaN      28.950001   \n",
      "3      SMH     NaT         28       NaN       NaN     538.616197   \n",
      "4      SPY     NaT         29       NaN       NaN    1029.045281   \n",
      "5      VLU     NaT         30       NaN       NaN     319.785875   \n",
      "6  VVLU.AX     NaT          3       NaN       NaN      81.900002   \n",
      "\n",
      "   Cash Flow (AUD)  Brokerage (AUD)  RealisedGain  \n",
      "0      -725.800018              0.0           NaN  \n",
      "1     -3164.249992              0.0           NaN  \n",
      "2      -202.650005              0.0           NaN  \n",
      "3    -15081.253513              0.0           NaN  \n",
      "4    -29842.313162              0.0           NaN  \n",
      "5     -9593.576240              0.0           NaN  \n",
      "6      -245.700005              0.0           NaN  \n",
      "[cgt] audit_df is empty (no CGT-relevant sells).\n",
      "[debug] Current totals \u2192 Portfolio: 102451.81, Net Invested: 102597.27\n",
      "[debug] Previous totals \u2192 Portfolio: 101511.06, Net Invested: 101847.92\n",
      "[debug] Saved new state \u2192 Portfolio: 102451.81, Net Invested: 102597.27\n",
      "[pptx prep] PortfolioValue series computed for 22 securities.\n",
      "[pptx] Skipped PowerPoint generation: name 'export_to_ppt' is not defined\n",
      "Workbook Successfully Updated\n",
      "=== MU VEC (sorted) ===\n",
      "Security\n",
      "MVW.AX     0.138002\n",
      "MTUM.AX    0.147510\n",
      "VMIN.AX    0.150947\n",
      "VAS.AX     0.166668\n",
      "A200.AX    0.167887\n",
      "VDHG.AX    0.201935\n",
      "IEU.AX     0.209687\n",
      "VSO.AX     0.214004\n",
      "QHAL.AX    0.215208\n",
      "VVLU.AX    0.221919\n",
      "QLTY.AX    0.231686\n",
      "SMLL.AX    0.237771\n",
      "VGE.AX     0.241159\n",
      "VLUE.AX    0.248529\n",
      "IJP.AX     0.258128\n",
      "QUAL.AX    0.259692\n",
      "VGS.AX     0.272765\n",
      "IVV.AX     0.289838\n",
      "IOO.AX     0.335189\n",
      "VLU        0.338841\n",
      "SPY        0.376761\n",
      "SMH        0.667540\n",
      "dtype: float64\n",
      "\n",
      "Min mu: 0.13800232890106434\n",
      "Max mu: 0.6675396279708364\n",
      "Mean mu: 0.25416661904779203\n",
      "Top 10 assets by expected return:\n",
      "Security\n",
      "VGE.AX     0.241159\n",
      "VLUE.AX    0.248529\n",
      "IJP.AX     0.258128\n",
      "QUAL.AX    0.259692\n",
      "VGS.AX     0.272765\n",
      "IVV.AX     0.289838\n",
      "IOO.AX     0.335189\n",
      "VLU        0.338841\n",
      "SPY        0.376761\n",
      "SMH        0.667540\n",
      "dtype: float64\n",
      "            Mkt-RF     SMB     HML     RMW     CMA     MOM      RF\n",
      "Date                                                              \n",
      "1963-07-01 -0.0067  0.0000 -0.0034 -0.0001  0.0016 -0.0023  0.0001\n",
      "1963-07-02  0.0079 -0.0026  0.0026 -0.0007 -0.0020  0.0044  0.0001\n",
      "1963-07-03  0.0063 -0.0017 -0.0009  0.0018 -0.0034  0.0038  0.0001\n",
      "1963-07-05  0.0040  0.0008 -0.0027  0.0009 -0.0034  0.0006  0.0001\n",
      "1963-07-08 -0.0063  0.0004 -0.0018 -0.0029  0.0014 -0.0045  0.0001\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 10) WRITE TO EXCEL \n",
    "# ------------------------------------------------------------\n",
    "def ensure_workbook(path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    with xw.App(visible=False, add_book=True) as app:\n",
    "        wb = app.books.add()\n",
    "        for nm in [\"Holdings\",\"Tilts\",\"OPT\",\"Input\",\"Cov\",\"FF5F\",\"Lots\"]:\n",
    "            try: wb.sheets[nm]\n",
    "            except: wb.sheets.add(nm)\n",
    "        # Minimal headers\n",
    "        wb.sheets[\"Holdings\"].range(\"A1\").value = [[\"Security\",\"Units\",\"Last Price\",\"FX to AUD\",\"Market Value\",\"Weight\",\"Include?\"]]\n",
    "        wb.sheets[\"Tilts\"].range(\"A1\").value = [[\"Factor\",\"Target\",\"Band\",\"Use?\"]]\n",
    "        wb.sheets[\"Tilts\"].range(\"A2\").value = [[f, (1.0 if i==0 else 0.0), 0.20, (i==0)] for i,f in enumerate(TILT_FACTORS)]\n",
    "        wb.sheets[\"Lots\"].range(\"A1\").value = [[\"Security\",\"AcqDate\",\"Units\",\"CostBaseAUD\"]]\n",
    "        wb.save(path); wb.close()\n",
    "\n",
    "# Call it right before Block 7 seed reads:\n",
    "ensure_workbook(filename)\n",
    "print(\"[cfg] excel_path:\", filename)\n",
    "\n",
    "# Define path for saving portfolio state if not already defined\n",
    "state_path = os.path.join(os.path.dirname(filename), \"portfolio_state.json\")\n",
    "global results\n",
    "\n",
    "# -------------------------------\n",
    "# Writers (used by Block 7)\n",
    "# -------------------------------\n",
    "def _write_tilts_sheet(wb, tilts_df, sheet_name=\"Tilts\"):\n",
    "    try:\n",
    "        sht = wb.sheets[sheet_name]\n",
    "    except Exception:\n",
    "        sht = wb.sheets.add(sheet_name, after=wb.sheets[-1])\n",
    "    try:\n",
    "        sht.used_range.clear_contents()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out = tilts_df.reset_index().rename(columns={\"index\": \"Factor\"})\n",
    "    out = out[[\"Factor\",\"Target\",\"Band\",\"Use?\"]]\n",
    "    sht.range(\"A1\").value = [[\"Factor\",\"Target\",\"Band\",\"Use?\"]]\n",
    "    sht.range(\"A2\").options(index=False, header=False).value = out\n",
    "    last_row = 1 + len(out)\n",
    "    try:\n",
    "        sht.range(f\"B2:B{last_row}\").api.NumberFormat = \"0.000\"\n",
    "        sht.range(f\"C2:C{last_row}\").api.NumberFormat = \"0.000\"\n",
    "        val_rng = sht.range(f\"D2:D{last_row}\").api\n",
    "        val_rng.Validation.Delete()\n",
    "        val_rng.Validation.Add(3, 1, 1, \"TRUE,FALSE\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    sht.autofit()\n",
    "\n",
    "\n",
    "def _write_holdings_sheet(wb, prices, units, include_flags,\n",
    "                          sheet_name=\"Holdings\", fx_to_aud_map=None):\n",
    "    if fx_to_aud_map is None:\n",
    "        usd_aud = get_usd_aud_fx()\n",
    "        fx_to_aud_map = fx_to_aud_for_tickers(prices.columns, usd_aud)\n",
    "\n",
    "    tickers_all = [\n",
    "        t for t in prices.columns \n",
    "        if t != \"PortfolioValue\"\n",
    "    ]    \n",
    "    last_px = prices.ffill().iloc[-1]\n",
    "    rows = []\n",
    "    units_s = pd.Series(units)\n",
    "    include_s = pd.Series(include_flags)\n",
    "    for t in tickers_all:\n",
    "        inc = bool(include_s.get(t, False))\n",
    "        rows.append({\n",
    "            \"Security\": t,\n",
    "            \"Units\": float(units_s.get(t, 0.0)),\n",
    "            \"Last Price\": float(pd.Series(last_px).get(t, np.nan)),\n",
    "            \"FX to AUD\": float(pd.Series(fx_to_aud_map).get(t, 1.0)),\n",
    "            \"Market Value\": 0.0,\n",
    "            \"Weight\": 0.0,\n",
    "            \"Include?\": inc\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    try:\n",
    "        sht = wb.sheets[sheet_name]\n",
    "    except Exception:\n",
    "        sht = wb.sheets.add(sheet_name, after=wb.sheets[-1])\n",
    "    try:\n",
    "        sht.used_range.clear_contents()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    sht.range('A1').value = [[\"Security\",\"Units\",\"Last Price\",\"FX to AUD\",\"Market Value\",\"Weight\",\"Include?\"]]\n",
    "    sht.range('A2').options(index=False, header=False).value = df\n",
    "    n = len(df); last_row = 1 + n\n",
    "    last_row = 1 + len(df)\n",
    "    if n >= 1:\n",
    "        sht.range('E2').formula = \"=B2*C2*D2\"\n",
    "        if n > 1:\n",
    "            sht.range(f\"E2:E{last_row}\").api.FillDown()\n",
    "        sumif_den = f\"SUMIF($G$2:$G${last_row},TRUE,$E$2:$E${last_row})\"\n",
    "        sht.range('F2').formula = f\"=IF({sumif_den}=0,0,IF($G2,E2/{sumif_den},0))\"\n",
    "        if n > 1:\n",
    "            sht.range(f\"F2:F{last_row}\").api.FillDown()\n",
    "        try:\n",
    "            val_rng = sht.range(f\"G2:G{last_row}\").api\n",
    "            val_rng.Validation.Delete()\n",
    "            val_rng.Validation.Add(3, 1, 1, \"TRUE,FALSE\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            sht.range(f\"C2:C{last_row}\").api.NumberFormat = \"0.0000\"\n",
    "            sht.range(f\"D2:D{last_row}\").api.NumberFormat = \"0.0000\"\n",
    "            sht.range(f\"E2:E{last_row}\").api.NumberFormat = \"$0.00\"\n",
    "            sht.range(f\"F2:F{last_row}\").api.NumberFormat = \"0.00%\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    sht.autofit()\n",
    "\n",
    "def update_efficient_frontier_chart(opt_sheet, stats_df, start_s_row, rf_annual, tan_ret, tan_vol, current_point, title_text, target_point=None, previous_point=None, factor_point=None):\n",
    "    \"\"\"Safe no-crash chart updater. Creates/refreshes a single scatter chart named 'Efficient Frontier' on OPT.\"\"\"\n",
    "    try:\n",
    "        co = opt_sheet.api.ChartObjects()\n",
    "        # Find existing chart by title or name\n",
    "        the_chart = None\n",
    "        for i in range(1, co.Count + 1):\n",
    "            ch = co.Item(i).Chart\n",
    "            if ch.HasTitle and \"Efficient Frontier\" in str(getattr(ch.ChartTitle, \"Text\", \"\")):\n",
    "                the_chart = co.Item(i); break\n",
    "        # stats_df was written at A{start_s_row+1} with a header row,\n",
    "            # so first numeric data row is start_s_row+2\n",
    "            first_row = start_s_row + 2\n",
    "            last_row  = first_row + len(stats_df) - 1\n",
    "\n",
    "            x_rng = opt_sheet.range(f\"C{first_row}:C{last_row}\").api  # Volatility (ann.)\n",
    "            y_rng = opt_sheet.range(f\"B{first_row}:B{last_row}\").api  # Achieved Return\n",
    "\n",
    "            if the_chart is None:\n",
    "                the_chart = co.Add(10, 10, 600, 360)  # x,y,w,h\n",
    "                the_chart.Chart.ChartType = 74  # xlXYScatterSmoothNoMarkers\n",
    "                the_chart.Chart.HasTitle = True\n",
    "                the_chart.Chart.ChartTitle.Text = \"Efficient Frontier\"\n",
    "\n",
    "ch = the_chart.Chart\n",
    "        try:\n",
    "            while ch.SeriesCollection().Count > 0:\n",
    "                ch.SeriesCollection(1).Delete()\n",
    "        except Exception:\n",
    "            pass\n",
    "        s1 = ch.SeriesCollection().NewSeries()\n",
    "        s1.Name = \"Efficient Frontier\"\n",
    "        s1.XValues = x_rng\n",
    "        s1.Values  = y_rng\n",
    "        try:\n",
    "            s1.ChartType = 74          # xlXYScatterSmoothNoMarkers\n",
    "            s1.MarkerStyle = -4142     # xlMarkerStyleNone\n",
    "            s1.Smooth = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Tangency point (if available)\n",
    "        if tan_ret is not None and tan_vol is not None and np.isfinite([tan_ret, tan_vol]).all():\n",
    "            s2 = ch.SeriesCollection().NewSeries()\n",
    "            s2.XValues = [float(tan_vol)]\n",
    "            s2.Values  = [float(tan_ret)]\n",
    "            s2.Name = \"Tangency\"\n",
    "        \n",
    "            # CAL line from (0, rf) to (tan_vol, tan_ret)\n",
    "            if rf_annual is not None and np.isfinite([rf_annual]).all():\n",
    "                s_cal = ch.SeriesCollection().NewSeries()\n",
    "                s_cal.XValues = [0.0, float(tan_vol)]\n",
    "                s_cal.Values  = [float(rf_annual), float(tan_ret)]\n",
    "                s_cal.Name = \"CAL\"\n",
    "                try:\n",
    "                    s_cal.ChartType = 74  # scatter with smooth lines\n",
    "                    s_cal.Smooth = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            # Current portfolio point (if available)\n",
    "            if current_point:\n",
    "                s3 = ch.SeriesCollection().NewSeries()\n",
    "                s3.Name = \"Current\"\n",
    "                s3.XValues = [float(current_point[0])]\n",
    "                s3.Values  = [float(current_point[1])]\n",
    "                try:\n",
    "                    s3.ChartType = -4169    # xlXYScatter (markers)\n",
    "                    s3.MarkerStyle = 8      # circle\n",
    "                    s3.MarkerSize = 8\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            \n",
    "            # Previous portfolio point (if available)\n",
    "            if previous_point:\n",
    "                sp = ch.SeriesCollection().NewSeries()\n",
    "                sp.Name = \"Previous\"\n",
    "                sp.XValues = [float(previous_point[0])]\n",
    "                sp.Values  = [float(previous_point[1])]\n",
    "                try:\n",
    "                    sp.ChartType = -4169    # xlXYScatter (markers)\n",
    "                    sp.MarkerStyle = 9      # diamond\n",
    "                    sp.MarkerSize = 9\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Factor-effected point (if available)\n",
    "            if factor_point:\n",
    "                sf = ch.SeriesCollection().NewSeries()\n",
    "                sf.Name = \"Factor-effected\"\n",
    "                sf.XValues = [float(factor_point[0])]\n",
    "                sf.Values  = [float(factor_point[1])]\n",
    "                try:\n",
    "                    sf.ChartType = -4169    # xlXYScatter (markers)\n",
    "                    sf.MarkerStyle = 4      # x\n",
    "                    sf.MarkerSize = 10\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "# Target (post-trade) point (if available)\n",
    "            if target_point:\n",
    "                s4 = ch.SeriesCollection().NewSeries()\n",
    "                s4.Name = \"Target\"\n",
    "                s4.XValues = [float(target_point[0])]\n",
    "                s4.Values  = [float(target_point[1])]\n",
    "                try:\n",
    "                    s4.ChartType = -4169    # xlXYScatter (markers)\n",
    "                    s4.MarkerStyle = 2      # plus\n",
    "                    s4.MarkerSize = 10\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Title with rf label\n",
    "        ch.ChartTitle.Text = title_text if title_text else \"Efficient Frontier\"\n",
    "    except Exception as e:\n",
    "        print(f\"[chart] Skipping chart update (safe wrapper): {e}\")\n",
    "\n",
    "\n",
    "# ---- 10A) Read seeds (no COM; avoids UsedRange issues) ----\n",
    "# Pull last-saved holdings / include flags / tilts from the workbook if available.\n",
    "seed_units, seed_include = _read_holdings_seed_from_path(_XL_PATH, sheet_name=\"Holdings\")\n",
    "tilt_seed = _read_tilts_seed_from_path(_XL_PATH, sheet_name=\"Tilts\")\n",
    "\n",
    "# Ensure MOM exists in the seed and rows are in the canonical order\n",
    "# ---- 10B) Combined dialog (holdings + tilts) ----\n",
    "res = edit_holdings_and_tilts_dialog(\n",
    "    prices=prices,\n",
    "    exclude=EXCLUDE_FROM_OPT,\n",
    "    seed_units=seed_units,\n",
    "    seed_include=seed_include,\n",
    "    seed_tilts=tilt_seed\n",
    ")\n",
    "if res is None:\n",
    "    units = seed_units.copy()\n",
    "    include_flags = seed_include.copy()\n",
    "    last_px_hold = prices.ffill().iloc[-1].reindex(units.index)\n",
    "    tilt_df = tilt_seed.copy()\n",
    "else:\n",
    "    units, last_px_hold, prices, include_flags, tilt_df = res\n",
    "    current_holdings_units = units.copy() \n",
    "\n",
    "# ---- Make optimiser globals available ----\n",
    "current_holdings_units = units\n",
    "securities_opt = list(units.index)\n",
    "lots_df = lots_df  # already loaded earlier in block 7\n",
    "gamma_cgt = 0.005        # soft penalty weight for CGT (tune as desired)\n",
    "beta_brokerage = 0.25    # soft penalty weight for brokerage (tune as desired)\n",
    "\n",
    "# --- helper: rebuild analytics from (possibly updated) prices ---\n",
    "def _rebuild_core_from_prices(prices, fx_ticker=\"USDAUD=X\", period=\"5y\"):\n",
    "    fx_raw = yf.download(fx_ticker, period=period, interval=\"1d\",\n",
    "                         auto_adjust=True, threads=False, progress=False)\n",
    "    fx = fx_raw[\"Close\"] if isinstance(fx_raw, pd.DataFrame) else fx_raw\n",
    "    if isinstance(fx, pd.DataFrame):\n",
    "        fx = fx.iloc[:, 0]\n",
    "    fx = pd.to_numeric(fx, errors=\"coerce\").reindex(prices.index).ffill()\n",
    "\n",
    "    usd_cols = [c for c in prices.columns\n",
    "                if not str(c).endswith(\".AX\") and not str(c).startswith(\"^\")]\n",
    "\n",
    "    prices_aud = prices.copy()\n",
    "\n",
    "    if usd_cols:\n",
    "        prices_aud.update(prices.loc[:, usd_cols].mul(fx, axis=0))\n",
    "\n",
    "    # FIX: fill missing values AFTER FX conversion but BEFORE returns\n",
    "    prices_aud = prices_aud.ffill().bfill()\n",
    "\n",
    "    # Melt into long format\n",
    "    d = (prices_aud.reset_index()\n",
    "         .melt(id_vars=\"Date\", var_name=\"Security\", value_name=\"Close\")\n",
    "         .sort_values([\"Security\", \"Date\"]))\n",
    "\n",
    "    d[\"Return\"] = d.groupby(\"Security\", sort=False)[\"Close\"].pct_change(fill_method=None)\n",
    "    d = d.dropna()\n",
    "\n",
    "    df_cov_wide = d.pivot(index=\"Date\", columns=\"Security\", values=\"Return\").sort_index()\n",
    "    Sigma_daily = df_cov_wide.cov()\n",
    "\n",
    "    d[\"LogRet\"] = np.log1p(d[\"Return\"])\n",
    "    mu_log_ann = d.groupby(\"Security\")[\"LogRet\"].mean() * 252.0\n",
    "    mu_ann_geo = np.expm1(mu_log_ann)\n",
    "\n",
    "    return prices_aud, d, df_cov_wide, Sigma_daily, mu_ann_geo\n",
    "\n",
    "\n",
    "# === Rebuild core analytics ===\n",
    "prices_aud_for_returns, df_melt, df_cov_wide, Sigma_daily, mu_ann_geo = _rebuild_core_from_prices(prices)\n",
    "\n",
    "# Tables used later\n",
    "n_opt = len(securities_opt)\n",
    "cov_plus = Sigma_opt.copy()\n",
    "cov_plus.loc[:, 'w'] = 0.0\n",
    "cov_plus.loc['w', :] = 0.0\n",
    "cov_plus.loc['w', 'w'] = 0.0\n",
    "exp_ret_df = mu_vec_opt.rename(exp_ret_label).to_frame()\n",
    "\n",
    "# FX map used by Holdings + trade plan\n",
    "usd_aud    = get_usd_aud_fx()\n",
    "fx_map_all = fx_to_aud_for_tickers(prices.columns, usd_aud)\n",
    "\n",
    "# ---- 10D) Reopen Excel and WRITE everything, then close ----\n",
    "if USE_XLWINGS:\n",
    "    try:\n",
    "        with xw.App(visible=False, add_book=False) as app:\n",
    "            wb = app.books.open(filename, update_links=False, read_only=False)\n",
    "            \n",
    "            if bool(wb.api.ReadOnly):\n",
    "                # If Excel forces read-only (usually file is already open/locked), write to a new file instead of CSV fallback\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                alt = base + \"_AUTO\" + ext\n",
    "                print(f\"[warn] Workbook opened read-only. Will write to: {alt}\")\n",
    "                wb.close()\n",
    "                wb = app.books.open(alt, update_links=False, read_only=False)\n",
    "            \n",
    "            wb.activate()\n",
    "            app.display_alerts = False\n",
    "            app.screen_updating = False\n",
    "            try: app.api.EnableEvents = False\n",
    "            except Exception: pass\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            # Pick the max-Sharpe portfolio column once for reuse\n",
    "            sh = pd.to_numeric(stats_df['Sharpe'], errors='coerce').fillna(-1)\n",
    "            best_idx = int(sh.values.argmax()) if len(sh) else 0\n",
    "            w_star = W.iloc[:, best_idx].reindex(W.index).fillna(0.0)\n",
    "\n",
    "            # 1) Cov sheet\n",
    "            try:\n",
    "                cov = wb.sheets['Cov']; cov.used_range.clear_contents()\n",
    "            except Exception:\n",
    "                cov = wb.sheets.add('Cov', after=wb.sheets[-1])\n",
    "            cov.range('A1').options(pd.DataFrame, index=True, header=True).value = Sigma_opt\n",
    "\n",
    "            # 2) Input sheet\n",
    "            try:\n",
    "                inp = wb.sheets['Input']; inp.used_range.clear_contents()\n",
    "            except Exception:\n",
    "                inp = wb.sheets.add('Input', after=wb.sheets[-1])\n",
    "            inp.range('A1').options(pd.DataFrame, index=False, header=True).value = df_melt\n",
    "\n",
    "            # 3) OPT sheet\n",
    "            try:\n",
    "                opt = wb.sheets['OPT']; opt.used_range.clear_contents()\n",
    "            except Exception:\n",
    "                opt = wb.sheets.add('OPT', after=wb.sheets[-1])\n",
    "\n",
    "            # Header\n",
    "            opt.range('A1').value = 'Optimal Portfolio Theory (long-only where possible)'\n",
    "            opt.range('A2').value = f\"Generated: {datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "            opt.range('A3').value = 'Expected returns use geometric (log-based) annualisation.'\n",
    "            opt.range('A4').value = 'Variance is daily; annual vol = sqrt(252) * stdev.'\n",
    "            try:\n",
    "                opt.range('A1').api.Font.Bold = True; opt.range('A1').api.Font.Size = 14\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Expected returns\n",
    "            opt.range('A6').value = exp_ret_label\n",
    "            opt.range('A7').options(pd.DataFrame, index=True, header=True).value = exp_ret_df\n",
    "            n_rows = exp_ret_df.shape[0] + 1\n",
    "            try:\n",
    "                opt.range(f\"B8:B{7+n_rows}\").api.NumberFormat = \"0.00%\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Covariance (+ weight row/col)\n",
    "            start_cov_row = 9 + n_rows\n",
    "            opt.range(f\"A{start_cov_row}\").value = 'Covariance Matrix (daily, model) with weight row/column'\n",
    "            opt.range(f\"A{start_cov_row+1}\").options(pd.DataFrame, index=True, header=True).value = cov_plus.fillna(0.0)\n",
    "\n",
    "            # Weights grid\n",
    "            start_w_row = start_cov_row + cov_plus.shape[0] + 4\n",
    "            opt.range(f\"A{start_w_row}\").value = 'Optimised Weights by Target Return'\n",
    "            opt.range(f\"A{start_w_row+1}\").options(pd.DataFrame, index=True, header=True).value = W\n",
    "            # --- Dynamic format for W (optimised weights table) ---\n",
    "            w_first = start_w_row + 1               # header row\n",
    "            w_data_first = w_first + 1              # first data row\n",
    "            w_rows = W.shape[0]\n",
    "            w_cols = W.shape[1]\n",
    "            \n",
    "            # Percent format for all weight cells\n",
    "            rng_w = opt.range(\n",
    "                f\"B{w_data_first}:{chr(ord('A')+w_cols)}{w_data_first + w_rows - 1}\"\n",
    "            )\n",
    "            try:\n",
    "                rng_w.api.NumberFormat = \"0.00%\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Portfolio Statistics\n",
    "            start_s_row = start_w_row + W.shape[0] + 4\n",
    "            opt.range(f\"A{start_s_row}\").value = 'Portfolio Statistics'\n",
    "            opt.range(f\"A{start_s_row+1}\").options(pd.DataFrame, index=False, header=True).value = stats_df\n",
    "            # ==========================================================\n",
    "            #  Efficient Frontier Chart\n",
    "            # ==========================================================\n",
    "            co_old = opt.api.ChartObjects()\n",
    "            to_delete = []\n",
    "            for i in range(1, co_old.Count + 1):\n",
    "                o = co_old.Item(i)\n",
    "                try:\n",
    "                    title_text = o.Chart.ChartTitle.Text\n",
    "                    if \"Efficient Frontier\" in str(title_text):\n",
    "                        to_delete.append(o)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            for o in to_delete:\n",
    "                o.Delete()\n",
    "\n",
    "            co = opt.api.ChartObjects()\n",
    "            chart_obj = None\n",
    "            \n",
    "            # Find existing chart by the *dynamic* title\n",
    "            for i in range(1, co.Count + 1):\n",
    "                o = co.Item(i)\n",
    "                try:\n",
    "                    if o.Chart.HasTitle and \"Efficient Frontier\" in str(o.Chart.ChartTitle.Text):\n",
    "                        chart_obj = o\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # If not found, create it\n",
    "            if chart_obj is None:\n",
    "                left = opt.range(\"I1\").api.Left       # right of the stats table\n",
    "                top  = opt.range(f\"A{start_s_row+1}\").api.Top\n",
    "                width = 480\n",
    "                height = 245\n",
    "            \n",
    "                chart_obj = co.Add(left, top, width, height)\n",
    "                ch = chart_obj.Chart\n",
    "                ch.ChartType = -4169       # XY scatter\n",
    "                ch.HasTitle = True\n",
    "                ch.ChartTitle.Text = chart_title\n",
    "            else:\n",
    "                ch = chart_obj.Chart\n",
    "            \n",
    "            # Reposition every run\n",
    "            chart_obj.Left   = opt.range(\"I1\").api.Left\n",
    "            chart_obj.Top    = opt.range(f\"A{start_s_row+1}\").api.Top\n",
    "            chart_obj.Width  = 480\n",
    "            chart_obj.Height = 245\n",
    "\n",
    "            # ----------------------------------------------------------\n",
    "            # Format Portfolio Statistics\n",
    "            # ----------------------------------------------------------\n",
    "            \n",
    "            stat_rows = stats_df.shape[0]\n",
    "            if stat_rows > 0:\n",
    "                header_row = start_s_row + 1\n",
    "                data_first = header_row + 1\n",
    "            \n",
    "                for col_name, fmt in {\n",
    "                    \"Achieved Return\": \"0.00%\",\n",
    "                    \"Volatility (ann.)\": \"0.00%\",\n",
    "                    \"Sharpe\": \"0.00\"\n",
    "                }.items():\n",
    "                    if col_name in stats_df.columns:\n",
    "                        col_idx = list(stats_df.columns).index(col_name)\n",
    "                        col_letter = chr(ord(\"A\") + col_idx)\n",
    "                        try:\n",
    "                            opt.range(\n",
    "                                f\"{col_letter}{data_first}:{col_letter}{data_first + stat_rows - 1}\"\n",
    "                            ).api.NumberFormat = fmt\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            # ================= Efficient Frontier chart updater =================\n",
    "            def _col_letter(idx0: int) -> str:\n",
    "                n = idx0 + 1  # A=1\n",
    "                letters = \"\"\n",
    "                while n:\n",
    "                    n, rem = divmod(n - 1, 26)\n",
    "                    letters = chr(65 + rem) + letters\n",
    "                return letters\n",
    "            \n",
    "            def _get_chart_by_title(opt_sheet, title_text: str):\n",
    "                # --- Hard reset: delete all previous Efficient Frontier charts ---\n",
    "                co = opt_sheet.api.ChartObjects()\n",
    "                delete_list = []\n",
    "                for i in range(1, co.Count + 1):\n",
    "                    o = co.Item(i)\n",
    "                    try:\n",
    "                        t = o.Chart.ChartTitle.Text\n",
    "                        if \"Efficient Frontier\" in str(t):\n",
    "                            delete_list.append(o)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                \n",
    "                for o in delete_list:\n",
    "                    o.Delete()\n",
    "\n",
    "                \"\"\"Return the COM Chart object whose Title text equals title_text (case/space-insensitive).\"\"\"\n",
    "                def _norm(s): return \" \".join(str(s).split()).casefold()\n",
    "                co = opt_sheet.api.ChartObjects()\n",
    "                want = _norm(title_text)\n",
    "                for i in range(1, co.Count + 1):\n",
    "                    o = co.Item(i)\n",
    "                    try:\n",
    "                        ch = o.Chart\n",
    "                        if ch.HasTitle and _norm(ch.ChartTitle.Text) == want:\n",
    "                            return ch\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                        return None\n",
    "                            \n",
    "                def update_efficient_frontier_chart(\n",
    "                        opt_sheet=opt,\n",
    "                        stats_df=stats_df,\n",
    "                        start_s_row=start_s_row,\n",
    "                        rf_annual=float(rf_annual),\n",
    "                        tan_ret=float(tan_ret),\n",
    "                        tan_vol=float(tan_vol),\n",
    "                        current_point=current_point,\n",
    "                        target_point=target_point,\n",
    "                        previous_point=previous_point,\n",
    "                        factor_point=factor_point,\n",
    "                        title_text=chart_title\n",
    "                    ):\n",
    "                    if title_text is None:\n",
    "                        title_text = chart_title   # dynamic RF label\n",
    "\n",
    "                ch = _get_chart_by_title(opt_sheet, title_text)\n",
    "                if ch is None:\n",
    "                    # Create a new chart in a sensible spot\n",
    "                    co = opt_sheet.api.ChartObjects()\n",
    "                    # left, top, width, height in points \u2014 tweak if you like\n",
    "                    chart_obj = co.Add(480, 20, 480, 360)\n",
    "                    ch = chart_obj.Chart\n",
    "                    ch.ChartType = 74  # XL_XY_SCATTER_SMOOTH_NO_MARKERS (safe default)\n",
    "                    ch.HasTitle = True\n",
    "                    ch.ChartTitle.Text = title_text\n",
    "                    ch.Axes(1).HasTitle = True; ch.Axes(1).AxisTitle.Text = \"Volatility (ann.)\"\n",
    "                    ch.Axes(2).HasTitle = True; ch.Axes(2).AxisTitle.Text = \"Achieved Return\"\n",
    "\n",
    "                \n",
    "                # ---- Validate columns in stats_df ----\n",
    "                cols = list(stats_df.columns)\n",
    "                try:\n",
    "                    j_ret = cols.index(\"Achieved Return\")\n",
    "                    j_vol = cols.index(\"Volatility (ann.)\")\n",
    "                except ValueError:\n",
    "                    raise RuntimeError(\"stats_df must have columns 'Achieved Return' and 'Volatility (ann.)'.\")\n",
    "            \n",
    "                nrows = int(stats_df.shape[0])\n",
    "                if nrows <= 0:\n",
    "                    raise RuntimeError(\"stats_df has no rows; nothing to plot.\")\n",
    "            \n",
    "                # ---- Build Excel range references for X (Vol) and Y (Return) from the table you wrote ----\n",
    "                header_row = start_s_row + 1      # header row in Excel where you wrote stats_df header\n",
    "                first_row  = header_row + 1       # first data row\n",
    "                col_vol = _col_letter(j_vol)      # zero-based -> Excel column letters relative to column A\n",
    "                col_ret = _col_letter(j_ret)\n",
    "                x_rng = opt_sheet.range(f\"{col_vol}{first_row}:{col_vol}{first_row + nrows - 1}\").api\n",
    "                y_rng = opt_sheet.range(f\"{col_ret}{first_row}:{col_ret}{first_row + nrows - 1}\").api\n",
    "            \n",
    "                # ---- Grab the chart ----\n",
    "                rf = rf_annual\n",
    "                ch = _get_chart_by_title(opt_sheet, title_text)\n",
    "            \n",
    "                # ---- Clear existing series (keep object & styling) ----\n",
    "                try:\n",
    "                    while ch.SeriesCollection().Count > 0:\n",
    "                        ch.SeriesCollection(1).Delete()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "                # Excel ChartType constants (avoid win32com constants; use literals)\n",
    "                XL_XY_SCATTER              = -4169  # points only\n",
    "                XL_XY_SCATTER_LINES        = 74     # scatter with lines (markers on)\n",
    "                XL_MARKERSTYLE_NONE        = -4142\n",
    "                XL_MARKERSTYLE_CIRCLE      = 8\n",
    "                XL_MARKERSTYLE_PLUS        = 2\n",
    "                XL_AXIS_CATEGORY           = 1      # for XY charts Excel still treats X as a value axis, but this works for formatting\n",
    "                XL_AXIS_VALUE              = 2\n",
    "            \n",
    "                # ---- Efficient Frontier (smooth line, no markers) ----\n",
    "                s_front = ch.SeriesCollection().NewSeries()\n",
    "                s_front.Name = '=\"Efficient Frontier\"'\n",
    "                s_front.XValues = x_rng\n",
    "                s_front.Values  = y_rng\n",
    "                s_front.ChartType = XL_XY_SCATTER_LINES\n",
    "                try:\n",
    "                    s_front.MarkerStyle = XL_MARKERSTYLE_NONE\n",
    "                    s_front.Smooth = True\n",
    "                    # optional line weight for visibility\n",
    "                    s_front.Format.Line.Weight = 1.5\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "                # ---- CAL (smooth line from (0, rf) to (tan_vol, tan_ret)) ----\n",
    "                if np.all(np.isfinite([rf_annual, tan_ret, tan_vol])):\n",
    "                    s_cal = ch.SeriesCollection().NewSeries()\n",
    "                    s_cal.Name = '=\"CAL\"'\n",
    "                    s_cal.XValues = (0.0, float(tan_vol))\n",
    "                    s_cal.Values  = (float(rf_annual), float(tan_ret))\n",
    "                    s_cal.ChartType = XL_XY_SCATTER_LINES\n",
    "                    try:\n",
    "                        s_cal.MarkerStyle = XL_MARKERSTYLE_NONE\n",
    "                        s_cal.Smooth = True\n",
    "                        s_cal.Format.Line.Weight = 1.25\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            \n",
    "                # ---- MVP (single point at min volatility) ----\n",
    "                try:\n",
    "                    vol_series = pd.to_numeric(stats_df[\"Volatility (ann.)\"], errors=\"coerce\")\n",
    "                    ret_series = pd.to_numeric(stats_df[\"Achieved Return\"], errors=\"coerce\")\n",
    "                    mask = vol_series.notna() & ret_series.notna()\n",
    "                    if mask.any():\n",
    "                        idx_mvp = vol_series[mask].idxmin()\n",
    "                        mvp_x = float(vol_series.loc[idx_mvp])\n",
    "                        mvp_y = float(ret_series.loc[idx_mvp])\n",
    "                        s_mvp = ch.SeriesCollection().NewSeries()\n",
    "                        s_mvp.Name    = '=\"MVP\"'\n",
    "                        s_mvp.XValues = (mvp_x,)   # tuples, not lists, play nicer with COM\n",
    "                        s_mvp.Values  = (mvp_y,)\n",
    "                        s_mvp.ChartType = XL_XY_SCATTER\n",
    "                        try:\n",
    "                            s_mvp.MarkerStyle = XL_MARKERSTYLE_CIRCLE\n",
    "                            s_mvp.MarkerSize  = 8\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                except Exception as e:\n",
    "                    print(f\"[chart] MVP error: {e}\")\n",
    "            \n",
    "                # ---- Current portfolio (single point) ----\n",
    "                if current_point is not None:\n",
    "                    try:\n",
    "                        curr_vol, curr_ret = map(float, current_point)\n",
    "                        if np.isfinite(curr_vol) and np.isfinite(curr_ret):\n",
    "                            s_cur = ch.SeriesCollection().NewSeries()\n",
    "                            s_cur.Name    = '=\"Current\"'\n",
    "                            s_cur.XValues = (curr_vol,)\n",
    "                            s_cur.Values  = (curr_ret,)\n",
    "                            s_cur.ChartType = XL_XY_SCATTER\n",
    "                            try:\n",
    "                                s_cur.MarkerStyle = XL_MARKERSTYLE_PLUS\n",
    "                                s_cur.MarkerSize  = 10\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                    except Exception as e:\n",
    "                        print(f\"[chart] Current point error: {e}\")\n",
    "                        print(\"Did Block 7 create results?\", \"results\" in globals())\n",
    "\n",
    "                # ---- Axis number formats to percentages (best-effort) ----\n",
    "                for ax_type in (XL_AXIS_CATEGORY, XL_AXIS_VALUE):\n",
    "                    try:\n",
    "                        ch.Axes(ax_type).TickLabels.NumberFormat = \"0.0%\"\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            \n",
    "                # Keep legend; if chart had one it stays. Optionally ensure it exists:\n",
    "                try:\n",
    "                    ch.HasLegend = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # ================= End chart updater =================            \n",
    "            # -------- Example usage (fits your existing variables) --------\n",
    "            # Compute current portfolio point if you want it plotted; otherwise pass current_point=None.\n",
    "            current_point = None\n",
    "            target_point = None\n",
    "            previous_point = None\n",
    "            factor_point = None\n",
    "            \n",
    "            try:\n",
    "                mu_use = mu_vec_opt.reindex(Sigma_opt.index).fillna(0.0).values\n",
    "                S_use  = Sigma_opt.values\n",
    "            \n",
    "                # --- Current (pre-trade) ---\n",
    "                curr_w = current_holdings_weights(\n",
    "                    units=units,\n",
    "                    last_prices=last_px_hold,\n",
    "                    investable=list(Sigma_opt.index),\n",
    "                    fx_to_aud=fx_map_all\n",
    "                ).reindex(Sigma_opt.index).fillna(0.0)\n",
    "                wv0 = curr_w.values\n",
    "                curr_ret = float(mu_use @ wv0)\n",
    "                curr_vol = float(np.sqrt(wv0 @ S_use @ wv0) * np.sqrt(252.0))\n",
    "                current_point = (curr_vol, curr_ret)\n",
    "\n",
    "                # --- Previous (seed / last-saved holdings) ---\n",
    "                try:\n",
    "                    if seed_units is not None and isinstance(seed_units, pd.Series) and not seed_units.empty:\n",
    "                        prev_w = current_holdings_weights(\n",
    "                            units=seed_units,\n",
    "                            last_prices=last_px_hold,\n",
    "                            investable=list(Sigma_opt.index),\n",
    "                            fx_to_aud=fx_map_all\n",
    "                        ).reindex(Sigma_opt.index).fillna(0.0)\n",
    "                        wp = prev_w.values\n",
    "                        prev_ret = float(mu_use @ wp)\n",
    "                        prev_vol = float(np.sqrt(wp @ S_use @ wp) * np.sqrt(252.0))\n",
    "                        previous_point = (prev_vol, prev_ret)\n",
    "                except Exception as _e_prev:\n",
    "                    previous_point = None\n",
    "\n",
    "                # --- Factor-effected point (achievable tilt weights) ---\n",
    "                try:\n",
    "                    if \"B\" in globals() and \"f_mean_ann\" in globals() and \"Fcov_daily\" in globals():\n",
    "                        B_sub = B.reindex(Sigma_opt.index).dropna(how=\"any\")\n",
    "                        if not B_sub.empty:\n",
    "                            _t_rec, _w_fac = recommend_factor_tilts_achievable(B_sub, f_mean_ann, Fcov_daily)\n",
    "                            w_fac = pd.Series(_w_fac, index=B_sub.index).reindex(Sigma_opt.index).fillna(0.0)\n",
    "                            if float(w_fac.sum()) != 0:\n",
    "                                w_fac = w_fac / float(w_fac.sum())\n",
    "                            wf = w_fac.values\n",
    "                            fac_ret = float(mu_use @ wf)\n",
    "                            fac_vol = float(np.sqrt(wf @ S_use @ wf) * np.sqrt(252.0))\n",
    "                            factor_point = (fac_vol, fac_ret)\n",
    "                except Exception as _e_fac:\n",
    "                    factor_point = None\n",
    "\n",
    "            \n",
    "                # --- Target (post-trade) using optimiser weights ---\n",
    "                w1 = pd.to_numeric(w_star, errors=\"coerce\").reindex(Sigma_opt.index).fillna(0.0)\n",
    "                if float(w1.sum()) != 0:\n",
    "                    w1 = w1 / float(w1.sum())\n",
    "                wv1 = w1.values\n",
    "                tgt_ret = float(mu_use @ wv1)\n",
    "                tgt_vol = float(np.sqrt(wv1 @ S_use @ wv1) * np.sqrt(252.0))\n",
    "                target_point = (tgt_vol, tgt_ret)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"[chart] Point compute error: {e}\")\n",
    "                current_point = None\n",
    "                target_point = None\n",
    "\n",
    "            \n",
    "            # Finally, update the existing chart on 'OPT'\n",
    "            # --- Efficient Frontier Chart Update (safe version) ---\n",
    "            try:\n",
    "                update_efficient_frontier_chart(\n",
    "                    opt_sheet=opt,\n",
    "                    stats_df=stats_df,\n",
    "                    start_s_row=start_s_row,\n",
    "                    rf_annual=float(rf_annual),\n",
    "                    tan_ret=float(tan_ret),\n",
    "                    tan_vol=float(tan_vol),\n",
    "                    current_point=current_point,\n",
    "                    title_text=chart_title\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[chart] Skipping chart update: {e}\")\n",
    " \n",
    "            co = opt.api.ChartObjects()\n",
    "            for i in range(1, co.Count + 1):\n",
    "                o = co.Item(i)\n",
    "                title = \"\"\n",
    "                try:\n",
    "                    if o.Chart.HasTitle:\n",
    "                        title = o.Chart.ChartTitle.Text\n",
    "                except Exception:\n",
    "                    pass\n",
    "                print(i, \"name:\", o.Name, \"| title:\", title)\n",
    "\n",
    "          \n",
    "            # ---- Build trade plan & costs BEFORE writing Trade Plan/Costs/Tilts ----\n",
    "            trade_rec, resid_rec = make_trade_plan(\n",
    "                units, last_px_hold, fx_map_all, w_star, include_zero_lines=True, include_flags=include_flags\n",
    "            )\n",
    "            \n",
    "            # --- Ensure 'Security' is a proper column BEFORE any downstream functions ---\n",
    "            trade_rec = trade_rec.copy()\n",
    "            trade_rec.index.name = \"Security\"\n",
    "            if \"Security\" not in trade_rec.columns:\n",
    "                trade_rec = trade_rec.reset_index()\n",
    "            \n",
    "            # --- Now it is safe to compute costs (some code expects trade_rec[\"Security\"]) ---\n",
    "            costs_rec = evaluate_transaction_costs(\n",
    "                trade_rec, lots_df, pd.Timestamp(prices.index[-1]), MARGINAL_TAX_RATE\n",
    "            )\n",
    "            \n",
    "            # --- Add per-row brokerage (keep your existing logic) ---\n",
    "            row_b = costs_rec.get(\"row_brokerage\", pd.Series(0.0, index=trade_rec.index))\n",
    "            row_b = pd.to_numeric(row_b, errors=\"coerce\").reindex(trade_rec.index).fillna(0.0)\n",
    "            \n",
    "            # No brokerage where \u0394 Units == 0\n",
    "            row_b = np.where(pd.to_numeric(trade_rec[\"\u0394 Units\"], errors=\"coerce\").fillna(0).astype(int) == 0, 0.0, row_b)\n",
    "            trade_rec[\"Brokerage (AUD)\"] = pd.Series(row_b, index=trade_rec.index).round(2)\n",
    "            \n",
    "            trade_rec.drop(\n",
    "                columns=[c for c in trade_rec.columns if str(c).lower().startswith(\"promo\")],\n",
    "                errors=\"ignore\",\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            # --- Lot expansion (safe now that 'Security' exists as a column) ---\n",
    "            lot_expanded = expand_with_lots(\n",
    "                trade_rec,\n",
    "                lots_df,\n",
    "                sale_date=pd.Timestamp(prices.index[-1]),\n",
    "                method=\"FIFO\"\n",
    "            )\n",
    "            print(\"\\n=== LOT-EXPANDED TABLE ===\")\n",
    "            print(lot_expanded.head(20))\n",
    "\n",
    "\n",
    "            if \"Security\" not in trade_rec.columns and trade_rec.index.name == \"Security\":\n",
    "                trade_rec = trade_rec.reset_index()\n",
    "            if \"Security\" not in trade_rec.columns:\n",
    "                trade_rec.insert(0, \"Security\", trade_rec.index.astype(str))\n",
    "            \n",
    "            if isinstance(lot_expanded, pd.DataFrame):\n",
    "                if \"Security\" not in lot_expanded.columns and lot_expanded.index.name == \"Security\":\n",
    "                    lot_expanded = lot_expanded.reset_index()\n",
    "\n",
    "            \n",
    "            # === Build CGT audit table (parcel-level) ===\n",
    "            try:\n",
    "                tax_bkd = costs_rec.get(\"breakdown\", {})\n",
    "                audit_df = tax_bkd.get(\"audit\", pd.DataFrame()).copy()\n",
    "\n",
    "                if not audit_df.empty:\n",
    "                    # Ensure proper dtypes\n",
    "                    audit_df[\"AcqDate\"] = pd.to_datetime(audit_df[\"AcqDate\"], errors=\"coerce\")\n",
    "                    audit_df[\"SaleDate\"] = pd.to_datetime(audit_df[\"SaleDate\"], errors=\"coerce\")\n",
    "                    audit_df[\"Qty\"]      = pd.to_numeric(audit_df[\"Qty\"], errors=\"coerce\")\n",
    "                    audit_df[\"Proceeds\"] = pd.to_numeric(audit_df[\"Proceeds\"], errors=\"coerce\")\n",
    "                    audit_df[\"CostBase\"] = pd.to_numeric(audit_df[\"CostBase\"], errors=\"coerce\")\n",
    "                    audit_df[\"Gain\"]     = pd.to_numeric(audit_df[\"Gain\"], errors=\"coerce\")\n",
    "\n",
    "                    # Holding period & discount flag (12-month rule)\n",
    "                    audit_df[\"HoldingDays\"] = (audit_df[\"SaleDate\"] - audit_df[\"AcqDate\"]).dt.days\n",
    "                    audit_df[\"LongTermEligible\"] = audit_df[\"LongTermEligible\"].astype(bool)\n",
    "\n",
    "                    # 50% discount only for positive gains that are LT eligible\n",
    "                    audit_df[\"DiscountRate\"] = 0.0\n",
    "                    audit_df.loc[(audit_df[\"Gain\"] > 0) & (audit_df[\"LongTermEligible\"]), \"DiscountRate\"] = 0.5\n",
    "\n",
    "                    audit_df[\"DiscountedGainIllustrative\"] = audit_df[\"Gain\"]\n",
    "                    mask_disc = (audit_df[\"Gain\"] > 0) & (audit_df[\"LongTermEligible\"])\n",
    "                    audit_df.loc[mask_disc, \"DiscountedGainIllustrative\"] = (\n",
    "                        audit_df.loc[mask_disc, \"Gain\"] * 0.5\n",
    "                    )\n",
    "\n",
    "                    # === Write parcel-level audit sheet ===\n",
    "                    try:\n",
    "                        try:\n",
    "                            sht_cgt = wb.sheets[\"CGT_Audit\"]\n",
    "                        except Exception:\n",
    "                            sht_cgt = wb.sheets.add(\"CGT_Audit\", after=wb.sheets[-1])\n",
    "\n",
    "                        sht_cgt.used_range.clear_contents()\n",
    "                        sht_cgt.range(\"A1\").value = [[\n",
    "                            \"Security\",\n",
    "                            \"Qty\",\n",
    "                            \"AcqDate\",\n",
    "                            \"SaleDate\",\n",
    "                            \"Proceeds\",\n",
    "                            \"CostBase\",\n",
    "                            \"Gain\",\n",
    "                            \"LongTermEligible\",\n",
    "                            \"HoldingDays\",\n",
    "                            \"DiscountRate\",\n",
    "                            \"DiscountedGainIllustrative\",\n",
    "                        ]]\n",
    "                        sht_cgt.range(\"A2\").options(index=False, header=False).value = audit_df[\n",
    "                            [\n",
    "                                \"Security\",\n",
    "                                \"Qty\",\n",
    "                                \"AcqDate\",\n",
    "                                \"SaleDate\",\n",
    "                                \"Proceeds\",\n",
    "                                \"CostBase\",\n",
    "                                \"Gain\",\n",
    "                                \"LongTermEligible\",\n",
    "                                \"HoldingDays\",\n",
    "                                \"DiscountRate\",\n",
    "                                \"DiscountedGainIllustrative\",\n",
    "                            ]\n",
    "                        ]\n",
    "                    except Exception as e_cgt_sheet:\n",
    "                        print(f\"[cgt] could not write CGT_Audit sheet: {e_cgt_sheet}\")\n",
    "\n",
    "                    # === Optional security-level summary ===\n",
    "                    try:\n",
    "                        sec_grp = audit_df.groupby(\"Security\", as_index=False).agg(\n",
    "                            ProceedsTotal=(\"Proceeds\", \"sum\"),\n",
    "                            CostBaseTotal=(\"CostBase\", \"sum\"),\n",
    "                            GainTotal=(\"Gain\", \"sum\"),\n",
    "                        )\n",
    "\n",
    "                        lt_mask = audit_df[\"LongTermEligible\"]\n",
    "                        st_mask = ~audit_df[\"LongTermEligible\"]\n",
    "\n",
    "                        lt_sum = (\n",
    "                            audit_df.loc[lt_mask]\n",
    "                            .groupby(\"Security\")[\"Gain\"]\n",
    "                            .sum()\n",
    "                            .rename(\"LongTermGain\")\n",
    "                        )\n",
    "                        st_sum = (\n",
    "                            audit_df.loc[st_mask]\n",
    "                            .groupby(\"Security\")[\"Gain\"]\n",
    "                            .sum()\n",
    "                            .rename(\"ShortTermGain\")\n",
    "                        )\n",
    "\n",
    "                        sec_summary = (\n",
    "                            sec_grp\n",
    "                            .merge(lt_sum, on=\"Security\", how=\"left\")\n",
    "                            .merge(st_sum, on=\"Security\", how=\"left\")\n",
    "                            .fillna(0.0)\n",
    "                        )\n",
    "\n",
    "                        sht_cgt.range(\"L1\").value = [[\n",
    "                            \"Security\",\n",
    "                            \"ProceedsTotal\",\n",
    "                            \"CostBaseTotal\",\n",
    "                            \"GainTotal\",\n",
    "                            \"LongTermGain\",\n",
    "                            \"ShortTermGain\",\n",
    "                        ]]\n",
    "                        sht_cgt.range(\"L2\").options(index=False, header=False).value = sec_summary[\n",
    "                            [\n",
    "                                \"Security\",\n",
    "                                \"ProceedsTotal\",\n",
    "                                \"CostBaseTotal\",\n",
    "                                \"GainTotal\",\n",
    "                                \"LongTermGain\",\n",
    "                                \"ShortTermGain\",\n",
    "                            ]\n",
    "                        ]\n",
    "                    except Exception as e_cgt_summary:\n",
    "                        print(f\"[cgt] could not write CGT summary: {e_cgt_summary}\")\n",
    "\n",
    "                else:\n",
    "                    print(\"[cgt] audit_df is empty (no CGT-relevant sells).\")\n",
    "\n",
    "            except Exception as e_cgt:\n",
    "                print(f\"[cgt] error building CGT audit table: {e_cgt}\")\n",
    "\n",
    "            \n",
    "            # ---- Achieved factor tilts table (from B and w_star) ----\n",
    "            tilts_out = None\n",
    "            if (B is not None) and (not B.empty):\n",
    "                factor_order = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"MOM\"]\n",
    "                w_use = w_star.reindex(B.index).fillna(0.0)\n",
    "                if float(w_use.sum()) > 0:\n",
    "                    w_use = w_use / float(w_use.sum())\n",
    "                # Use the chosen portfolio weights; align to B\u2019s asset index and normalise\n",
    "                w_vec = pd.to_numeric(w_star, errors=\"coerce\")\n",
    "                w_vec = w_vec.reindex(B.index).fillna(0.0)\n",
    "                if float(w_vec.sum()) > 0:\n",
    "                    w_vec = w_vec / float(w_vec.sum())\n",
    "                \n",
    "                achieved_series = (B.T @ w_vec).rename(\"Achieved \u03b2\")\n",
    "                if \"factor_order\" in globals() and factor_order:\n",
    "                    achieved_series = achieved_series.reindex(factor_order)\n",
    "\n",
    "                if isinstance(tilt_df, pd.DataFrame) and not tilt_df.empty:\n",
    "                    tgt = tilt_df.reindex(factor_order)\n",
    "                    tilts_out = pd.DataFrame({\n",
    "                        \"Use?\":  tgt[\"Use?\"].astype(str).str.upper().isin([\"TRUE\",\"1\",\"Y\",\"YES\",\"T\"]).map({True:\"Yes\", False:\"No\"}),\n",
    "                        \"Target \u03b2\":  pd.to_numeric(tgt[\"Target\"], errors=\"coerce\"),\n",
    "                        \"Band\":      pd.to_numeric(tgt[\"Band\"],   errors=\"coerce\"),\n",
    "                        \"Achieved \u03b2\": achieved_series,\n",
    "                    })\n",
    "                    tilts_out[\"Diff\"] = tilts_out[\"Achieved \u03b2\"] - tilts_out[\"Target \u03b2\"]\n",
    "                    tilts_out[\"Within Band?\"] = (tilts_out[\"Diff\"].abs() <= tilts_out[\"Band\"]).map({True: \"Yes\", False: \"No\"})\n",
    "                else:\n",
    "                    tilts_out = achieved_series.to_frame()\n",
    "\n",
    "            # ---------- Layout anchors (avoid overlaps) ----------\n",
    "            anchor_row = start_s_row + stats_df.shape[0] + 4\n",
    "            TP_COL, COST_COL, TILT_COL = \"A\", \"J\", \"M\"\n",
    "\n",
    "            # ---------- LEFT: Trade Plan ----------\n",
    "            opt.range(f\"{TP_COL}{anchor_row}\").value = \"Trade Plan (rounded units)\"\n",
    "            opt.range(f\"{TP_COL}{anchor_row+1}\").options(pd.DataFrame, index=False, header=True).value = trade_rec\n",
    "            # --- Dynamic formatting for Trade Plan table ---\n",
    "            tp_start = anchor_row + 1\n",
    "            tp_data_first = tp_start + 1\n",
    "            tp_rows = trade_rec.shape[0]\n",
    "            tp_cols = trade_rec.shape[1]\n",
    "            \n",
    "            for col in trade_rec.columns:\n",
    "                col_idx = list(trade_rec.columns).index(col)\n",
    "                col_letter = chr(ord(\"A\") + col_idx)\n",
    "                rng = opt.range(f\"{col_letter}{tp_data_first}:{col_letter}{tp_data_first + tp_rows - 1}\")\n",
    "            \n",
    "                fmt = None\n",
    "                if \"Px\" in col or \"Flow\" in col or \"Brokerage\" in col:\n",
    "                    fmt = \"$#,##0.00\"\n",
    "                elif \"Units\" in col:\n",
    "                    fmt = \"0\"\n",
    "                if fmt:\n",
    "                    try:\n",
    "                        rng.api.NumberFormat = fmt\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            # --- Add portfolio value & cash summary underneath the Trade Plan ---\n",
    "            net_invested = 0.0\n",
    "            cash_balance = 0.0\n",
    "        \n",
    "            if not trade_rec.empty:\n",
    "                # Value of target holdings\n",
    "                tgt_units = pd.to_numeric(trade_rec[\"Target Units\"], errors=\"coerce\").fillna(0.0)\n",
    "                last_px_aud = pd.to_numeric(trade_rec[\"Last Px (AUD)\"], errors=\"coerce\").fillna(0.0)\n",
    "                net_invested = float((tgt_units * last_px_aud).sum())\n",
    "        \n",
    "                # Net cash after trades (positive = cash released, negative = extra cash needed)\n",
    "                cash_balance = float(pd.to_numeric(trade_rec[\"Cash Flow (AUD)\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "        \n",
    "            total_portfolio = net_invested + cash_balance\n",
    "        \n",
    "            summary_row = anchor_row + trade_rec.shape[0] + 4\n",
    "            opt.range(f\"{TP_COL}{summary_row}\").value = [\n",
    "                [\"Portfolio Value (Holdings)\", net_invested],\n",
    "                [\"Cash\",                      cash_balance],\n",
    "                [\"Total Portfolio\",           total_portfolio],\n",
    "            ]\n",
    "            try:\n",
    "                # Bold the three summary labels and format the numbers as currency\n",
    "                rng_labels = opt.range(f\"{TP_COL}{summary_row}:{TP_COL}{summary_row+2}\").api\n",
    "                rng_labels.Font.Bold = True\n",
    "                rng_vals = opt.range(f\"{TP_COL}{summary_row}:{TP_COL}{summary_row+2}\").offset(0, 1).api\n",
    "                rng_vals.NumberFormat = \"$0.00\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # ---------- MIDDLE: Transaction Costs summary ----------\n",
    "            opt.range(f\"{COST_COL}{anchor_row}\").value = \"Transaction Costs (AUD)\"\n",
    "            opt.range(f\"{COST_COL}{anchor_row+1}\").value = [\n",
    "                [\"Brokerage\", \"CGT Tax\", \"Total\"],\n",
    "                [costs_rec[\"brokerage\"], costs_rec[\"cgt_tax\"], costs_rec[\"total_cost\"]],\n",
    "            ]\n",
    "            try:\n",
    "                opt.range(f\"{COST_COL}{anchor_row+2}\").api.NumberFormat = \"0.00\"\n",
    "                opt.range(f\"{COST_COL}{anchor_row+2}\").offset(0,1).api.NumberFormat = \"0.00\"\n",
    "                opt.range(f\"{COST_COL}{anchor_row+2}\").offset(0,2).api.NumberFormat = \"0.00\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # ---------- RIGHT: Achieved Factor Tilts ----------\n",
    "            if tilts_out is not None:\n",
    "                opt.range(f\"{TILT_COL}{anchor_row}\").value = \"Achieved Factor Tilts vs Targets\"\n",
    "                opt.range(f\"{TILT_COL}{anchor_row+1}\").options(pd.DataFrame, index=True, header=True).value = tilts_out\n",
    "                t_rows = tilts_out.shape[0] + 1\n",
    "                t_first = anchor_row + 1\n",
    "                t_data_first = t_first + 1\n",
    "                try:\n",
    "                    for col_name in [\"Target \u03b2\",\"Band\",\"Achieved \u03b2\",\"Diff\"]:\n",
    "                        if col_name in tilts_out.columns:\n",
    "                            idx = list(tilts_out.columns).index(col_name)\n",
    "                            col_letter = chr(ord(TILT_COL) + 1 + idx)  # after index column\n",
    "                            opt.range(f\"{col_letter}{t_data_first}:{col_letter}{t_first+t_rows}\").api.NumberFormat = \"0.000\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # ---------- BELOW RIGHT: Factor Feasible Ranges (long-only, sum=1) ----------\n",
    "            if (B is not None) and (not B.empty):\n",
    "                factor_order = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"MOM\"]\n",
    "                rng_df = compute_factor_feasible_ranges(B, include_flags=include_flags, factor_order=factor_order)\n",
    "            \n",
    "                # Optional: show your target & achieved alongside the ranges\n",
    "                if isinstance(tilts_out, pd.DataFrame):\n",
    "                    # pull Target and Achieved columns safely\n",
    "                    tgt = pd.to_numeric(tilts_out.get(\"Target \u03b2\", np.nan), errors=\"coerce\")\n",
    "                    ach = pd.to_numeric(tilts_out.get(\"Achieved \u03b2\", np.nan), errors=\"coerce\")\n",
    "                    rng_df = rng_df.join(tgt.rename(\"Target \u03b2\")).join(ach.rename(\"Achieved \u03b2\"))\n",
    "                    rng_df[\"Within Range?\"] = (rng_df[\"Target \u03b2\"] >= rng_df[\"Min \u03b2\"]) & (rng_df[\"Target \u03b2\"] <= rng_df[\"Max \u03b2\"])\n",
    "            \n",
    "                # place a few rows *below* the achieved-tilts table to avoid overlap\n",
    "                tilt_rows = (tilts_out.shape[0] + 2) if isinstance(tilts_out, pd.DataFrame) else 3\n",
    "                ranges_anchor = anchor_row + tilt_rows + 2\n",
    "            \n",
    "                opt.range(f\"{TILT_COL}{ranges_anchor}\").value = \"Factor Feasible Ranges (long-only, sum=1)\"\n",
    "                opt.range(f\"{TILT_COL}{ranges_anchor+1}\").options(pd.DataFrame, index=True, header=True).value = rng_df\n",
    "            \n",
    "                # number formats\n",
    "                rr = ranges_anchor + 1\n",
    "                rr_rows = rng_df.shape[0] + 1\n",
    "                try:\n",
    "                    # format numeric columns to 3 decimals if present\n",
    "                    for col_name in [\"Min \u03b2\",\"Max \u03b2\",\"Target \u03b2\",\"Achieved \u03b2\"]:\n",
    "                        if col_name in rng_df.columns:\n",
    "                            idx = list(rng_df.columns).index(col_name)\n",
    "                            # first data column is one to the right of TILT_COL\n",
    "                            col_letter = chr(ord(TILT_COL) + 1 + idx)\n",
    "                            opt.range(f\"{col_letter}{rr+1}:{col_letter}{rr+rr_rows}\").api.NumberFormat = \"0.000\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Final tidy\n",
    "            try: opt.autofit()\n",
    "            except Exception: pass\n",
    "\n",
    "            # 4) FF5F sheet (optional transparency)\n",
    "            try:\n",
    "                ff5s = wb.sheets['FF5F']; ff5s.used_range.clear_contents()\n",
    "            except Exception:\n",
    "                ff5s = wb.sheets.add('FF5F', after=wb.sheets[-1])\n",
    "            ff5s.range('A1').options(pd.DataFrame, index=True, header=True).value = ff_aud\n",
    "\n",
    "            # ---- Update Lots and overwrite Holdings with target units (for next run) ----\n",
    "            UPDATED_LOTS = _update_lots_after_trades(lots_df, trade_rec, pd.Timestamp(prices.index[-1]), fx_map_all)\n",
    "            try:\n",
    "                sht_lots = wb.sheets['Lots']\n",
    "            except Exception:\n",
    "                sht_lots = wb.sheets.add('Lots', after=wb.sheets[-1])\n",
    "            sht_lots.used_range.clear_contents()\n",
    "            sht_lots.range(\"A1\").value = [[\"Security\",\"AcqDate\",\"Units\",\"CostBaseAUD\"]]\n",
    "            sht_lots.range(\"A2\").options(index=False, header=False).value = UPDATED_LOTS\n",
    "\n",
    "            tgt_units_full = compute_target_units_for_holdings(\n",
    "                units, last_px_hold, fx_map_all, w_star, include_flags\n",
    "            )\n",
    "            _write_holdings_sheet(wb, prices, tgt_units_full, include_flags,\n",
    "                                  sheet_name=\"Holdings\", fx_to_aud_map=fx_map_all)\n",
    "\n",
    "            # --- Step 1: Compute current portfolio values ---\n",
    "            if not trade_rec.empty:\n",
    "                trade_rec[\"Target Units\"] = pd.to_numeric(trade_rec[\"Target Units\"], errors=\"coerce\").fillna(0.0)\n",
    "                trade_rec[\"Last Px (AUD)\"] = pd.to_numeric(trade_rec[\"Last Px (AUD)\"], errors=\"coerce\").fillna(0.0)\n",
    "                trade_rec[\"Value\"] = trade_rec[\"Target Units\"] * trade_rec[\"Last Px (AUD)\"]\n",
    "                net_invested = float(trade_rec[\"Value\"].sum())\n",
    "                # Net cash after trades (already net of brokerage)\n",
    "                cash_balance = float(pd.to_numeric(trade_rec[\"Cash Flow (AUD)\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "            else:\n",
    "                net_invested = 0.0\n",
    "                cash_balance = 0.0\n",
    "        \n",
    "            # Brokerage (for reporting only \u2013 already baked into cash_balance)\n",
    "            total_brokerage = float(costs_rec.get(\"brokerage\", 0.0))\n",
    "        \n",
    "            # Total portfolio = holdings + cash\n",
    "            total_portfolio = net_invested + cash_balance\n",
    "            \n",
    "            print(f\"[debug] Current totals \u2192 Portfolio: {total_portfolio:.2f}, Net Invested: {net_invested:.2f}\")\n",
    "            \n",
    "            # --- Step 2: Load previous run data (AFTER calculating current totals) ---\n",
    "            if os.path.exists(state_path):\n",
    "                with open(state_path, \"r\") as f:\n",
    "                    prev_state = json.load(f)\n",
    "                previous_portfolio = prev_state.get(\"portfolio_value\", 0.0)\n",
    "                previous_invested = prev_state.get(\"net_invested\", 0.0)\n",
    "                print(f\"[debug] Previous totals \u2192 Portfolio: {previous_portfolio:.2f}, Net Invested: {previous_invested:.2f}\")\n",
    "            else:\n",
    "                previous_portfolio = 0.0\n",
    "                previous_invested = 0.0\n",
    "                print(\"[info] No previous state file found \u2014 starting fresh deltas at 0.\")\n",
    "            \n",
    "            # --- Step 3: Compute deltas for PowerPoint ---\n",
    "            results = {\n",
    "                \"total_brokerage\": total_brokerage,\n",
    "                \"net_invested\": net_invested,\n",
    "                \"total_portfolio_value\": total_portfolio,\n",
    "                \"portfolio_change\": total_portfolio - previous_portfolio,\n",
    "                \"net_invested_change\": net_invested - previous_invested,\n",
    "            }\n",
    "            \n",
    "            # --- Step 4: Save current state for next comparison ---\n",
    "            with open(state_path, \"w\") as f:\n",
    "                json.dump(\n",
    "                    {\"portfolio_value\": total_portfolio, \"net_invested\": net_invested},\n",
    "                    f,\n",
    "                    indent=2\n",
    "                )\n",
    "            print(f\"[debug] Saved new state \u2192 Portfolio: {total_portfolio:.2f}, Net Invested: {net_invested:.2f}\")\n",
    "            \n",
    "            # --- Step 5: Generate PowerPoint summary ---\n",
    "            trades = trade_rec.copy()\n",
    "            charts = None\n",
    "            \n",
    "            # --- Step 6: Compute PortfolioValue for PowerPoint charts (no Excel readback) ---\n",
    "            try:\n",
    "                # Use the in-memory target units you already computed\n",
    "                units_ser = pd.to_numeric(pd.Series(tgt_units_full), errors=\"coerce\").fillna(0.0)\n",
    "                valid_tickers = [t for t in units_ser.index.astype(str) if t in prices.columns]\n",
    "            \n",
    "                if not valid_tickers:\n",
    "                    raise ValueError(\"No valid tickers found in prices for target holdings.\")\n",
    "            \n",
    "                port_prices = prices[valid_tickers].copy().ffill().bfill()\n",
    "                u = units_ser.reindex(valid_tickers).astype(float).values\n",
    "                portfolio_value_series = (port_prices * u).sum(axis=1).ffill().bfill()\n",
    "            \n",
    "                print(f\"[pptx prep] PortfolioValue series computed for {len(valid_tickers)} securities.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[pptx prep] Could not compute PortfolioValue: {e}\")\n",
    "         \n",
    "            try:\n",
    "                ppt_path = export_to_ppt(results, trades, charts)\n",
    "            except Exception as e:\n",
    "                print(f\"[pptx] Skipped PowerPoint generation: {e}\")\n",
    "\n",
    "           \n",
    "            wb.save()\n",
    "            wb.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Excel fallback] xlwings/COM error \u2192 exporting CSVs instead: {e}\")\n",
    "        export_dir = os.path.join(os.path.dirname(filename), \"Exports\")\n",
    "        try: os.makedirs(export_dir, exist_ok=True)\n",
    "        except Exception: pass\n",
    "        try: exp_ret_df.to_csv(os.path.join(export_dir, \"expected_returns.csv\"))\n",
    "        except Exception as ee: print(f\"[export] expected_returns.csv: {ee}\")\n",
    "        try: cov_plus.to_csv(os.path.join(export_dir, \"covariance_plus.csv\"))\n",
    "        except Exception as ee: print(f\"[export] covariance_plus.csv: {ee}\")\n",
    "        try: W.to_csv(os.path.join(export_dir, \"weights_grid.csv\"))\n",
    "        except Exception as ee: print(f\"[export] weights_grid.csv: {ee}\")\n",
    "        try: stats_df.to_csv(os.path.join(export_dir, \"portfolio_stats.csv\"), index=False)\n",
    "        except Exception as ee: print(f\"[export] portfolio_stats.csv: {ee}\")\n",
    "        try: tilt_df.to_csv(os.path.join(export_dir, \"tilts.csv\"))\n",
    "        except Exception as ee: print(f\"[export] tilts.csv: {ee}\")\n",
    "        try: df_melt.to_csv(os.path.join(export_dir, \"returns_long.csv\"), index=False)\n",
    "        except Exception as ee: print(f\"[export] returns_long.csv: {ee}\")\n",
    "else:\n",
    "    # ---------- Headless fallback: write key outputs as CSVs ----------\n",
    "    export_dir = os.path.join(os.path.dirname(filename), \"Exports\")\n",
    "    try: os.makedirs(export_dir, exist_ok=True)\n",
    "    except Exception: pass\n",
    "    try: exp_ret_df.to_csv(os.path.join(export_dir, \"expected_returns.csv\"))\n",
    "    except Exception as e: print(f\"[export] expected_returns.csv: {e}\")\n",
    "    try: cov_plus.to_csv(os.path.join(export_dir, \"covariance_plus.csv\"))\n",
    "    except Exception as e: print(f\"[export] covariance_plus.csv: {e}\")\n",
    "    try: W.to_csv(os.path.join(export_dir, \"weights_grid.csv\"))\n",
    "    except Exception as e: print(f\"[export] weights_grid.csv: {e}\")\n",
    "    try: stats_df.to_csv(os.path.join(export_dir, \"portfolio_stats.csv\"), index=False)\n",
    "    except Exception as e: print(f\"[export] portfolio_stats.csv: {e}\")\n",
    "    try: tilt_df.to_csv(os.path.join(export_dir, \"tilts.csv\"))\n",
    "    except Exception as e: print(f\"[export] tilts.csv: {e}\")\n",
    "    try: df_melt.to_csv(os.path.join(export_dir, \"returns_long.csv\"), index=False)\n",
    "    except Exception as e: print(f\"[export] returns_long.csv: {e}\")\n",
    "\n",
    "print(\"Workbook Successfully Updated\")\n",
    "\n",
    "# --- Create a Desktop shortcut (optional, safe in any context) ---\n",
    "try:\n",
    "    if HAS_WIN32COM:\n",
    "        shortcut_path = str(Path.home() / \"Desktop\" / \"Portfolio Optimiser.lnk\")\n",
    "\n",
    "        # Prefer the exe if it exists; otherwise point at the script we\u2019re running.\n",
    "        # Works when frozen, when run as .py, and in Jupyter (falls back to .py name in APP_DIR).\n",
    "        if getattr(sys, \"frozen\", False):\n",
    "            target = Path(sys.executable)\n",
    "        else:\n",
    "            # Try the current file if available; else fall back to a known script name in this folder\n",
    "            if \"__file__\" in globals():\n",
    "                target = Path(__file__).resolve()\n",
    "            else:\n",
    "                # Adjust the name if your launcher script is 'Main.py' instead\n",
    "                # (You have both Main.py and Portfolio_Optimiser3110.py in your screenshot.)\n",
    "                candidate = APP_DIR / \"Portfolio_Optimiser1411.py\"\n",
    "                target = candidate if candidate.exists() else (APP_DIR / \"Main.py\")\n",
    "\n",
    "        shell = win32.Dispatch(\"WScript.Shell\")\n",
    "        sc = shell.CreateShortCut(shortcut_path)\n",
    "        sc.WindowStyle = 1  # normal window\n",
    "        sc.Arguments = \"\"   # no extra args      \n",
    "        sc.Targetpath = str(target)\n",
    "        sc.WorkingDirectory = str(target.parent)\n",
    "        # Use icon.ico if present; otherwise the target itself\n",
    "        icon_path = APP_DIR / \"icon.ico\"\n",
    "        sc.IconLocation = str(icon_path if icon_path.exists() else target)\n",
    "        sc.save()\n",
    "    else:\n",
    "        print(\"[shortcut] pywin32 not available; skipping Desktop shortcut.\")\n",
    "except Exception as e:\n",
    "    print(f\"[shortcut] skipped due to error: {e}\")\n",
    "print(\"=== MU VEC (sorted) ===\")\n",
    "print(mu_vec_opt.sort_values())\n",
    "print(\"\\nMin mu:\", mu_vec_opt.min())\n",
    "print(\"Max mu:\", mu_vec_opt.max())\n",
    "print(\"Mean mu:\", mu_vec_opt.mean())\n",
    "print(\"Top 10 assets by expected return:\")\n",
    "print(mu_vec_opt.sort_values().tail(10))\n",
    "print(ff5_raw.head())\n",
    "\n",
    "# --- Build Efficient Frontier PNG for PowerPoint (optional) ---\n",
    "charts = globals().get(\"charts\", {}) or {}\n",
    "try:\n",
    "    APP_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    eff_path = os.path.join(APP_DIR, \"efficient_frontier.png\")\n",
    "\n",
    "    # Expect stats_df columns: 'Achieved Return' and 'Volatility (ann.)'\n",
    "    _x = pd.to_numeric(stats_df[\"Volatility (ann.)\"], errors=\"coerce\")\n",
    "    _y = pd.to_numeric(stats_df[\"Achieved Return\"], errors=\"coerce\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 4.8))\n",
    "    ax.plot(_x, _y, linewidth=2.0)\n",
    "    ax.set_title(chart_title)\n",
    "    ax.set_xlabel(\"Volatility (ann.)\")\n",
    "    ax.set_ylabel(\"Achieved Return\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    # RF marker (0 vol)\n",
    "    if rf_annual is not None and np.isfinite([rf_annual]).all():\n",
    "        ax.scatter([0.0], [float(rf_annual)], s=40, marker=\"o\", label=\"Cash (rf)\")\n",
    "\n",
    "    # Points\n",
    "    if current_point:\n",
    "        ax.scatter([float(current_point[0])], [float(current_point[1])], s=60, marker=\"s\", label=\"Current\")\n",
    "    if previous_point:\n",
    "        ax.scatter([float(previous_point[0])], [float(previous_point[1])], s=60, marker=\"D\", label=\"Previous\")\n",
    "    if factor_point:\n",
    "        ax.scatter([float(factor_point[0])], [float(factor_point[1])], s=60, marker=\"x\", label=\"Factor-effected\")\n",
    "    if target_point:\n",
    "        ax.scatter([float(target_point[0])], [float(target_point[1])], s=70, marker=\"+\", label=\"Target\")\n",
    "\n",
    "    ax.legend()\n",
    "    fig.savefig(eff_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    charts[\"efficient_frontier_path\"] = eff_path\n",
    "except Exception as _e_ppt_front:\n",
    "    print(f\"[pptx] Efficient frontier chart not saved: {_e_ppt_front}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Block 8: Finishers / Launchers Excel and Code ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: auto-open the workbook in Excel (independent of xlwings) ---\n",
    "OPEN_AFTER_SAVE = bool(CFG.get(\"open_after_save\", True))\n",
    "\n",
    "def _os_open(path):\n",
    "    try:\n",
    "        # Windows\n",
    "        os.startfile(path)  # type: ignore[attr-defined]\n",
    "    except AttributeError:\n",
    "        # macOS / Linux fallback\n",
    "        import subprocess, sys\n",
    "        if sys.platform == \"darwin\":\n",
    "            subprocess.run([\"open\", path])\n",
    "        else:\n",
    "            subprocess.run([\"xdg-open\", path])\n",
    "\n",
    "if OPEN_AFTER_SAVE:\n",
    "    _os_open(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- BLOCK 9: PowerPoint Report Generator ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BLOCK 8: PowerPoint Report Generator ---\n",
    "def add_header_footer(slide, title_text: str, footer_text: str = \"\"):\n",
    "    \"\"\"Adds a consistent header and footer banner with text.\"\"\"\n",
    "    # Header banner\n",
    "    header = slide.shapes.add_shape(\n",
    "        1,  # mso_shape.rectangle\n",
    "        Inches(0), Inches(0),\n",
    "        Inches(10), Inches(1)\n",
    "    )\n",
    "    fill = header.fill\n",
    "    fill.solid()\n",
    "    fill.fore_color.rgb = RGBColor(0, 51, 102)  # dark navy\n",
    "    header.line.fill.background()  # no border\n",
    "\n",
    "    # Header text\n",
    "    tf = header.text_frame\n",
    "    tf.text = title_text\n",
    "    p = tf.paragraphs[0]\n",
    "    p.font.bold = True\n",
    "    p.font.size = Pt(32)\n",
    "    p.font.color.rgb = RGBColor(255, 255, 255)\n",
    "    p.alignment = 1  # centre\n",
    "\n",
    "    # Footer banner\n",
    "    footer = slide.shapes.add_shape(\n",
    "        1, Inches(0), Inches(7), Inches(10), Inches(0.4)\n",
    "    )\n",
    "    fill = footer.fill\n",
    "    fill.solid()\n",
    "    fill.fore_color.rgb = RGBColor(230, 230, 230)\n",
    "    footer.line.fill.background()\n",
    "\n",
    "    # Footer text\n",
    "    tf = footer.text_frame\n",
    "    tf.text = footer_text or \"Generated by Portfolio Optimiser\"\n",
    "    p = tf.paragraphs[0]\n",
    "    p.font.size = Pt(12)\n",
    "    p.font.color.rgb = RGBColor(80, 80, 80)\n",
    "    p.alignment = 1  # centre\n",
    "\n",
    "def export_to_ppt(results, trades, charts=None):\n",
    "    \"\"\"\n",
    "    Generates a professional PowerPoint summary based on your custom template.\n",
    "    \"\"\"\n",
    "\n",
    "    APP_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "\n",
    "    # Template file (design only, never edited)\n",
    "    template_path = os.path.join(APP_DIR, \"PowerPoint_Template.pptx\")\n",
    "\n",
    "    # Output path \u2014 always overwrite this file\n",
    "    ppt_path = os.path.join(APP_DIR, \"Portfolio_Report.pptx\")\n",
    "\n",
    "    # Load your custom template\n",
    "    prs = Presentation(template_path)\n",
    "   \n",
    "    # --- SLIDE 1: Title and TimeStamp ---\n",
    "    slide = prs.slides[0] \n",
    "    \n",
    "    # --- Title text box ---  \n",
    "    if slide.shapes.title:\n",
    "        slide.shapes.title.text = \"Portfolio Performance Overview\"\n",
    "    \n",
    "    # --- Timestamp text box ---\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"Last updated on the %d %B %Y at %I:%M %p\")\n",
    "    ts_box = slide.shapes.add_textbox(Inches(0.8), Inches(6.0), Inches(9), Inches(0.5))\n",
    "    tf2 = ts_box.text_frame\n",
    "    tf2.word_wrap = False\n",
    "    p2 = tf2.add_paragraph()\n",
    "    p2.text = timestamp\n",
    "    p2.font.size = Pt(16)\n",
    "    p2.alignment = PP_ALIGN.LEFT  # uses default colour/font\n",
    "\n",
    "    # --- SLIDE 2: Trade Plan + Brokerage ---\n",
    "    slide_layout = prs.slide_layouts[20]  # clean layout from your master\n",
    "    slide = prs.slides.add_slide(slide_layout)\n",
    "    \n",
    "    # Title\n",
    "    if slide.shapes.title:\n",
    "        slide.shapes.title.text = \"Trade Plan and Brokerage Overview\"\n",
    "    \n",
    "    # --- Draw Trade Plan table ---\n",
    "    if trades is not None and not trades.empty:\n",
    "        # Map existing columns to your new desired display names\n",
    "        rename_map = {\n",
    "            \"Curr Units\": \"Current\",\n",
    "            \"Target Units\": \"Target\",\n",
    "            \"\u0394 Units\": \"Change\",\n",
    "            \"Last Px (AUD)\": \"Last Price\",\n",
    "            \"Cash Flow (AUD)\": \"Cash Flow\",\n",
    "            \"Brokerage (AUD)\": \"Brokerage\",\n",
    "        }\n",
    "    \n",
    "        # Select, copy, and rename columns\n",
    "        cols_needed = [\"Security\"] + list(rename_map.keys())\n",
    "        cols_present = [c for c in cols_needed if c in trades.columns]\n",
    "        df = trades[cols_present].copy()\n",
    "        if \"Security\" not in df.columns and trades.index.name == \"Security\":\n",
    "            df = trades.reset_index()[cols_present]\n",
    "        df.rename(columns=rename_map, inplace=True)\n",
    "        # Reorder to your final order but only for those that exist\n",
    "        final_order = [c for c in [\"Security\",\"Current\",\"Target\",\"Change\",\"Last Price\",\"Brokerage\",\"Cash Flow\"] if c in df.columns]\n",
    "        df = df[final_order]\n",
    "            \n",
    "    \n",
    "        # --- Clean and format values ---\n",
    "        df[\"Security\"] = df[\"Security\"].astype(str).str.replace(\".AX\", \"\", regex=False)\n",
    "        for col in [\"Last Price\", \"Cash Flow\", \"Brokerage\"]:\n",
    "            df[col] = (\n",
    "                pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                .round(2)\n",
    "                .apply(lambda x: f\"-${abs(x):,.2f}\" if x < 0 else f\"${x:,.2f}\")\n",
    "            )\n",
    "    \n",
    "        # --- Determine if we split into two tables ---\n",
    "        rows, cols = df.shape\n",
    "        split = rows > 15\n",
    "        half = math.ceil(rows / 2) if split else rows\n",
    "        table_sets = [df.iloc[:half]] if not split else [df.iloc[:half], df.iloc[half:]]\n",
    "        left_positions = [Inches(0.5), Inches(5.0)] if split else [Inches(0.5)]\n",
    "    \n",
    "        def autofit_table_width(table, df, total_width_in=4.7):\n",
    "            \"\"\"\n",
    "            True auto-fit for PowerPoint tables with a small minimum width for narrow headers\n",
    "            (prevents wrapping for 'Target' and 'Change').\n",
    "            \"\"\"\n",
    "            from pptx.util import Inches\n",
    "        \n",
    "            def est_width(text):\n",
    "                return len(str(text)) * 0.085  # empirical average for 9pt Calibri text\n",
    "        \n",
    "            est_widths = []\n",
    "            for col in df.columns:\n",
    "                header_w = est_width(col)\n",
    "                data_w = max(est_width(v) for v in df[col].astype(str))\n",
    "                width = max(header_w, data_w)\n",
    "        \n",
    "                # Minimum width safeguard for narrow headers\n",
    "                if col.lower() in (\"target\", \"change\"):\n",
    "                    width = max(width, 0.55)  # ensures ~0.55 inches minimum\n",
    "                elif col.lower() == \"security\":\n",
    "                    width = max(width, 0.7)   # ensures symbol names have room\n",
    "        \n",
    "                est_widths.append(width)\n",
    "        \n",
    "            total_est = sum(est_widths)\n",
    "            scale = total_width_in / total_est\n",
    "        \n",
    "            for j, est in enumerate(est_widths):\n",
    "                table.columns[j].width = Inches(est * scale)\n",
    "\n",
    "        \n",
    "        # --- Draw tables ---\n",
    "        for idx, subdf in enumerate(table_sets):\n",
    "            top = Inches(1.8)\n",
    "            left = Inches(0.25 + idx * 4.75)  # tighter to edge\n",
    "            width = Inches(4.75)\n",
    "            height = Inches(4.8)\n",
    "        \n",
    "            table = slide.shapes.add_table(\n",
    "                rows=subdf.shape[0] + 1,\n",
    "                cols=subdf.shape[1],\n",
    "                left=left,\n",
    "                top=top,\n",
    "                width=width,\n",
    "                height=height\n",
    "            ).table\n",
    "        \n",
    "            # Auto-fit widths based on text lengths\n",
    "            autofit_table_width(table, subdf, total_width_in=4.7)\n",
    "        \n",
    "            # **Disable word wrapping for all cells (keeps headers on one line)**\n",
    "            for cell in table.iter_cells():\n",
    "                cell.text_frame.word_wrap = False\n",
    "        \n",
    "            # Reduce row height\n",
    "            for r in range(len(table.rows)):\n",
    "                table.rows[r].height = Inches(0.23)\n",
    "        \n",
    "            # Header\n",
    "            for j, col_name in enumerate(subdf.columns):\n",
    "                cell = table.cell(0, j)\n",
    "                cell.text = col_name\n",
    "                tf = cell.text_frame\n",
    "                tf.word_wrap = False\n",
    "                tf.margin_left = 0\n",
    "                tf.margin_right = 0\n",
    "                tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "                p = tf.paragraphs[0]\n",
    "                p.font.bold = True\n",
    "                p.font.size = Pt(8)  \n",
    "                p.alignment = PP_ALIGN.CENTER\n",
    "        \n",
    "            # Data rows\n",
    "            for i, (_, row) in enumerate(subdf.iterrows(), start=1):\n",
    "                for j, val in enumerate(row):\n",
    "                    cell = table.cell(i, j)\n",
    "                    cell.text = str(val)\n",
    "                    p = cell.text_frame.paragraphs[0]\n",
    "                    p.font.size = Pt(9)\n",
    "                    if j == 0:  # security column bold\n",
    "                        p.font.bold = True\n",
    "                    p.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "    \n",
    "        # --- Summary bar across top ---\n",
    "        left = Inches(0.5)\n",
    "        top = Inches(1.1)  # just below the title\n",
    "        width = Inches(9.0)\n",
    "        height = Inches(0.5)\n",
    "        textbox = slide.shapes.add_textbox(left, top, width, height)\n",
    "        tf = textbox.text_frame\n",
    "        tf.word_wrap = False\n",
    "        tf.clear()\n",
    "        \n",
    "        # --- Fetch values ---\n",
    "        total_portfolio = results.get(\"total_portfolio_value\", 0)\n",
    "        total_brokerage = results.get(\"total_brokerage\", 0)\n",
    "        net_invested = results.get(\"net_invested\", 0)\n",
    "        portfolio_change = results.get(\"portfolio_change\", 0)\n",
    "        net_invested_change = results.get(\"net_invested_change\", 0)\n",
    "        \n",
    "        # --- Helper to format change text ---\n",
    "        def add_change_run(paragraph, val):\n",
    "            run = paragraph.add_run()\n",
    "            if val == 0:\n",
    "                run.text = \"\"\n",
    "                return\n",
    "            sign = \"+\" if val > 0 else \"\"\n",
    "            run.text = f\" ({sign}{val:,.2f})\"\n",
    "            run.font.size = Pt(14)\n",
    "            if val > 0:\n",
    "                run.font.color.rgb = RGBColor(0, 128, 0)  # green\n",
    "            elif val < 0:\n",
    "                run.font.color.rgb = RGBColor(192, 0, 0)  # red\n",
    "            else:\n",
    "                run.font.color.rgb = RGBColor(80, 80, 80)\n",
    "        \n",
    "        # --- Main summary line ---\n",
    "        p = tf.add_paragraph()\n",
    "        p.font.size = Pt(14)\n",
    "        p.font.bold = True\n",
    "        p.alignment = PP_ALIGN.CENTER\n",
    "        \n",
    "        # Text with separate runs for coloured numbers\n",
    "        run1 = p.add_run()\n",
    "        run1.text = f\"Total Portfolio: ${total_portfolio:,.2f}\"\n",
    "        run1.font.size = Pt(14)\n",
    "        run1.font.bold = True\n",
    "        add_change_run(p, portfolio_change)\n",
    "        \n",
    "        run2 = p.add_run()\n",
    "        run2.text = f\"     Total Brokerage: ${total_brokerage:,.2f}     \"\n",
    "        run2.font.size = Pt(14)\n",
    "        run2.font.bold = True\n",
    "        run2.font.color.rgb = RGBColor(0, 0, 0)\n",
    "        \n",
    "        run3 = p.add_run()\n",
    "        run3.text = f\"Net Invested: ${net_invested:,.2f}\"\n",
    "        run3.font.size = Pt(14)\n",
    "        run3.font.bold = True\n",
    "        add_change_run(p, net_invested_change)\n",
    "\n",
    "        # --- Slide 2: Portfolio vs Indices ---\n",
    "        \n",
    "    # --- Cash summary (derived from Trade Plan cash flows) ---\n",
    "    try:\n",
    "        cash_balance = 0.0\n",
    "        if trades is not None and not trades.empty and \"Cash Flow (AUD)\" in trades.columns:\n",
    "            cash_balance = float(pd.to_numeric(trades[\"Cash Flow (AUD)\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "        cash_box = slide.shapes.add_textbox(Inches(7.2), Inches(5.8), Inches(2.6), Inches(0.6))\n",
    "        tfc = cash_box.text_frame\n",
    "        tfc.clear()\n",
    "        p = tfc.paragraphs[0]\n",
    "        p.text = f\"Cash: {cash_balance:,.0f} AUD\"\n",
    "        p.font.size = Pt(18)\n",
    "        p.font.bold = True\n",
    "        p.alignment = PP_ALIGN.RIGHT\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # --- SLIDE 3: Efficient Frontier ---\n",
    "    if charts and isinstance(charts, dict) and charts.get(\"efficient_frontier_path\") and os.path.exists(charts[\"efficient_frontier_path\"]):\n",
    "        slide_layout = prs.slide_layouts[20]  # clean layout\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        if slide.shapes.title:\n",
    "            slide.shapes.title.text = \"Efficient Frontier\"\n",
    "\n",
    "        try:\n",
    "            slide.shapes.add_picture(charts[\"efficient_frontier_path\"], Inches(0.8), Inches(1.3), width=Inches(8.2), height=Inches(4.6))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "slide_layout = prs.slide_layouts[20]  # clean layout from your master\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        \n",
    "        # Title\n",
    "        if slide.shapes.title:\n",
    "            slide.shapes.title.text = \"Portfolio Performance\"\n",
    "        \n",
    "        # --- Get 3-month portfolio + benchmarks ---\n",
    "        lookback_days = 90\n",
    "        recent_prices = prices.iloc[-lookback_days:].copy()\n",
    "        \n",
    "        benchmarks = [\"^AORD\", \"^GSPC\", \"^IXIC\"]\n",
    "        benchmark_data = yf.download(\n",
    "            benchmarks,\n",
    "            start=recent_prices.index[0],\n",
    "            end=recent_prices.index[-1],\n",
    "            progress=False,\n",
    "            auto_adjust=True,\n",
    "            threads=False\n",
    "        )\n",
    "        \n",
    "        # Handle multi-index and fill missing values\n",
    "        if isinstance(benchmark_data.columns, pd.MultiIndex):\n",
    "            benchmark_data = benchmark_data[\"Close\"]\n",
    "            benchmark_data = benchmark_data.ffill().bfill()\n",
    "        \n",
    "        # --- Align and clean ---\n",
    "        common_index = recent_prices.index.intersection(benchmark_data.index)\n",
    "        recent_prices = recent_prices.reindex(common_index)\n",
    "        benchmark_data = benchmark_data.reindex(common_index)\n",
    "        \n",
    "        pval = portfolio_value_series.reindex(recent_prices.index).ffill().bfill()\n",
    "        \n",
    "        mask = pval.notna() & benchmark_data.notna().any(axis=1)\n",
    "        pval = pval[mask]\n",
    "        benchmark_data = benchmark_data.loc[mask]\n",
    "        \n",
    "        portfolio_returns = pval / pval.iloc[0] - 1\n",
    "        benchmark_returns = benchmark_data.div(benchmark_data.iloc[0]).subtract(1)\n",
    "        \n",
    "        # Friendly labels\n",
    "        benchmark_returns = benchmark_returns.rename(columns={\n",
    "            \"^AORD\": \"ASX\",\n",
    "            \"^GSPC\": \"S&P 500\",\n",
    "            \"^IXIC\": \"NASDAQ\"\n",
    "        })\n",
    "        \n",
    "        # --- Combine into one DataFrame ---\n",
    "        perf_df = pd.concat(\n",
    "            [portfolio_returns.rename(\"Portfolio\")] +\n",
    "            [benchmark_returns[c].rename(c) for c in benchmark_returns.columns],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Optional: clip extreme outliers (prevents visual spikes)\n",
    "        perf_df = perf_df.clip(lower=-0.2, upper=0.5)\n",
    "        \n",
    "        # --- Plot ---\n",
    "        fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "        perf_df.mul(100).plot(ax=ax, linewidth=1.8)\n",
    "        ax.set_title(\"Portfolio vs ASX, S&P 500, NASDAQ (3-Month Performance)\")\n",
    "        ax.set_ylabel(\"Return (%)\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "        \n",
    "        chart_path = os.path.join(APP_DIR, \"perf_vs_indices.png\")\n",
    "        fig.savefig(chart_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # --- Insert chart in PowerPoint ---\n",
    "        slide.shapes.add_picture(chart_path, Inches(0.8), Inches(1.3), width=Inches(8.2), height=Inches(4.4))\n",
    "\n",
    "\n",
    "\n",
    "        prs.save(ppt_path)\n",
    "        print(f\"[pptx] Report saved to: {ppt_path}\")\n",
    "        return ppt_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Block 10: Finishers / Launchers PPTX ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pptx] Report saved to: C:\\Users\\Fionn Guina\\Portfolio_Optimiser\\Portfolio_Report.pptx\n"
     ]
    }
   ],
   "source": [
    "# 1) Generate the report (optional)\n",
    "ppt_path = None\n",
    "if CFG.get(\"generate_report\", True):\n",
    "    ppt_path = export_to_ppt(results, trades, charts)  # your function returns the .pptx path\n",
    "\n",
    "# 2) Open the PowerPoint if requested (either via dialog or config)\n",
    "open_ppt = bool(user_opts.get(\"open_ppt\", CFG.get(\"open_ppt_after_save\", True)))\n",
    "if open_ppt and ppt_path and os.path.exists(ppt_path):\n",
    "    _os_open(ppt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(securities_opt) - set(Sigma_opt.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_ff5_mom_daily EXISTS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    rf = get_ff5_mom_daily()\n",
    "    print(\"get_ff5_mom_daily EXISTS\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(B, f_mean_ann, Fcov_daily)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(recommend_factor_tilts_achievable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}